diff --git a/examples/sac.py b/examples/sac.py
index 0d86396..7b98fb0 100644
--- a/examples/sac.py
+++ b/examples/sac.py
@@ -1,4 +1,7 @@
-from gym.envs.mujoco import HalfCheetahEnv
+from gym.envs.mujoco import HopperEnv
+
+import sys
+sys.path.append('../')
 
 import rlkit.torch.pytorch_util as ptu
 from rlkit.data_management.env_replay_buffer import EnvReplayBuffer
@@ -12,8 +15,8 @@ from rlkit.torch.torch_rl_algorithm import TorchBatchRLAlgorithm
 
 
 def experiment(variant):
-    expl_env = NormalizedBoxEnv(HalfCheetahEnv())
-    eval_env = NormalizedBoxEnv(HalfCheetahEnv())
+    expl_env = NormalizedBoxEnv(HopperEnv())
+    eval_env = NormalizedBoxEnv(HopperEnv())
     obs_dim = expl_env.observation_space.low.size
     action_dim = eval_env.action_space.low.size
 
@@ -106,6 +109,6 @@ if __name__ == "__main__":
             use_automatic_entropy_tuning=True,
         ),
     )
-    setup_logger('name-of-experiment', variant=variant)
+    setup_logger('Test sac on hopper env', variant=variant)
     # ptu.set_gpu_mode(True)  # optionally set the GPU (default=False)
     experiment(variant)
diff --git a/examples/td3.py b/examples/td3.py
index b86fa50..0ad84c8 100644
--- a/examples/td3.py
+++ b/examples/td3.py
@@ -6,7 +6,7 @@ a bit noisy from one epoch to the next (occasionally dips dow to ~2000).
 
 Note that one epoch = 5k steps, so 200 epochs = 1 million steps.
 """
-from gym.envs.mujoco import HalfCheetahEnv
+from gym.envs.mujoco import HopperEnv
 
 import rlkit.torch.pytorch_util as ptu
 from rlkit.data_management.env_replay_buffer import EnvReplayBuffer
@@ -22,8 +22,8 @@ from rlkit.torch.torch_rl_algorithm import TorchBatchRLAlgorithm
 
 
 def experiment(variant):
-    expl_env = NormalizedBoxEnv(HalfCheetahEnv())
-    eval_env = NormalizedBoxEnv(HalfCheetahEnv())
+    expl_env = NormalizedBoxEnv(HopperEnv())
+    eval_env = NormalizedBoxEnv(HopperEnv())
     obs_dim = expl_env.observation_space.low.size
     action_dim = expl_env.action_space.low.size
     qf1 = ConcatMlp(
diff --git a/rlkit/torch/networks/__init__.py b/rlkit/torch/networks/__init__.py
index 47d00e0..2e5d867 100644
--- a/rlkit/torch/networks/__init__.py
+++ b/rlkit/torch/networks/__init__.py
@@ -13,11 +13,12 @@ from rlkit.torch.networks.image_state import ImageStatePolicy, ImageStateQ
 from rlkit.torch.networks.linear_transform import LinearTransform
 from rlkit.torch.networks.normalization import LayerNorm
 from rlkit.torch.networks.mlp import (
-    Mlp, ConcatMlp, MlpPolicy, TanhMlpPolicy,
+    Mlp, ConcatMlp, MlpPolicy, TanhMlpPolicy, FlattenMlp,
     MlpQf,
     MlpQfWithObsProcessor,
     ConcatMultiHeadedMlp,
 )
+from rlkit.torch.networks.custom import *
 from rlkit.torch.networks.pretrained_cnn import PretrainedCNN
 from rlkit.torch.networks.two_headed_mlp import TwoHeadMlp
 
diff --git a/rlkit/torch/networks/custom.py b/rlkit/torch/networks/custom.py
index d43435a..0e72671 100644
--- a/rlkit/torch/networks/custom.py
+++ b/rlkit/torch/networks/custom.py
@@ -1,3 +1,14 @@
 """
 Random networks
 """
+from rlkit.torch.core import PyTorchModule
+from torch import nn
+import torch.nn.functional as F
+
+class LinearMlp(PyTorchModule):
+    def __init__(self, input_size, output_size):
+        super().__init__()
+        self.fc = nn.Linear(input_size, output_size)
+
+    def forward(self, x):
+        return self.fc(x)
diff --git a/rlkit/torch/networks/mlp.py b/rlkit/torch/networks/mlp.py
index a9ad8f1..310b6e3 100644
--- a/rlkit/torch/networks/mlp.py
+++ b/rlkit/torch/networks/mlp.py
@@ -11,6 +11,7 @@ from rlkit.torch.networks import LayerNorm
 from rlkit.torch.pytorch_util import activation_from_string
 
 
+
 class Mlp(PyTorchModule):
     def __init__(
             self,
@@ -55,6 +56,8 @@ class Mlp(PyTorchModule):
         self.last_fc = nn.Linear(in_size, output_size)
         self.last_fc.weight.data.uniform_(-init_w, init_w)
         self.last_fc.bias.data.fill_(0)
+        
+        self.to(ptu.device)
 
     def forward(self, input, return_preactivations=False):
         h = input
@@ -71,6 +74,16 @@ class Mlp(PyTorchModule):
             return output
 
 
+class FlattenMlp(Mlp):
+    """
+    Flatten inputs along dimension 1 and then pass through MLP.
+    """
+    def forward(self, *inputs, **kwargs):
+        flat_inputs = torch.cat(inputs, dim=1)
+        return super().forward(flat_inputs, **kwargs)
+
+
+
 class MultiHeadedMlp(Mlp):
     """
                    .-> linear head 0
@@ -128,7 +141,7 @@ class ConcatMultiHeadedMlp(MultiHeadedMlp):
 
 class ConcatMlp(Mlp):
     """
-    Concatenate inputs along dimension and then pass through MLP.
+    Concatenate inputs along dimension and then pass through MLP. It is just a better version of flattenMLP
     """
     def __init__(self, *args, dim=1, **kwargs):
         super().__init__(*args, **kwargs)
diff --git a/rlkit/torch/sac/sac.py b/rlkit/torch/sac/sac.py
index 825dfb9..6da9c3c 100644
--- a/rlkit/torch/sac/sac.py
+++ b/rlkit/torch/sac/sac.py
@@ -212,6 +212,11 @@ class SACTrainer(TorchTrainer, LossFunction):
                 'Log Pis',
                 ptu.get_numpy(log_pi),
             ))
+            eval_statistics.update(create_stats_ordered_dict(
+                'Bellman error',
+                ptu.get_numpy((min(q1_pred, q2_pred) - q_target) ** 2),
+            ))
+            
             policy_statistics = add_prefix(dist.get_diagnostics(), "policy/")
             eval_statistics.update(policy_statistics)
             if self.use_automatic_entropy_tuning:
