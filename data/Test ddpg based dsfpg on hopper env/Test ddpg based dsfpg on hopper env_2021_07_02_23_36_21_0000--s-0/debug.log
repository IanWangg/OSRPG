2021-07-02 23:36:33.334014 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 0 finished
---------------------------------  ---------------
replay_buffer/size                 11000
trainer/QF Loss                        1.01287
trainer/Policy Loss                   -0.000341987
trainer/Raw Policy Loss               -0.000341987
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean             0.000359317
trainer/Q Predictions Std              0.000538951
trainer/Q Predictions Max              0.00279533
trainer/Q Predictions Min             -0.00117497
trainer/Q Targets Mean                 0.0386763
trainer/Q Targets Std                  0.110658
trainer/Q Targets Max                  0.402267
trainer/Q Targets Min                 -0.314851
trainer/Bellman Errors Mean            0.013684
trainer/Bellman Errors Std             0.0262469
trainer/Bellman Errors Max             0.160806
trainer/Bellman Errors Min             6.02756e-08
trainer/Policy Action Mean            -0.000550279
trainer/Policy Action Std              0.00140235
trainer/Policy Action Max              0.00516964
trainer/Policy Action Min             -0.0068737
exploration/num steps total        11000
exploration/num paths total          569
exploration/path length Mean          17.8571
exploration/path length Std            9.61106
exploration/path length Max           58
exploration/path length Min            7
exploration/Rewards Mean               0.830593
exploration/Rewards Std                0.56508
exploration/Rewards Max                2.27269
exploration/Rewards Min               -1.46885
exploration/Returns Mean              14.832
exploration/Returns Std               15.4942
exploration/Returns Max               83.8656
exploration/Returns Min                1.53992
exploration/Actions Mean               0.0107687
exploration/Actions Std                0.482452
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                 56
exploration/Average Returns           14.832
evaluation/num steps total           937
evaluation/num paths total             6
evaluation/path length Mean          156.167
evaluation/path length Std            27.8533
evaluation/path length Max           202
evaluation/path length Min           120
evaluation/Rewards Mean                0.951323
evaluation/Rewards Std                 0.0721166
evaluation/Rewards Max                 1.02193
evaluation/Rewards Min                 0.684272
evaluation/Returns Mean              148.565
evaluation/Returns Std                31.1703
evaluation/Returns Max               193.456
evaluation/Returns Min               108.007
evaluation/Actions Mean               -0.000128117
evaluation/Actions Std                 0.000297663
evaluation/Actions Max                 0.000373636
evaluation/Actions Min                -0.000780189
evaluation/Num Paths                   6
evaluation/Average Returns           148.565
time/data storing (s)                  0.00350316
time/evaluation sampling (s)           0.364093
time/exploration sampling (s)          0.474369
time/logging (s)                       0.00389414
time/saving (s)                        0.00154998
time/training (s)                      6.05017
time/epoch (s)                         6.89758
time/total (s)                        12.6946
Epoch                                  0
---------------------------------  ---------------
2021-07-02 23:36:40.120057 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 1 finished
---------------------------------  ---------------
replay_buffer/size                 12000
trainer/QF Loss                        0.00541489
trainer/Policy Loss                   -5.76232
trainer/Raw Policy Loss               -5.76232
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean             4.94444
trainer/Q Predictions Std              2.73461
trainer/Q Predictions Max             12.978
trainer/Q Predictions Min             -2.39293
trainer/Q Targets Mean                 4.97336
trainer/Q Targets Std                  2.75401
trainer/Q Targets Max                 12.7648
trainer/Q Targets Min                 -2.30798
trainer/Bellman Errors Mean            0.437625
trainer/Bellman Errors Std             4.2024
trainer/Bellman Errors Max            67.1632
trainer/Bellman Errors Min             2.14158e-06
trainer/Policy Action Mean             0.999993
trainer/Policy Action Std              2.67212e-05
trainer/Policy Action Max              1
trainer/Policy Action Min              0.999583
exploration/num steps total        12000
exploration/num paths total          610
exploration/path length Mean          24.3902
exploration/path length Std            3.64175
exploration/path length Max           40
exploration/path length Min           17
exploration/Rewards Mean               1.70381
exploration/Rewards Std                0.45862
exploration/Rewards Max                2.48793
exploration/Rewards Min                1.00578
exploration/Returns Mean              41.5564
exploration/Returns Std                6.03358
exploration/Returns Max               68.04
exploration/Returns Min               24.0894
exploration/Actions Mean               0.776279
exploration/Actions Std                0.322828
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                 41
exploration/Average Returns           41.5564
evaluation/num steps total          1931
evaluation/num paths total            51
evaluation/path length Mean           22.0889
evaluation/path length Std             0.50869
evaluation/path length Max            23
evaluation/path length Min            21
evaluation/Rewards Mean                1.75796
evaluation/Rewards Std                 0.462709
evaluation/Rewards Max                 2.41498
evaluation/Rewards Min                 1.00945
evaluation/Returns Mean               38.8314
evaluation/Returns Std                 0.836865
evaluation/Returns Max                40.4121
evaluation/Returns Min                36.9721
evaluation/Actions Mean                0.999967
evaluation/Actions Std                 7.78625e-05
evaluation/Actions Max                 1
evaluation/Actions Min                 0.99959
evaluation/Num Paths                  45
evaluation/Average Returns            38.8314
time/data storing (s)                  0.00345997
time/evaluation sampling (s)           0.39435
time/exploration sampling (s)          0.391075
time/logging (s)                       0.00400106
time/saving (s)                        0.00140583
time/training (s)                      5.99014
time/epoch (s)                         6.78443
time/total (s)                        19.4804
Epoch                                  1
---------------------------------  ---------------
2021-07-02 23:36:46.861289 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 2 finished
---------------------------------  ---------------
replay_buffer/size                 13000
trainer/QF Loss                        0.00211913
trainer/Policy Loss                  -14.5514
trainer/Raw Policy Loss              -14.5514
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean            12.9849
trainer/Q Predictions Std              4.47263
trainer/Q Predictions Max             23.6238
trainer/Q Predictions Min              0.147185
trainer/Q Targets Mean                12.9622
trainer/Q Targets Std                  4.7806
trainer/Q Targets Max                 23.3362
trainer/Q Targets Min                 -0.691719
trainer/Bellman Errors Mean            3.64354
trainer/Bellman Errors Std            24.2426
trainer/Bellman Errors Max           322.29
trainer/Bellman Errors Min             3.45835e-08
trainer/Policy Action Mean             0.999999
trainer/Policy Action Std              4.35256e-06
trainer/Policy Action Max              1
trainer/Policy Action Min              0.999935
exploration/num steps total        13000
exploration/num paths total          651
exploration/path length Mean          24.3902
exploration/path length Std            3.15361
exploration/path length Max           37
exploration/path length Min           20
exploration/Rewards Mean               1.72051
exploration/Rewards Std                0.465431
exploration/Rewards Max                2.61357
exploration/Rewards Min                0.991965
exploration/Returns Mean              41.9638
exploration/Returns Std                5.48166
exploration/Returns Max               66.5199
exploration/Returns Min               30.2099
exploration/Actions Mean               0.777934
exploration/Actions Std                0.325453
exploration/Actions Max                1
exploration/Actions Min               -0.723943
exploration/Num Paths                 41
exploration/Average Returns           41.9638
evaluation/num steps total          2930
evaluation/num paths total            96
evaluation/path length Mean           22.2
evaluation/path length Std             0.618241
evaluation/path length Max            23
evaluation/path length Min            21
evaluation/Rewards Mean                1.75816
evaluation/Rewards Std                 0.462763
evaluation/Rewards Max                 2.41549
evaluation/Rewards Min                 1.00723
evaluation/Returns Mean               39.0313
evaluation/Returns Std                 1.0096
evaluation/Returns Max                40.595
evaluation/Returns Min                36.7562
evaluation/Actions Mean                0.999998
evaluation/Actions Std                 7.27673e-06
evaluation/Actions Max                 1
evaluation/Actions Min                 0.999956
evaluation/Num Paths                  45
evaluation/Average Returns            39.0313
time/data storing (s)                  0.00344084
time/evaluation sampling (s)           0.401222
time/exploration sampling (s)          0.384699
time/logging (s)                       0.00399162
time/saving (s)                        0.00139726
time/training (s)                      5.94485
time/epoch (s)                         6.7396
time/total (s)                        26.2213
Epoch                                  2
---------------------------------  ---------------
2021-07-02 23:36:53.568150 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 3 finished
---------------------------------  ---------------
replay_buffer/size                 14000
trainer/QF Loss                        0.00297778
trainer/Policy Loss                  -24.7255
trainer/Raw Policy Loss              -24.7255
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean            22.5281
trainer/Q Predictions Std              8.17283
trainer/Q Predictions Max             35.1927
trainer/Q Predictions Min             -1.50043
trainer/Q Targets Mean                22.6425
trainer/Q Targets Std                  8.42225
trainer/Q Targets Max                 35.7797
trainer/Q Targets Min                 -1.07889
trainer/Bellman Errors Mean            3.53019
trainer/Bellman Errors Std            18.5246
trainer/Bellman Errors Max           236.145
trainer/Bellman Errors Min             1.14087e-08
trainer/Policy Action Mean             1
trainer/Policy Action Std              1.13523e-06
trainer/Policy Action Max              1
trainer/Policy Action Min              0.999987
exploration/num steps total        14000
exploration/num paths total          693
exploration/path length Mean          23.8095
exploration/path length Std            2.61168
exploration/path length Max           36
exploration/path length Min           22
exploration/Rewards Mean               1.72121
exploration/Rewards Std                0.473754
exploration/Rewards Max                2.57827
exploration/Rewards Min                0.647171
exploration/Returns Mean              40.9811
exploration/Returns Std                2.97827
exploration/Returns Max               50.3273
exploration/Returns Min               38.1191
exploration/Actions Mean               0.796628
exploration/Actions Std                0.31104
exploration/Actions Max                1
exploration/Actions Min               -0.782555
exploration/Num Paths                 42
exploration/Average Returns           40.9811
evaluation/num steps total          3922
evaluation/num paths total           141
evaluation/path length Mean           22.0444
evaluation/path length Std             0.594626
evaluation/path length Max            23
evaluation/path length Min            21
evaluation/Rewards Mean                1.75799
evaluation/Rewards Std                 0.464179
evaluation/Rewards Max                 2.41265
evaluation/Rewards Min                 1.00688
evaluation/Returns Mean               38.754
evaluation/Returns Std                 0.975264
evaluation/Returns Max                40.5448
evaluation/Returns Min                36.8941
evaluation/Actions Mean                0.999999
evaluation/Actions Std                 2.50049e-06
evaluation/Actions Max                 1
evaluation/Actions Min                 0.999984
evaluation/Num Paths                  45
evaluation/Average Returns            38.754
time/data storing (s)                  0.00339726
time/evaluation sampling (s)           0.382799
time/exploration sampling (s)          0.385639
time/logging (s)                       0.00403387
time/saving (s)                        0.00144573
time/training (s)                      5.92801
time/epoch (s)                         6.70533
time/total (s)                        32.9279
Epoch                                  3
---------------------------------  ---------------
2021-07-02 23:37:00.374200 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 4 finished
---------------------------------  ---------------
replay_buffer/size                 15000
trainer/QF Loss                        0.00239281
trainer/Policy Loss                  -30.8099
trainer/Raw Policy Loss              -30.8099
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean            29.1132
trainer/Q Predictions Std             10.8876
trainer/Q Predictions Max             45.5505
trainer/Q Predictions Min             -3.37822
trainer/Q Targets Mean                29.2187
trainer/Q Targets Std                 11.0324
trainer/Q Targets Max                 47.8628
trainer/Q Targets Min                 -0.926742
trainer/Bellman Errors Mean            3.34282
trainer/Bellman Errors Std            13.8453
trainer/Bellman Errors Max           172.393
trainer/Bellman Errors Min             4.35524e-07
trainer/Policy Action Mean             1
trainer/Policy Action Std              9.73076e-07
trainer/Policy Action Max              1
trainer/Policy Action Min              0.999989
exploration/num steps total        15000
exploration/num paths total          735
exploration/path length Mean          23.8095
exploration/path length Std            4.93886
exploration/path length Max           43
exploration/path length Min            3
exploration/Rewards Mean               1.74103
exploration/Rewards Std                0.462292
exploration/Rewards Max                2.66394
exploration/Rewards Min                1.00589
exploration/Returns Mean              41.4532
exploration/Returns Std                9.06819
exploration/Returns Max               78.3117
exploration/Returns Min                3.26408
exploration/Actions Mean               0.782929
exploration/Actions Std                0.312599
exploration/Actions Max                1
exploration/Actions Min               -0.724688
exploration/Num Paths                 42
exploration/Average Returns           41.4532
evaluation/num steps total          4915
evaluation/num paths total           186
evaluation/path length Mean           22.0667
evaluation/path length Std             0.573488
evaluation/path length Max            23
evaluation/path length Min            21
evaluation/Rewards Mean                1.75851
evaluation/Rewards Std                 0.46349
evaluation/Rewards Max                 2.40806
evaluation/Rewards Min                 1.00721
evaluation/Returns Mean               38.8044
evaluation/Returns Std                 0.998326
evaluation/Returns Max                40.6244
evaluation/Returns Min                36.7016
evaluation/Actions Mean                0.999999
evaluation/Actions Std                 2.48855e-06
evaluation/Actions Max                 1
evaluation/Actions Min                 0.999985
evaluation/Num Paths                  45
evaluation/Average Returns            38.8044
time/data storing (s)                  0.0034303
time/evaluation sampling (s)           0.38788
time/exploration sampling (s)          0.384311
time/logging (s)                       0.00620739
time/saving (s)                        0.00146726
time/training (s)                      6.02333
time/epoch (s)                         6.80662
time/total (s)                        39.7359
Epoch                                  4
---------------------------------  ---------------
2021-07-02 23:37:07.197950 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 5 finished
---------------------------------  ---------------
replay_buffer/size                 16000
trainer/QF Loss                        0.00175661
trainer/Policy Loss                  -37.0581
trainer/Raw Policy Loss              -37.0581
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean            34.0291
trainer/Q Predictions Std             11.6778
trainer/Q Predictions Max             84.1666
trainer/Q Predictions Min              0.0833783
trainer/Q Targets Mean                33.7533
trainer/Q Targets Std                 11.9369
trainer/Q Targets Max                 86.0331
trainer/Q Targets Min                 -0.514067
trainer/Bellman Errors Mean            4.90845
trainer/Bellman Errors Std            21.9556
trainer/Bellman Errors Max           255.495
trainer/Bellman Errors Min             4.72792e-08
trainer/Policy Action Mean             0.338393
trainer/Policy Action Std              0.876849
trainer/Policy Action Max              1
trainer/Policy Action Min             -0.999984
exploration/num steps total        16000
exploration/num paths total          785
exploration/path length Mean          20
exploration/path length Std           10.8204
exploration/path length Max           87
exploration/path length Min           10
exploration/Rewards Mean               1.49689
exploration/Rewards Std                0.483928
exploration/Rewards Max                2.8838
exploration/Rewards Min                0.760495
exploration/Returns Mean              29.9377
exploration/Returns Std               23.3982
exploration/Returns Max              178.003
exploration/Returns Min                9.61047
exploration/Actions Mean               0.444881
exploration/Actions Std                0.611462
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                 50
exploration/Average Returns           29.9377
evaluation/num steps total          5906
evaluation/num paths total           243
evaluation/path length Mean           17.386
evaluation/path length Std             0.486822
evaluation/path length Max            18
evaluation/path length Min            17
evaluation/Rewards Mean                1.49513
evaluation/Rewards Std                 0.452829
evaluation/Rewards Max                 2.1654
evaluation/Rewards Min                 0.911648
evaluation/Returns Mean               25.9942
evaluation/Returns Std                 1.022
evaluation/Returns Max                27.4921
evaluation/Returns Min                24.8431
evaluation/Actions Mean                0.597746
evaluation/Actions Std                 0.710856
evaluation/Actions Max                 1
evaluation/Actions Min                -0.995126
evaluation/Num Paths                  57
evaluation/Average Returns            25.9942
time/data storing (s)                  0.00338645
time/evaluation sampling (s)           0.398706
time/exploration sampling (s)          0.385237
time/logging (s)                       0.00416413
time/saving (s)                        0.00144351
time/training (s)                      6.02588
time/epoch (s)                         6.81881
time/total (s)                        46.5571
Epoch                                  5
---------------------------------  ---------------
2021-07-02 23:37:14.121611 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 6 finished
---------------------------------  ---------------
replay_buffer/size                 17000
trainer/QF Loss                        0.00166791
trainer/Policy Loss                  -44.5183
trainer/Raw Policy Loss              -44.5183
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean            41.3527
trainer/Q Predictions Std             12.8295
trainer/Q Predictions Max             67.201
trainer/Q Predictions Min             -2.03814
trainer/Q Targets Mean                41.1985
trainer/Q Targets Std                 13.6237
trainer/Q Targets Max                 66.7196
trainer/Q Targets Min                 -0.663267
trainer/Bellman Errors Mean           13.1319
trainer/Bellman Errors Std            64.4098
trainer/Bellman Errors Max           768.39
trainer/Bellman Errors Min             7.27376e-06
trainer/Policy Action Mean             0.481974
trainer/Policy Action Std              0.826507
trainer/Policy Action Max              1
trainer/Policy Action Min             -1
exploration/num steps total        17000
exploration/num paths total          803
exploration/path length Mean          55.5556
exploration/path length Std           19.0561
exploration/path length Max           74
exploration/path length Min           10
exploration/Rewards Mean               1.8579
exploration/Rewards Std                0.421336
exploration/Rewards Max                3.36794
exploration/Rewards Min                0.822773
exploration/Returns Mean             103.217
exploration/Returns Std               39.7277
exploration/Returns Max              145.796
exploration/Returns Min                9.96759
exploration/Actions Mean               0.437647
exploration/Actions Std                0.637396
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                 18
exploration/Average Returns          103.217
evaluation/num steps total          6895
evaluation/num paths total           258
evaluation/path length Mean           65.9333
evaluation/path length Std             0.928559
evaluation/path length Max            67
evaluation/path length Min            64
evaluation/Rewards Mean                1.86378
evaluation/Rewards Std                 0.36686
evaluation/Rewards Max                 3.35359
evaluation/Rewards Min                 0.966325
evaluation/Returns Mean              122.885
evaluation/Returns Std                 2.9286
evaluation/Returns Max               127.206
evaluation/Returns Min               117.799
evaluation/Actions Mean                0.527599
evaluation/Actions Std                 0.793398
evaluation/Actions Max                 1
evaluation/Actions Min                -0.999446
evaluation/Num Paths                  15
evaluation/Average Returns           122.885
time/data storing (s)                  0.00339328
time/evaluation sampling (s)           0.370059
time/exploration sampling (s)          0.374562
time/logging (s)                       0.00397073
time/saving (s)                        0.00164085
time/training (s)                      6.16822
time/epoch (s)                         6.92184
time/total (s)                        53.4802
Epoch                                  6
---------------------------------  ---------------
2021-07-02 23:37:21.020775 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 7 finished
---------------------------------  ---------------
replay_buffer/size                 18000
trainer/QF Loss                        0.00268994
trainer/Policy Loss                  -55.1572
trainer/Raw Policy Loss              -55.1572
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean            50.8388
trainer/Q Predictions Std             17.5089
trainer/Q Predictions Max             90.8577
trainer/Q Predictions Min             -3.43154
trainer/Q Targets Mean                50.1315
trainer/Q Targets Std                 18.2477
trainer/Q Targets Max                 91.1013
trainer/Q Targets Min                 -0.939694
trainer/Bellman Errors Mean           19.8213
trainer/Bellman Errors Std           123.797
trainer/Bellman Errors Max          1611.49
trainer/Bellman Errors Min             3.16166e-05
trainer/Policy Action Mean             0.391001
trainer/Policy Action Std              0.864015
trainer/Policy Action Max              1
trainer/Policy Action Min             -1
exploration/num steps total        18000
exploration/num paths total          816
exploration/path length Mean          76.9231
exploration/path length Std           13.4934
exploration/path length Max           87
exploration/path length Min           40
exploration/Rewards Mean               2.02066
exploration/Rewards Std                0.559365
exploration/Rewards Max                3.76943
exploration/Rewards Min                0.854911
exploration/Returns Mean             155.435
exploration/Returns Std               34.8133
exploration/Returns Max              181.216
exploration/Returns Min               68.7831
exploration/Actions Mean               0.505189
exploration/Actions Std                0.606501
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                 13
exploration/Average Returns          155.435
evaluation/num steps total          7840
evaluation/num paths total           269
evaluation/path length Mean           85.9091
evaluation/path length Std             0.514259
evaluation/path length Max            87
evaluation/path length Min            85
evaluation/Rewards Mean                2.04897
evaluation/Rewards Std                 0.503757
evaluation/Rewards Max                 3.06739
evaluation/Rewards Min                 0.877018
evaluation/Returns Mean              176.025
evaluation/Returns Std                 1.30542
evaluation/Returns Max               178.56
evaluation/Returns Min               174.229
evaluation/Actions Mean                0.649448
evaluation/Actions Std                 0.733086
evaluation/Actions Max                 1
evaluation/Actions Min                -0.999824
evaluation/Num Paths                  11
evaluation/Average Returns           176.025
time/data storing (s)                  0.00346309
time/evaluation sampling (s)           0.416478
time/exploration sampling (s)          0.400627
time/logging (s)                       0.00366492
time/saving (s)                        0.00144635
time/training (s)                      6.07141
time/epoch (s)                         6.89709
time/total (s)                        60.3787
Epoch                                  7
---------------------------------  ---------------
2021-07-02 23:37:27.916353 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 8 finished
---------------------------------  ---------------
replay_buffer/size                 19000
trainer/QF Loss                        0.00236252
trainer/Policy Loss                  -73.3628
trainer/Raw Policy Loss              -73.3628
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean            67.5552
trainer/Q Predictions Std             23.838
trainer/Q Predictions Max            149.333
trainer/Q Predictions Min             -1.63757
trainer/Q Targets Mean                67.2487
trainer/Q Targets Std                 23.5621
trainer/Q Targets Max                149.703
trainer/Q Targets Min                 -0.448485
trainer/Bellman Errors Mean           12.9334
trainer/Bellman Errors Std            32.702
trainer/Bellman Errors Max           261.093
trainer/Bellman Errors Min             0.000283007
trainer/Policy Action Mean             0.309699
trainer/Policy Action Std              0.867761
trainer/Policy Action Max              1
trainer/Policy Action Min             -1
exploration/num steps total        19000
exploration/num paths total          831
exploration/path length Mean          66.6667
exploration/path length Std           23.0352
exploration/path length Max           93
exploration/path length Min           25
exploration/Rewards Mean               1.97513
exploration/Rewards Std                0.591116
exploration/Rewards Max                3.81689
exploration/Rewards Min                0.708371
exploration/Returns Mean             131.675
exploration/Returns Std               54.2469
exploration/Returns Max              211.048
exploration/Returns Min               40.148
exploration/Actions Mean               0.280933
exploration/Actions Std                0.655002
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                 15
exploration/Average Returns          131.675
evaluation/num steps total          8822
evaluation/num paths total           281
evaluation/path length Mean           81.8333
evaluation/path length Std             0.897527
evaluation/path length Max            84
evaluation/path length Min            81
evaluation/Rewards Mean                1.97743
evaluation/Rewards Std                 0.443948
evaluation/Rewards Max                 2.75369
evaluation/Rewards Min                 0.769966
evaluation/Returns Mean              161.82
evaluation/Returns Std                 1.68243
evaluation/Returns Max               165.597
evaluation/Returns Min               158.994
evaluation/Actions Mean                0.344926
evaluation/Actions Std                 0.810217
evaluation/Actions Max                 1
evaluation/Actions Min                -0.999992
evaluation/Num Paths                  12
evaluation/Average Returns           161.82
time/data storing (s)                  0.00339506
time/evaluation sampling (s)           0.397794
time/exploration sampling (s)          0.375837
time/logging (s)                       0.00380734
time/saving (s)                        0.00160316
time/training (s)                      6.11172
time/epoch (s)                         6.89415
time/total (s)                        67.2741
Epoch                                  8
---------------------------------  ---------------
2021-07-02 23:37:34.748866 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 9 finished
---------------------------------  --------------
replay_buffer/size                 20000
trainer/QF Loss                        0.00244617
trainer/Policy Loss                 -107.425
trainer/Raw Policy Loss             -107.425
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean            96.7503
trainer/Q Predictions Std             31.1984
trainer/Q Predictions Max            176.139
trainer/Q Predictions Min             -7.93432
trainer/Q Targets Mean                96.6087
trainer/Q Targets Std                 31.6568
trainer/Q Targets Max                165.307
trainer/Q Targets Min                 -1.20525
trainer/Bellman Errors Mean           48.7906
trainer/Bellman Errors Std           222.012
trainer/Bellman Errors Max          3195.4
trainer/Bellman Errors Min             1.5847e-06
trainer/Policy Action Mean            -0.431679
trainer/Policy Action Std              0.827353
trainer/Policy Action Max              1
trainer/Policy Action Min             -1
exploration/num steps total        20000
exploration/num paths total          862
exploration/path length Mean          32.2581
exploration/path length Std            9.1439
exploration/path length Max           50
exploration/path length Min            6
exploration/Rewards Mean               0.309696
exploration/Rewards Std                0.24978
exploration/Rewards Max                0.96031
exploration/Rewards Min               -0.544034
exploration/Returns Mean               9.99019
exploration/Returns Std                3.13779
exploration/Returns Max               20.0829
exploration/Returns Min                4.08886
exploration/Actions Mean               0.0403368
exploration/Actions Std                0.65027
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                 31
exploration/Average Returns            9.99019
evaluation/num steps total          9806
evaluation/num paths total           306
evaluation/path length Mean           39.36
evaluation/path length Std             1.16207
evaluation/path length Max            41
evaluation/path length Min            37
evaluation/Rewards Mean                0.268711
evaluation/Rewards Std                 0.236484
evaluation/Rewards Max                 0.932805
evaluation/Rewards Min                -0.195409
evaluation/Returns Mean               10.5765
evaluation/Returns Std                 0.214426
evaluation/Returns Max                10.9844
evaluation/Returns Min                10.2003
evaluation/Actions Mean                0.0401226
evaluation/Actions Std                 0.758476
evaluation/Actions Max                 0.998312
evaluation/Actions Min                -1
evaluation/Num Paths                  25
evaluation/Average Returns            10.5765
time/data storing (s)                  0.00359737
time/evaluation sampling (s)           0.364842
time/exploration sampling (s)          0.391068
time/logging (s)                       0.00410175
time/saving (s)                        0.00147048
time/training (s)                      6.06604
time/epoch (s)                         6.83112
time/total (s)                        74.1066
Epoch                                  9
---------------------------------  --------------
2021-07-02 23:37:41.656609 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 10 finished
---------------------------------  ---------------
replay_buffer/size                 21000
trainer/QF Loss                        0.0016373
trainer/Policy Loss                 -140.963
trainer/Raw Policy Loss             -140.963
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean           127.58
trainer/Q Predictions Std             39.5685
trainer/Q Predictions Max            231.945
trainer/Q Predictions Min             -1.82342
trainer/Q Targets Mean               128.274
trainer/Q Targets Std                 43.3709
trainer/Q Targets Max                226.833
trainer/Q Targets Min                 -0.636366
trainer/Bellman Errors Mean          173.937
trainer/Bellman Errors Std           961.863
trainer/Bellman Errors Max         10210.3
trainer/Bellman Errors Min             7.17177e-07
trainer/Policy Action Mean            -0.339696
trainer/Policy Action Std              0.85444
trainer/Policy Action Max              1
trainer/Policy Action Min             -1
exploration/num steps total        21000
exploration/num paths total          869
exploration/path length Mean         142.857
exploration/path length Std           69.6715
exploration/path length Max          236
exploration/path length Min           28
exploration/Rewards Mean               0.896614
exploration/Rewards Std                0.204566
exploration/Rewards Max                1.55609
exploration/Rewards Min                0.0647903
exploration/Returns Mean             128.088
exploration/Returns Std               64.8318
exploration/Returns Max              222.01
exploration/Returns Min               22.4654
exploration/Actions Mean               0.0516032
exploration/Actions Std                0.551914
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                  7
exploration/Average Returns          128.088
evaluation/num steps total         10806
evaluation/num paths total           307
evaluation/path length Mean         1000
evaluation/path length Std             0
evaluation/path length Max          1000
evaluation/path length Min          1000
evaluation/Rewards Mean                1.00252
evaluation/Rewards Std                 0.0315577
evaluation/Rewards Max                 1.06394
evaluation/Rewards Min                 0.640401
evaluation/Returns Mean             1002.52
evaluation/Returns Std                 0
evaluation/Returns Max              1002.52
evaluation/Returns Min              1002.52
evaluation/Actions Mean               -0.00408055
evaluation/Actions Std                 0.390424
evaluation/Actions Max                 0.950754
evaluation/Actions Min                -0.999997
evaluation/Num Paths                   1
evaluation/Average Returns          1002.52
time/data storing (s)                  0.00341125
time/evaluation sampling (s)           0.370873
time/exploration sampling (s)          0.384827
time/logging (s)                       0.00375629
time/saving (s)                        0.00146868
time/training (s)                      6.14111
time/epoch (s)                         6.90545
time/total (s)                        81.0137
Epoch                                 10
---------------------------------  ---------------
2021-07-02 23:37:48.422936 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 11 finished
---------------------------------  ---------------
replay_buffer/size                 22000
trainer/QF Loss                        0.00155931
trainer/Policy Loss                 -176.366
trainer/Raw Policy Loss             -176.366
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean           161.961
trainer/Q Predictions Std             57.0614
trainer/Q Predictions Max            268.256
trainer/Q Predictions Min             -7.4901
trainer/Q Targets Mean               160.66
trainer/Q Targets Std                 60.304
trainer/Q Targets Max                262.436
trainer/Q Targets Min                 -5.62243
trainer/Bellman Errors Mean          273.201
trainer/Bellman Errors Std          1687.96
trainer/Bellman Errors Max         15622.3
trainer/Bellman Errors Min             0.000118696
trainer/Policy Action Mean            -0.0512069
trainer/Policy Action Std              0.900627
trainer/Policy Action Max              1
trainer/Policy Action Min             -1
exploration/num steps total        22000
exploration/num paths total          877
exploration/path length Mean         125
exploration/path length Std           96.013
exploration/path length Max          273
exploration/path length Min           15
exploration/Rewards Mean               1.08369
exploration/Rewards Std                0.373647
exploration/Rewards Max                1.91621
exploration/Rewards Min               -0.00234427
exploration/Returns Mean             135.461
exploration/Returns Std              102.319
exploration/Returns Max              303.431
exploration/Returns Min               13.8401
exploration/Actions Mean               0.128129
exploration/Actions Std                0.666564
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                  8
exploration/Average Returns          135.461
evaluation/num steps total         11806
evaluation/num paths total           308
evaluation/path length Mean         1000
evaluation/path length Std             0
evaluation/path length Max          1000
evaluation/path length Min          1000
evaluation/Rewards Mean                1.15025
evaluation/Rewards Std                 0.236524
evaluation/Rewards Max                 1.72977
evaluation/Rewards Min                 0.456765
evaluation/Returns Mean             1150.25
evaluation/Returns Std                 0
evaluation/Returns Max              1150.25
evaluation/Returns Min              1150.25
evaluation/Actions Mean                0.126658
evaluation/Actions Std                 0.690942
evaluation/Actions Max                 1
evaluation/Actions Min                -1
evaluation/Num Paths                   1
evaluation/Average Returns          1150.25
time/data storing (s)                  0.00338682
time/evaluation sampling (s)           0.37013
time/exploration sampling (s)          0.38189
time/logging (s)                       0.00407408
time/saving (s)                        0.00198667
time/training (s)                      6.00334
time/epoch (s)                         6.76481
time/total (s)                        87.78
Epoch                                 11
---------------------------------  ---------------
2021-07-02 23:37:55.229711 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 12 finished
---------------------------------  --------------
replay_buffer/size                 23000
trainer/QF Loss                        0.00224376
trainer/Policy Loss                 -217.52
trainer/Raw Policy Loss             -217.52
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean           200.86
trainer/Q Predictions Std             66.6038
trainer/Q Predictions Max            327.288
trainer/Q Predictions Min            -11.1602
trainer/Q Targets Mean               198.348
trainer/Q Targets Std                 68.9784
trainer/Q Targets Max                323.373
trainer/Q Targets Min                 -1.29255
trainer/Bellman Errors Mean          363.564
trainer/Bellman Errors Std          2479
trainer/Bellman Errors Max         29336.8
trainer/Bellman Errors Min             4.9267e-07
trainer/Policy Action Mean             0.0990436
trainer/Policy Action Std              0.898114
trainer/Policy Action Max              1
trainer/Policy Action Min             -1
exploration/num steps total        23000
exploration/num paths total          887
exploration/path length Mean         100
exploration/path length Std           42.0928
exploration/path length Max          184
exploration/path length Min           39
exploration/Rewards Mean               1.84296
exploration/Rewards Std                0.498006
exploration/Rewards Max                3.3706
exploration/Rewards Min                0.62105
exploration/Returns Mean             184.296
exploration/Returns Std               98.4403
exploration/Returns Max              372.155
exploration/Returns Min               59.9698
exploration/Actions Mean              -0.024605
exploration/Actions Std                0.654111
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                 10
exploration/Average Returns          184.296
evaluation/num steps total         12805
evaluation/num paths total           313
evaluation/path length Mean          199.8
evaluation/path length Std             3.76298
evaluation/path length Max           204
evaluation/path length Min           194
evaluation/Rewards Mean                1.8986
evaluation/Rewards Std                 0.453996
evaluation/Rewards Max                 2.83331
evaluation/Rewards Min                 0.721465
evaluation/Returns Mean              379.34
evaluation/Returns Std                 7.83904
evaluation/Returns Max               389.886
evaluation/Returns Min               370.31
evaluation/Actions Mean               -0.0922792
evaluation/Actions Std                 0.842744
evaluation/Actions Max                 1
evaluation/Actions Min                -1
evaluation/Num Paths                   5
evaluation/Average Returns           379.34
time/data storing (s)                  0.00340694
time/evaluation sampling (s)           0.356628
time/exploration sampling (s)          0.383635
time/logging (s)                       0.00377344
time/saving (s)                        0.00147363
time/training (s)                      6.05547
time/epoch (s)                         6.80439
time/total (s)                        94.5861
Epoch                                 12
---------------------------------  --------------
2021-07-02 23:38:02.057425 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 13 finished
---------------------------------  --------------
replay_buffer/size                 24000
trainer/QF Loss                        0.00213805
trainer/Policy Loss                 -261.178
trainer/Raw Policy Loss             -261.178
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean           242.435
trainer/Q Predictions Std             85.3332
trainer/Q Predictions Max            492.625
trainer/Q Predictions Min            -20.8073
trainer/Q Targets Mean               241.983
trainer/Q Targets Std                 88.5126
trainer/Q Targets Max                468.65
trainer/Q Targets Min                -10.3855
trainer/Bellman Errors Mean          438.212
trainer/Bellman Errors Std          2962.79
trainer/Bellman Errors Max         36012.3
trainer/Bellman Errors Min             0.00397714
trainer/Policy Action Mean             0.216122
trainer/Policy Action Std              0.863182
trainer/Policy Action Max              1
trainer/Policy Action Min             -1
exploration/num steps total        24000
exploration/num paths total          893
exploration/path length Mean         166.667
exploration/path length Std           27.3719
exploration/path length Max          201
exploration/path length Min          130
exploration/Rewards Mean               2.46054
exploration/Rewards Std                0.703899
exploration/Rewards Max                3.94489
exploration/Rewards Min                0.714808
exploration/Returns Mean             410.09
exploration/Returns Std               87.2598
exploration/Returns Max              520.109
exploration/Returns Min              260.655
exploration/Actions Mean              -0.020662
exploration/Actions Std                0.682842
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                  6
exploration/Average Returns          410.09
evaluation/num steps total         13636
evaluation/num paths total           317
evaluation/path length Mean          207.75
evaluation/path length Std             6.21993
evaluation/path length Max           212
evaluation/path length Min           197
evaluation/Rewards Mean                2.66084
evaluation/Rewards Std                 0.84295
evaluation/Rewards Max                 4.31356
evaluation/Rewards Min                 0.804716
evaluation/Returns Mean              552.79
evaluation/Returns Std                18.9236
evaluation/Returns Max               582.723
evaluation/Returns Min               532.948
evaluation/Actions Mean               -0.0901957
evaluation/Actions Std                 0.792445
evaluation/Actions Max                 0.999999
evaluation/Actions Min                -1
evaluation/Num Paths                   4
evaluation/Average Returns           552.79
time/data storing (s)                  0.00337347
time/evaluation sampling (s)           0.352602
time/exploration sampling (s)          0.373642
time/logging (s)                       0.00363434
time/saving (s)                        0.00145049
time/training (s)                      6.09108
time/epoch (s)                         6.82578
time/total (s)                       101.413
Epoch                                 13
---------------------------------  --------------
2021-07-02 23:38:08.872091 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 14 finished
---------------------------------  --------------
replay_buffer/size                 25000
trainer/QF Loss                        0.00221163
trainer/Policy Loss                 -309.378
trainer/Raw Policy Loss             -309.378
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean           290.627
trainer/Q Predictions Std             83.6518
trainer/Q Predictions Max            563.334
trainer/Q Predictions Min              0.460707
trainer/Q Targets Mean               285.407
trainer/Q Targets Std                 88.9789
trainer/Q Targets Max                539.874
trainer/Q Targets Min                 -0.0745214
trainer/Bellman Errors Mean          723.849
trainer/Bellman Errors Std          3812.1
trainer/Bellman Errors Max         49152.7
trainer/Bellman Errors Min             0.00215173
trainer/Policy Action Mean             0.209366
trainer/Policy Action Std              0.868479
trainer/Policy Action Max              1
trainer/Policy Action Min             -1
exploration/num steps total        25000
exploration/num paths total          900
exploration/path length Mean         142.857
exploration/path length Std           65.571
exploration/path length Max          267
exploration/path length Min           59
exploration/Rewards Mean               1.91881
exploration/Rewards Std                0.469861
exploration/Rewards Max                3.15111
exploration/Rewards Min                0.899383
exploration/Returns Mean             274.116
exploration/Returns Std              111.91
exploration/Returns Max              470.75
exploration/Returns Min              106.467
exploration/Actions Mean               0.148628
exploration/Actions Std                0.683455
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                  7
exploration/Average Returns          274.116
evaluation/num steps total         14486
evaluation/num paths total           322
evaluation/path length Mean          170
evaluation/path length Std            59.3431
evaluation/path length Max           249
evaluation/path length Min           121
evaluation/Rewards Mean                1.68448
evaluation/Rewards Std                 0.336315
evaluation/Rewards Max                 2.74951
evaluation/Rewards Min                 0.899758
evaluation/Returns Mean              286.361
evaluation/Returns Std               115.925
evaluation/Returns Max               451.702
evaluation/Returns Min               190.298
evaluation/Actions Mean                0.270871
evaluation/Actions Std                 0.716051
evaluation/Actions Max                 1
evaluation/Actions Min                -0.999996
evaluation/Num Paths                   5
evaluation/Average Returns           286.361
time/data storing (s)                  0.00332001
time/evaluation sampling (s)           0.358468
time/exploration sampling (s)          0.371473
time/logging (s)                       0.00364402
time/saving (s)                        0.00154257
time/training (s)                      6.0745
time/epoch (s)                         6.81295
time/total (s)                       108.228
Epoch                                 14
---------------------------------  --------------
2021-07-02 23:38:15.701849 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 15 finished
---------------------------------  ----------------
replay_buffer/size                  26000
trainer/QF Loss                         0.0021365
trainer/Policy Loss                  -346.475
trainer/Raw Policy Loss              -346.475
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean            323.186
trainer/Q Predictions Std              97.1822
trainer/Q Predictions Max             661.249
trainer/Q Predictions Min             -10.2916
trainer/Q Targets Mean                323.963
trainer/Q Targets Std                 105.976
trainer/Q Targets Max                 644.117
trainer/Q Targets Min                  -1.45718
trainer/Bellman Errors Mean          1556.75
trainer/Bellman Errors Std          10032.7
trainer/Bellman Errors Max         128862
trainer/Bellman Errors Min              0.000643135
trainer/Policy Action Mean              0.15793
trainer/Policy Action Std               0.876557
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         26000
exploration/num paths total           910
exploration/path length Mean          100
exploration/path length Std            59.3818
exploration/path length Max           162
exploration/path length Min            12
exploration/Rewards Mean                1.95915
exploration/Rewards Std                 0.803236
exploration/Rewards Max                 3.90148
exploration/Rewards Min                 0.534542
exploration/Returns Mean              195.915
exploration/Returns Std               140.737
exploration/Returns Max               404.316
exploration/Returns Min                11.0757
exploration/Actions Mean                0.0557163
exploration/Actions Std                 0.71355
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  10
exploration/Average Returns           195.915
evaluation/num steps total          15353
evaluation/num paths total            326
evaluation/path length Mean           216.75
evaluation/path length Std             79.2161
evaluation/path length Max            347
evaluation/path length Min            153
evaluation/Rewards Mean                 1.5544
evaluation/Rewards Std                  0.469848
evaluation/Rewards Max                  2.89396
evaluation/Rewards Min                  0.652241
evaluation/Returns Mean               336.916
evaluation/Returns Std                132.305
evaluation/Returns Max                542.374
evaluation/Returns Min                219.725
evaluation/Actions Mean                 0.334651
evaluation/Actions Std                  0.714726
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    4
evaluation/Average Returns            336.916
time/data storing (s)                   0.00342194
time/evaluation sampling (s)            0.373533
time/exploration sampling (s)           0.377277
time/logging (s)                        0.00439019
time/saving (s)                         0.00153095
time/training (s)                       6.06866
time/epoch (s)                          6.82881
time/total (s)                        115.058
Epoch                                  15
---------------------------------  ----------------
2021-07-02 23:38:23.833973 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 16 finished
---------------------------------  ---------------
replay_buffer/size                 27000
trainer/QF Loss                        0.00174525
trainer/Policy Loss                 -403.843
trainer/Raw Policy Loss             -403.843
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean           379.099
trainer/Q Predictions Std            118.218
trainer/Q Predictions Max            782.606
trainer/Q Predictions Min            -15.9614
trainer/Q Targets Mean               378.794
trainer/Q Targets Std                126.306
trainer/Q Targets Max                776.97
trainer/Q Targets Min                 -4.37403
trainer/Bellman Errors Mean         1227.2
trainer/Bellman Errors Std          7843.17
trainer/Bellman Errors Max         96890.6
trainer/Bellman Errors Min             0.000649341
trainer/Policy Action Mean             0.144357
trainer/Policy Action Std              0.876647
trainer/Policy Action Max              1
trainer/Policy Action Min             -1
exploration/num steps total        27000
exploration/num paths total          921
exploration/path length Mean          90.9091
exploration/path length Std           37.1152
exploration/path length Max          157
exploration/path length Min           10
exploration/Rewards Mean               1.70672
exploration/Rewards Std                0.665395
exploration/Rewards Max                3.8955
exploration/Rewards Min                0.588448
exploration/Returns Mean             155.157
exploration/Returns Std               91.0681
exploration/Returns Max              391.39
exploration/Returns Min                9.64111
exploration/Actions Mean               0.060325
exploration/Actions Std                0.686695
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                 11
exploration/Average Returns          155.157
evaluation/num steps total         16227
evaluation/num paths total           331
evaluation/path length Mean          174.8
evaluation/path length Std            34.0317
evaluation/path length Max           221
evaluation/path length Min           134
evaluation/Rewards Mean                1.74906
evaluation/Rewards Std                 0.471777
evaluation/Rewards Max                 3.12009
evaluation/Rewards Min                 0.837155
evaluation/Returns Mean              305.735
evaluation/Returns Std                57.5739
evaluation/Returns Max               366.748
evaluation/Returns Min               224.631
evaluation/Actions Mean                0.103233
evaluation/Actions Std                 0.801628
evaluation/Actions Max                 1
evaluation/Actions Min                -1
evaluation/Num Paths                   5
evaluation/Average Returns           305.735
time/data storing (s)                  0.00352834
time/evaluation sampling (s)           0.375923
time/exploration sampling (s)          0.410384
time/logging (s)                       0.00584326
time/saving (s)                        0.00206786
time/training (s)                      7.33415
time/epoch (s)                         8.1319
time/total (s)                       123.191
Epoch                                 16
---------------------------------  ---------------
2021-07-02 23:38:31.790177 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 17 finished
---------------------------------  ---------------
replay_buffer/size                 28000
trainer/QF Loss                        0.00165277
trainer/Policy Loss                 -451.727
trainer/Raw Policy Loss             -451.727
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean           424.891
trainer/Q Predictions Std            123.995
trainer/Q Predictions Max            704.635
trainer/Q Predictions Min             -5.51853
trainer/Q Targets Mean               425.091
trainer/Q Targets Std                125.421
trainer/Q Targets Max                715.06
trainer/Q Targets Min                 -3.08386
trainer/Bellman Errors Mean          856.288
trainer/Bellman Errors Std          4844.26
trainer/Bellman Errors Max         61555
trainer/Bellman Errors Min             4.96302e-06
trainer/Policy Action Mean             0.223422
trainer/Policy Action Std              0.830593
trainer/Policy Action Max              1
trainer/Policy Action Min             -1
exploration/num steps total        28000
exploration/num paths total          928
exploration/path length Mean         142.857
exploration/path length Std           62.4693
exploration/path length Max          225
exploration/path length Min           42
exploration/Rewards Mean               1.89583
exploration/Rewards Std                0.627696
exploration/Rewards Max                3.97926
exploration/Rewards Min                0.514938
exploration/Returns Mean             270.832
exploration/Returns Std              130.859
exploration/Returns Max              428.75
exploration/Returns Min               66.6066
exploration/Actions Mean               0.0870178
exploration/Actions Std                0.711742
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                  7
exploration/Average Returns          270.832
evaluation/num steps total         17140
evaluation/num paths total           337
evaluation/path length Mean          152.167
evaluation/path length Std             3.3375
evaluation/path length Max           155
evaluation/path length Min           147
evaluation/Rewards Mean                2.00681
evaluation/Rewards Std                 0.477122
evaluation/Rewards Max                 3.11041
evaluation/Rewards Min                 0.715314
evaluation/Returns Mean              305.37
evaluation/Returns Std                17.0708
evaluation/Returns Max               320.614
evaluation/Returns Min               273.469
evaluation/Actions Mean                0.227504
evaluation/Actions Std                 0.801991
evaluation/Actions Max                 1
evaluation/Actions Min                -1
evaluation/Num Paths                   6
evaluation/Average Returns           305.37
time/data storing (s)                  0.00353672
time/evaluation sampling (s)           0.410511
time/exploration sampling (s)          0.402325
time/logging (s)                       0.00384999
time/saving (s)                        0.00156273
time/training (s)                      7.12969
time/epoch (s)                         7.95147
time/total (s)                       131.145
Epoch                                 17
---------------------------------  ---------------
2021-07-02 23:38:40.358254 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 18 finished
---------------------------------  --------------
replay_buffer/size                 29000
trainer/QF Loss                        0.0013055
trainer/Policy Loss                 -498.371
trainer/Raw Policy Loss             -498.371
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean           465.819
trainer/Q Predictions Std            130.079
trainer/Q Predictions Max            765.51
trainer/Q Predictions Min             -4.33739
trainer/Q Targets Mean               471.288
trainer/Q Targets Std                135.724
trainer/Q Targets Max                765.479
trainer/Q Targets Min                 -0.272077
trainer/Bellman Errors Mean         1209.27
trainer/Bellman Errors Std          6467.73
trainer/Bellman Errors Max         68698
trainer/Bellman Errors Min             0.00398292
trainer/Policy Action Mean             0.248607
trainer/Policy Action Std              0.815695
trainer/Policy Action Max              1
trainer/Policy Action Min             -1
exploration/num steps total        29000
exploration/num paths total          937
exploration/path length Mean         111.111
exploration/path length Std           63.5688
exploration/path length Max          203
exploration/path length Min            3
exploration/Rewards Mean               2.28883
exploration/Rewards Std                0.86522
exploration/Rewards Max                4.57366
exploration/Rewards Min                0.769778
exploration/Returns Mean             254.315
exploration/Returns Std              177.499
exploration/Returns Max              586.265
exploration/Returns Min                2.60935
exploration/Actions Mean               0.0272267
exploration/Actions Std                0.709696
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                  9
exploration/Average Returns          254.315
evaluation/num steps total         18114
evaluation/num paths total           340
evaluation/path length Mean          324.667
evaluation/path length Std           143.704
evaluation/path length Max           519
evaluation/path length Min           176
evaluation/Rewards Mean                2.80923
evaluation/Rewards Std                 0.830686
evaluation/Rewards Max                 4.82558
evaluation/Rewards Min                 0.837288
evaluation/Returns Mean              912.062
evaluation/Returns Std               395.866
evaluation/Returns Max              1435.49
evaluation/Returns Min               478.36
evaluation/Actions Mean               -0.0212034
evaluation/Actions Std                 0.80885
evaluation/Actions Max                 1
evaluation/Actions Min                -1
evaluation/Num Paths                   3
evaluation/Average Returns           912.062
time/data storing (s)                  0.00333978
time/evaluation sampling (s)           0.414701
time/exploration sampling (s)          0.437197
time/logging (s)                       0.00588645
time/saving (s)                        0.00233428
time/training (s)                      7.70459
time/epoch (s)                         8.56805
time/total (s)                       139.714
Epoch                                 18
---------------------------------  --------------
2021-07-02 23:38:48.886440 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 19 finished
---------------------------------  ---------------
replay_buffer/size                  30000
trainer/QF Loss                         0.00190938
trainer/Policy Loss                  -561.717
trainer/Raw Policy Loss              -561.717
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean            527.231
trainer/Q Predictions Std             150.527
trainer/Q Predictions Max             843.356
trainer/Q Predictions Min             -21.842
trainer/Q Targets Mean                526.172
trainer/Q Targets Std                 158.362
trainer/Q Targets Max                 831.753
trainer/Q Targets Min                  -0.609551
trainer/Bellman Errors Mean          1921.14
trainer/Bellman Errors Std          12208.1
trainer/Bellman Errors Max         159689
trainer/Bellman Errors Min              0.0498752
trainer/Policy Action Mean              0.124877
trainer/Policy Action Std               0.849324
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         30000
exploration/num paths total           947
exploration/path length Mean          100
exploration/path length Std            47.9082
exploration/path length Max           200
exploration/path length Min            36
exploration/Rewards Mean                1.57074
exploration/Rewards Std                 0.527561
exploration/Rewards Max                 2.71929
exploration/Rewards Min                 0.0943433
exploration/Returns Mean              157.074
exploration/Returns Std                79.9186
exploration/Returns Max               266.216
exploration/Returns Min                60.5811
exploration/Actions Mean                0.182795
exploration/Actions Std                 0.679165
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  10
exploration/Average Returns           157.074
evaluation/num steps total          18923
evaluation/num paths total            344
evaluation/path length Mean           202.25
evaluation/path length Std             60.1223
evaluation/path length Max            265
evaluation/path length Min            103
evaluation/Rewards Mean                 1.80016
evaluation/Rewards Std                  0.877537
evaluation/Rewards Max                  4.6231
evaluation/Rewards Min                  0.605615
evaluation/Returns Mean               364.082
evaluation/Returns Std                170.628
evaluation/Returns Max                614.188
evaluation/Returns Min                133.799
evaluation/Actions Mean                 0.285853
evaluation/Actions Std                  0.742571
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    4
evaluation/Average Returns            364.082
time/data storing (s)                   0.00443477
time/evaluation sampling (s)            0.483727
time/exploration sampling (s)           0.403412
time/logging (s)                        0.00378534
time/saving (s)                         0.0016655
time/training (s)                       7.62642
time/epoch (s)                          8.52344
time/total (s)                        148.24
Epoch                                  19
---------------------------------  ---------------
2021-07-02 23:38:56.650201 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 20 finished
---------------------------------  ----------------
replay_buffer/size                  31000
trainer/QF Loss                         0.00160315
trainer/Policy Loss                  -631.458
trainer/Raw Policy Loss              -631.458
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean            592.951
trainer/Q Predictions Std             168.955
trainer/Q Predictions Max             943.132
trainer/Q Predictions Min             -22.6871
trainer/Q Targets Mean                592.675
trainer/Q Targets Std                 176.073
trainer/Q Targets Max                 918.585
trainer/Q Targets Min                  -0.501413
trainer/Bellman Errors Mean          2621.26
trainer/Bellman Errors Std          18370.4
trainer/Bellman Errors Max         267901
trainer/Bellman Errors Min              8.95001e-05
trainer/Policy Action Mean              0.324454
trainer/Policy Action Std               0.813658
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         31000
exploration/num paths total           958
exploration/path length Mean           90.9091
exploration/path length Std            52.9347
exploration/path length Max           185
exploration/path length Min            14
exploration/Rewards Mean                2.40707
exploration/Rewards Std                 0.901769
exploration/Rewards Max                 4.52736
exploration/Rewards Min                 0.538277
exploration/Returns Mean              218.824
exploration/Returns Std               140.332
exploration/Returns Max               427.495
exploration/Returns Min                12.0593
exploration/Actions Mean                0.207301
exploration/Actions Std                 0.679258
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  11
exploration/Average Returns           218.824
evaluation/num steps total          19881
evaluation/num paths total            355
evaluation/path length Mean            87.0909
evaluation/path length Std              0.792527
evaluation/path length Max             88
evaluation/path length Min             86
evaluation/Rewards Mean                 2.29397
evaluation/Rewards Std                  0.739341
evaluation/Rewards Max                  3.42658
evaluation/Rewards Min                  0.730926
evaluation/Returns Mean               199.784
evaluation/Returns Std                  2.26426
evaluation/Returns Max                203.044
evaluation/Returns Min                196.487
evaluation/Actions Mean                 0.400069
evaluation/Actions Std                  0.687529
evaluation/Actions Max                  1
evaluation/Actions Min                 -0.998914
evaluation/Num Paths                   11
evaluation/Average Returns            199.784
time/data storing (s)                   0.00355257
time/evaluation sampling (s)            0.389186
time/exploration sampling (s)           0.405772
time/logging (s)                        0.00402462
time/saving (s)                         0.00160041
time/training (s)                       6.95772
time/epoch (s)                          7.76186
time/total (s)                        156.003
Epoch                                  20
---------------------------------  ----------------
2021-07-02 23:39:04.753205 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 21 finished
---------------------------------  ---------------
replay_buffer/size                  32000
trainer/QF Loss                         0.00135335
trainer/Policy Loss                  -720.725
trainer/Raw Policy Loss              -720.725
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean            685.882
trainer/Q Predictions Std             172.657
trainer/Q Predictions Max            1012.62
trainer/Q Predictions Min               2.32737
trainer/Q Targets Mean                681.547
trainer/Q Targets Std                 187.868
trainer/Q Targets Max                1005.36
trainer/Q Targets Min                  -5.85293
trainer/Bellman Errors Mean          3306.43
trainer/Bellman Errors Std          24438.5
trainer/Bellman Errors Max         288454
trainer/Bellman Errors Min              0.0177691
trainer/Policy Action Mean              0.289115
trainer/Policy Action Std               0.793769
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         32000
exploration/num paths total           969
exploration/path length Mean           90.9091
exploration/path length Std            35.1838
exploration/path length Max           146
exploration/path length Min             5
exploration/Rewards Mean                2.03836
exploration/Rewards Std                 0.823751
exploration/Rewards Max                 4.21299
exploration/Rewards Min                 0.253731
exploration/Returns Mean              185.305
exploration/Returns Std                91.5217
exploration/Returns Max               302.497
exploration/Returns Min                 4.29976
exploration/Actions Mean                0.212105
exploration/Actions Std                 0.685769
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  11
exploration/Average Returns           185.305
evaluation/num steps total          20813
evaluation/num paths total            366
evaluation/path length Mean            84.7273
evaluation/path length Std              0.749656
evaluation/path length Max             86
evaluation/path length Min             83
evaluation/Rewards Mean                 2.26901
evaluation/Rewards Std                  0.725638
evaluation/Rewards Max                  3.372
evaluation/Rewards Min                  0.745847
evaluation/Returns Mean               192.247
evaluation/Returns Std                  2.25079
evaluation/Returns Max                195.447
evaluation/Returns Min                187.445
evaluation/Actions Mean                 0.418949
evaluation/Actions Std                  0.553927
evaluation/Actions Max                  1
evaluation/Actions Min                 -0.996569
evaluation/Num Paths                   11
evaluation/Average Returns            192.247
time/data storing (s)                   0.00348999
time/evaluation sampling (s)            0.435721
time/exploration sampling (s)           0.390935
time/logging (s)                        0.0059683
time/saving (s)                         0.00201629
time/training (s)                       7.26451
time/epoch (s)                          8.10264
time/total (s)                        164.108
Epoch                                  21
---------------------------------  ---------------
2021-07-02 23:39:12.917856 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 22 finished
---------------------------------  ---------------
replay_buffer/size                  33000
trainer/QF Loss                         0.0015735
trainer/Policy Loss                  -754.977
trainer/Raw Policy Loss              -754.977
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean            713.38
trainer/Q Predictions Std             212.93
trainer/Q Predictions Max            1036.55
trainer/Q Predictions Min             -31.7718
trainer/Q Targets Mean                708.126
trainer/Q Targets Std                 225.365
trainer/Q Targets Max                 999.505
trainer/Q Targets Min                  -7.11471
trainer/Bellman Errors Mean          4160.71
trainer/Bellman Errors Std          20545.8
trainer/Bellman Errors Max         183330
trainer/Bellman Errors Min              0.00825942
trainer/Policy Action Mean              0.139173
trainer/Policy Action Std               0.818391
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         33000
exploration/num paths total           977
exploration/path length Mean          125
exploration/path length Std            35.8922
exploration/path length Max           162
exploration/path length Min            71
exploration/Rewards Mean                1.94735
exploration/Rewards Std                 0.95105
exploration/Rewards Max                 4.97929
exploration/Rewards Min                -0.404139
exploration/Returns Mean              243.419
exploration/Returns Std               122.058
exploration/Returns Max               439.868
exploration/Returns Min                93.1637
exploration/Actions Mean                0.158856
exploration/Actions Std                 0.707453
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                   8
exploration/Average Returns           243.419
evaluation/num steps total          21805
evaluation/num paths total            375
evaluation/path length Mean           110.222
evaluation/path length Std              7.52445
evaluation/path length Max            114
evaluation/path length Min             89
evaluation/Rewards Mean                 2.62635
evaluation/Rewards Std                  1.00097
evaluation/Rewards Max                  4.64834
evaluation/Rewards Min                  0.95097
evaluation/Returns Mean               289.482
evaluation/Returns Std                 29.3806
evaluation/Returns Max                301.493
evaluation/Returns Min                206.48
evaluation/Actions Mean                 0.256017
evaluation/Actions Std                  0.7177
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    9
evaluation/Average Returns            289.482
time/data storing (s)                   0.00347753
time/evaluation sampling (s)            0.433767
time/exploration sampling (s)           0.390955
time/logging (s)                        0.00602357
time/saving (s)                         0.00205056
time/training (s)                       7.32562
time/epoch (s)                          8.16189
time/total (s)                        172.272
Epoch                                  22
---------------------------------  ---------------
2021-07-02 23:39:20.965502 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 23 finished
---------------------------------  ----------------
replay_buffer/size                  34000
trainer/QF Loss                         0.00150063
trainer/Policy Loss                  -835.064
trainer/Raw Policy Loss              -835.064
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean            795.8
trainer/Q Predictions Std             202.41
trainer/Q Predictions Max            1145.2
trainer/Q Predictions Min               0.20449
trainer/Q Targets Mean                799.185
trainer/Q Targets Std                 199.918
trainer/Q Targets Max                1129.48
trainer/Q Targets Min                  -0.704645
trainer/Bellman Errors Mean          2888.92
trainer/Bellman Errors Std          16597.1
trainer/Bellman Errors Max         229124
trainer/Bellman Errors Min              2.15173e-05
trainer/Policy Action Mean              0.358601
trainer/Policy Action Std               0.766178
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         34000
exploration/num paths total           988
exploration/path length Mean           90.9091
exploration/path length Std            32.8923
exploration/path length Max           148
exploration/path length Min            26
exploration/Rewards Mean                2.13952
exploration/Rewards Std                 0.71227
exploration/Rewards Max                 4.89883
exploration/Rewards Min                 0.897624
exploration/Returns Mean              194.502
exploration/Returns Std                90.1437
exploration/Returns Max               347.904
exploration/Returns Min                40.1906
exploration/Actions Mean                0.234908
exploration/Actions Std                 0.660855
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  11
exploration/Average Returns           194.502
evaluation/num steps total          22728
evaluation/num paths total            385
evaluation/path length Mean            92.3
evaluation/path length Std              0.458258
evaluation/path length Max             93
evaluation/path length Min             92
evaluation/Rewards Mean                 2.02629
evaluation/Rewards Std                  0.494205
evaluation/Rewards Max                  3.10043
evaluation/Rewards Min                  0.982682
evaluation/Returns Mean               187.027
evaluation/Returns Std                  2.26057
evaluation/Returns Max                191.211
evaluation/Returns Min                184.311
evaluation/Actions Mean                 0.334778
evaluation/Actions Std                  0.727603
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                   10
evaluation/Average Returns            187.027
time/data storing (s)                   0.00344072
time/evaluation sampling (s)            0.416412
time/exploration sampling (s)           0.463359
time/logging (s)                        0.00403548
time/saving (s)                         0.00190255
time/training (s)                       7.15374
time/epoch (s)                          8.04289
time/total (s)                        180.317
Epoch                                  23
---------------------------------  ----------------
2021-07-02 23:39:29.602652 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 24 finished
---------------------------------  ----------------
replay_buffer/size                  35000
trainer/QF Loss                         0.00159713
trainer/Policy Loss                  -876.195
trainer/Raw Policy Loss              -876.195
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean            848.085
trainer/Q Predictions Std             226.139
trainer/Q Predictions Max            1278.03
trainer/Q Predictions Min               0.395371
trainer/Q Targets Mean                845.367
trainer/Q Targets Std                 235.502
trainer/Q Targets Max                1247.59
trainer/Q Targets Min                  -0.602503
trainer/Bellman Errors Mean          3342.61
trainer/Bellman Errors Std          35072.2
trainer/Bellman Errors Max         557678
trainer/Bellman Errors Min              1.432e-05
trainer/Policy Action Mean             -0.000740226
trainer/Policy Action Std               0.86856
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         35000
exploration/num paths total          1017
exploration/path length Mean           34.4828
exploration/path length Std            17.4269
exploration/path length Max            95
exploration/path length Min             9
exploration/Rewards Mean                0.35398
exploration/Rewards Std                 0.361887
exploration/Rewards Max                 1.49352
exploration/Rewards Min                -0.543148
exploration/Returns Mean               12.2062
exploration/Returns Std                12.1557
exploration/Returns Max                68.7533
exploration/Returns Min                 2.46363
exploration/Actions Mean                0.148931
exploration/Actions Std                 0.617318
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  29
exploration/Average Returns            12.2062
evaluation/num steps total          23683
evaluation/num paths total            392
evaluation/path length Mean           136.429
evaluation/path length Std             27.0072
evaluation/path length Max            200
evaluation/path length Min            117
evaluation/Rewards Mean                 0.561247
evaluation/Rewards Std                  0.422754
evaluation/Rewards Max                  1.73434
evaluation/Rewards Min                 -0.239496
evaluation/Returns Mean                76.5701
evaluation/Returns Std                 31.1336
evaluation/Returns Max                151.463
evaluation/Returns Min                 55.8497
evaluation/Actions Mean                 0.213967
evaluation/Actions Std                  0.604936
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    7
evaluation/Average Returns             76.5701
time/data storing (s)                   0.00355462
time/evaluation sampling (s)            0.382842
time/exploration sampling (s)           0.401744
time/logging (s)                        0.00421254
time/saving (s)                         0.00153103
time/training (s)                       7.84147
time/epoch (s)                          8.63536
time/total (s)                        188.954
Epoch                                  24
---------------------------------  ----------------
2021-07-02 23:39:37.764604 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 25 finished
---------------------------------  ---------------
replay_buffer/size                  36000
trainer/QF Loss                         0.00136602
trainer/Policy Loss                  -936.232
trainer/Raw Policy Loss              -936.232
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean            889.175
trainer/Q Predictions Std             218.659
trainer/Q Predictions Max            1303.13
trainer/Q Predictions Min              12.5007
trainer/Q Targets Mean                880.106
trainer/Q Targets Std                 230.724
trainer/Q Targets Max                1244.65
trainer/Q Targets Min                   0.690718
trainer/Bellman Errors Mean          3973.39
trainer/Bellman Errors Std          19899.9
trainer/Bellman Errors Max         176059
trainer/Bellman Errors Min              0.00297073
trainer/Policy Action Mean              0.0611337
trainer/Policy Action Std               0.830902
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         36000
exploration/num paths total          1035
exploration/path length Mean           55.5556
exploration/path length Std            46.0099
exploration/path length Max           165
exploration/path length Min             8
exploration/Rewards Mean                0.425461
exploration/Rewards Std                 0.320776
exploration/Rewards Max                 1.40803
exploration/Rewards Min                -0.522131
exploration/Returns Mean               23.6367
exploration/Returns Std                21.9124
exploration/Returns Max                84.5444
exploration/Returns Min                 4.09031
exploration/Actions Mean                0.268667
exploration/Actions Std                 0.621693
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  18
exploration/Average Returns            23.6367
evaluation/num steps total          24647
evaluation/num paths total            412
evaluation/path length Mean            48.2
evaluation/path length Std              1.07703
evaluation/path length Max             50
evaluation/path length Min             47
evaluation/Rewards Mean                 0.397473
evaluation/Rewards Std                  0.16015
evaluation/Rewards Max                  0.904459
evaluation/Rewards Min                  0.155061
evaluation/Returns Mean                19.1582
evaluation/Returns Std                  0.634022
evaluation/Returns Max                 20.1189
evaluation/Returns Min                 18.1863
evaluation/Actions Mean                 0.240806
evaluation/Actions Std                  0.589601
evaluation/Actions Max                  0.999996
evaluation/Actions Min                 -1
evaluation/Num Paths                   20
evaluation/Average Returns             19.1582
time/data storing (s)                   0.00352279
time/evaluation sampling (s)            0.378099
time/exploration sampling (s)           0.466508
time/logging (s)                        0.00619134
time/saving (s)                         0.00207212
time/training (s)                       7.30547
time/epoch (s)                          8.16186
time/total (s)                        197.118
Epoch                                  25
---------------------------------  ---------------
2021-07-02 23:39:45.445765 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 26 finished
---------------------------------  ---------------
replay_buffer/size                  37000
trainer/QF Loss                         0.00227006
trainer/Policy Loss                 -1020.11
trainer/Raw Policy Loss             -1020.11
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean            963.284
trainer/Q Predictions Std             284.453
trainer/Q Predictions Max            1365.26
trainer/Q Predictions Min             -14.9962
trainer/Q Targets Mean                960.587
trainer/Q Targets Std                 294.518
trainer/Q Targets Max                1384.09
trainer/Q Targets Min                  -0.51565
trainer/Bellman Errors Mean          4531.55
trainer/Bellman Errors Std          39184.6
trainer/Bellman Errors Max         618697
trainer/Bellman Errors Min              0.0109569
trainer/Policy Action Mean              0.0506689
trainer/Policy Action Std               0.856121
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         37000
exploration/num paths total          1047
exploration/path length Mean           83.3333
exploration/path length Std            64.7049
exploration/path length Max           235
exploration/path length Min             8
exploration/Rewards Mean                0.894375
exploration/Rewards Std                 0.610846
exploration/Rewards Max                 3.39581
exploration/Rewards Min                -0.145761
exploration/Returns Mean               74.5313
exploration/Returns Std                78.6177
exploration/Returns Max               265.14
exploration/Returns Min                 4.63081
exploration/Actions Mean                0.246993
exploration/Actions Std                 0.637056
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  12
exploration/Average Returns            74.5313
evaluation/num steps total          25178
evaluation/num paths total            414
evaluation/path length Mean           265.5
evaluation/path length Std             60.5
evaluation/path length Max            326
evaluation/path length Min            205
evaluation/Rewards Mean                 0.804249
evaluation/Rewards Std                  0.304513
evaluation/Rewards Max                  1.99556
evaluation/Rewards Min                  0.179997
evaluation/Returns Mean               213.528
evaluation/Returns Std                 58.3691
evaluation/Returns Max                271.897
evaluation/Returns Min                155.159
evaluation/Actions Mean                 0.139669
evaluation/Actions Std                  0.589174
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    2
evaluation/Average Returns            213.528
time/data storing (s)                   0.00352656
time/evaluation sampling (s)            0.393599
time/exploration sampling (s)           0.391696
time/logging (s)                        0.00346546
time/saving (s)                         0.00204687
time/training (s)                       6.88136
time/epoch (s)                          7.6757
time/total (s)                        204.795
Epoch                                  26
---------------------------------  ---------------
2021-07-02 23:39:53.507817 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 27 finished
---------------------------------  ----------------
replay_buffer/size                  38000
trainer/QF Loss                         0.00219075
trainer/Policy Loss                 -1045.05
trainer/Raw Policy Loss             -1045.05
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean            977.413
trainer/Q Predictions Std             317.531
trainer/Q Predictions Max            1465.57
trainer/Q Predictions Min             -12.0615
trainer/Q Targets Mean                974.269
trainer/Q Targets Std                 332.448
trainer/Q Targets Max                1466.31
trainer/Q Targets Min                  -0.226995
trainer/Bellman Errors Mean          4680.74
trainer/Bellman Errors Std          25846.1
trainer/Bellman Errors Max         309679
trainer/Bellman Errors Min              0.000203982
trainer/Policy Action Mean              0.160658
trainer/Policy Action Std               0.849711
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         38000
exploration/num paths total          1055
exploration/path length Mean          125
exploration/path length Std            85.84
exploration/path length Max           271
exploration/path length Min            18
exploration/Rewards Mean                1.07883
exploration/Rewards Std                 0.668585
exploration/Rewards Max                 3.60653
exploration/Rewards Min                 0.00697003
exploration/Returns Mean              134.853
exploration/Returns Std               136.136
exploration/Returns Max               450.835
exploration/Returns Min                 9.1644
exploration/Actions Mean                0.185525
exploration/Actions Std                 0.634736
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                   8
exploration/Average Returns           134.853
evaluation/num steps total          25877
evaluation/num paths total            416
evaluation/path length Mean           349.5
evaluation/path length Std             13.5
evaluation/path length Max            363
evaluation/path length Min            336
evaluation/Rewards Mean                 1.13063
evaluation/Rewards Std                  0.647189
evaluation/Rewards Max                  4.27771
evaluation/Rewards Min                  0.186359
evaluation/Returns Mean               395.155
evaluation/Returns Std                 20.2489
evaluation/Returns Max                415.404
evaluation/Returns Min                374.906
evaluation/Actions Mean                -0.0170397
evaluation/Actions Std                  0.68353
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    2
evaluation/Average Returns            395.155
time/data storing (s)                   0.00347264
time/evaluation sampling (s)            0.388949
time/exploration sampling (s)           0.405251
time/logging (s)                        0.00360725
time/saving (s)                         0.00158207
time/training (s)                       7.25744
time/epoch (s)                          8.0603
time/total (s)                        212.857
Epoch                                  27
---------------------------------  ----------------
2021-07-02 23:40:01.478738 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 28 finished
---------------------------------  ---------------
replay_buffer/size                  39000
trainer/QF Loss                         0.00147877
trainer/Policy Loss                 -1121.29
trainer/Raw Policy Loss             -1121.29
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           1050.36
trainer/Q Predictions Std             308.815
trainer/Q Predictions Max            1462.59
trainer/Q Predictions Min             -19.3685
trainer/Q Targets Mean               1047.83
trainer/Q Targets Std                 313.215
trainer/Q Targets Max                1466.84
trainer/Q Targets Min                  -0.719978
trainer/Bellman Errors Mean          4083.38
trainer/Bellman Errors Std          20863.9
trainer/Bellman Errors Max         199039
trainer/Bellman Errors Min              0.0362635
trainer/Policy Action Mean              0.309103
trainer/Policy Action Std               0.812871
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         39000
exploration/num paths total          1066
exploration/path length Mean           90.9091
exploration/path length Std            79.2711
exploration/path length Max           279
exploration/path length Min            15
exploration/Rewards Mean                1.68482
exploration/Rewards Std                 1.01586
exploration/Rewards Max                 5.45979
exploration/Rewards Min                -0.432905
exploration/Returns Mean              153.166
exploration/Returns Std               172.778
exploration/Returns Max               545.58
exploration/Returns Min                 6.83525
exploration/Actions Mean                0.107749
exploration/Actions Std                 0.678325
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  11
exploration/Average Returns           153.166
evaluation/num steps total          26690
evaluation/num paths total            419
evaluation/path length Mean           271
evaluation/path length Std            110.312
evaluation/path length Max            427
evaluation/path length Min            192
evaluation/Rewards Mean                 1.3214
evaluation/Rewards Std                  0.955482
evaluation/Rewards Max                  4.74749
evaluation/Rewards Min                  0.147606
evaluation/Returns Mean               358.098
evaluation/Returns Std                283.052
evaluation/Returns Max                758.39
evaluation/Returns Min                156.264
evaluation/Actions Mean                 0.379775
evaluation/Actions Std                  0.683269
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    3
evaluation/Average Returns            358.098
time/data storing (s)                   0.00347714
time/evaluation sampling (s)            0.434706
time/exploration sampling (s)           0.489418
time/logging (s)                        0.00553076
time/saving (s)                         0.00202565
time/training (s)                       7.0355
time/epoch (s)                          7.97065
time/total (s)                        220.83
Epoch                                  28
---------------------------------  ---------------
2021-07-02 23:40:09.539427 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 29 finished
---------------------------------  ---------------
replay_buffer/size                  40000
trainer/QF Loss                         0.00119978
trainer/Policy Loss                 -1202
trainer/Raw Policy Loss             -1202
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           1128.67
trainer/Q Predictions Std             360.39
trainer/Q Predictions Max            1644.08
trainer/Q Predictions Min             -59.6696
trainer/Q Targets Mean               1116.09
trainer/Q Targets Std                 384.534
trainer/Q Targets Max                1634.69
trainer/Q Targets Min                 -27.86
trainer/Bellman Errors Mean          8687.78
trainer/Bellman Errors Std          41984.5
trainer/Bellman Errors Max         360474
trainer/Bellman Errors Min              0.00554927
trainer/Policy Action Mean              0.288037
trainer/Policy Action Std               0.822951
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         40000
exploration/num paths total          1078
exploration/path length Mean           83.3333
exploration/path length Std            31.6395
exploration/path length Max           119
exploration/path length Min             9
exploration/Rewards Mean                1.96492
exploration/Rewards Std                 0.615031
exploration/Rewards Max                 4.75944
exploration/Rewards Min                 0.773554
exploration/Returns Mean              163.743
exploration/Returns Std                75.6489
exploration/Returns Max               285.278
exploration/Returns Min                 8.52135
exploration/Actions Mean                0.199035
exploration/Actions Std                 0.693497
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  12
exploration/Average Returns           163.743
evaluation/num steps total          27683
evaluation/num paths total            429
evaluation/path length Mean            99.3
evaluation/path length Std              5.728
evaluation/path length Max            110
evaluation/path length Min             90
evaluation/Rewards Mean                 1.98866
evaluation/Rewards Std                  0.511751
evaluation/Rewards Max                  3.28043
evaluation/Rewards Min                  0.744066
evaluation/Returns Mean               197.474
evaluation/Returns Std                 14.9861
evaluation/Returns Max                220.975
evaluation/Returns Min                170.887
evaluation/Actions Mean                 0.210239
evaluation/Actions Std                  0.765174
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                   10
evaluation/Average Returns            197.474
time/data storing (s)                   0.00363059
time/evaluation sampling (s)            0.431733
time/exploration sampling (s)           0.585058
time/logging (s)                        0.00413457
time/saving (s)                         0.00157984
time/training (s)                       7.0305
time/epoch (s)                          8.05664
time/total (s)                        228.888
Epoch                                  29
---------------------------------  ---------------
2021-07-02 23:40:17.273351 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 30 finished
---------------------------------  ---------------
replay_buffer/size                  41000
trainer/QF Loss                         0.00208515
trainer/Policy Loss                 -1249.38
trainer/Raw Policy Loss             -1249.38
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           1200.68
trainer/Q Predictions Std             367.725
trainer/Q Predictions Max            1688.59
trainer/Q Predictions Min              26.3322
trainer/Q Targets Mean               1211.39
trainer/Q Targets Std                 367.661
trainer/Q Targets Max                1709.98
trainer/Q Targets Min                  -0.0466931
trainer/Bellman Errors Mean          3109.35
trainer/Bellman Errors Std          11355.7
trainer/Bellman Errors Max         133553
trainer/Bellman Errors Min              0.00489248
trainer/Policy Action Mean              0.283165
trainer/Policy Action Std               0.801156
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         41000
exploration/num paths total          1087
exploration/path length Mean          111.111
exploration/path length Std            52.8046
exploration/path length Max           219
exploration/path length Min            23
exploration/Rewards Mean                2.10979
exploration/Rewards Std                 0.734096
exploration/Rewards Max                 4.77344
exploration/Rewards Min                 0.885508
exploration/Returns Mean              234.422
exploration/Returns Std               118.196
exploration/Returns Max               458.457
exploration/Returns Min                35.7573
exploration/Actions Mean                0.149428
exploration/Actions Std                 0.711182
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                   9
exploration/Average Returns           234.422
evaluation/num steps total          28609
evaluation/num paths total            437
evaluation/path length Mean           115.75
evaluation/path length Std             11.1102
evaluation/path length Max            137
evaluation/path length Min            104
evaluation/Rewards Mean                 2.38073
evaluation/Rewards Std                  0.951354
evaluation/Rewards Max                  4.98213
evaluation/Rewards Min                  0.967719
evaluation/Returns Mean               275.57
evaluation/Returns Std                 49.9692
evaluation/Returns Max                372.092
evaluation/Returns Min                225.655
evaluation/Actions Mean                 0.106398
evaluation/Actions Std                  0.814546
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    8
evaluation/Average Returns            275.57
time/data storing (s)                   0.00348612
time/evaluation sampling (s)            0.376037
time/exploration sampling (s)           0.397467
time/logging (s)                        0.00581981
time/saving (s)                         0.00150156
time/training (s)                       6.94926
time/epoch (s)                          7.73357
time/total (s)                        236.624
Epoch                                  30
---------------------------------  ---------------
2021-07-02 23:40:24.968347 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 31 finished
---------------------------------  ---------------
replay_buffer/size                  42000
trainer/QF Loss                         0.00161043
trainer/Policy Loss                 -1374.13
trainer/Raw Policy Loss             -1374.13
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           1311.64
trainer/Q Predictions Std             385.108
trainer/Q Predictions Max            1895.74
trainer/Q Predictions Min               1.96573
trainer/Q Targets Mean               1316.26
trainer/Q Targets Std                 391.542
trainer/Q Targets Max                1856.93
trainer/Q Targets Min                 -13.9035
trainer/Bellman Errors Mean          3958.59
trainer/Bellman Errors Std          16952.8
trainer/Bellman Errors Max         202175
trainer/Bellman Errors Min              0.00284566
trainer/Policy Action Mean              0.314297
trainer/Policy Action Std               0.781166
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         42000
exploration/num paths total          1099
exploration/path length Mean           83.3333
exploration/path length Std            44.2537
exploration/path length Max           146
exploration/path length Min            18
exploration/Rewards Mean                2.05248
exploration/Rewards Std                 0.625051
exploration/Rewards Max                 4.23519
exploration/Rewards Min                 0.17796
exploration/Returns Mean              171.04
exploration/Returns Std               107.984
exploration/Returns Max               349.608
exploration/Returns Min                27.4318
exploration/Actions Mean                0.130908
exploration/Actions Std                 0.689193
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  12
exploration/Average Returns           171.04
evaluation/num steps total          29529
evaluation/num paths total            448
evaluation/path length Mean            83.6364
evaluation/path length Std              0.771389
evaluation/path length Max             85
evaluation/path length Min             82
evaluation/Rewards Mean                 2.06256
evaluation/Rewards Std                  0.602991
evaluation/Rewards Max                  3.30978
evaluation/Rewards Min                  0.951876
evaluation/Returns Mean               172.505
evaluation/Returns Std                  0.995216
evaluation/Returns Max                174.261
evaluation/Returns Min                171.042
evaluation/Actions Mean                 0.302321
evaluation/Actions Std                  0.712607
evaluation/Actions Max                  1
evaluation/Actions Min                 -0.999854
evaluation/Num Paths                   11
evaluation/Average Returns            172.505
time/data storing (s)                   0.00345254
time/evaluation sampling (s)            0.399706
time/exploration sampling (s)           0.386754
time/logging (s)                        0.00604428
time/saving (s)                         0.00206909
time/training (s)                       6.89392
time/epoch (s)                          7.69195
time/total (s)                        244.318
Epoch                                  31
---------------------------------  ---------------
2021-07-02 23:40:33.638283 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 32 finished
---------------------------------  ---------------
replay_buffer/size                  43000
trainer/QF Loss                         0.00130717
trainer/Policy Loss                 -1408.55
trainer/Raw Policy Loss             -1408.55
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           1351.04
trainer/Q Predictions Std             418.171
trainer/Q Predictions Max            1958.69
trainer/Q Predictions Min               1.38635
trainer/Q Targets Mean               1344.84
trainer/Q Targets Std                 430.171
trainer/Q Targets Max                1883.24
trainer/Q Targets Min                 -11.9056
trainer/Bellman Errors Mean          5324.28
trainer/Bellman Errors Std          35743.1
trainer/Bellman Errors Max         417369
trainer/Bellman Errors Min              0.0678811
trainer/Policy Action Mean              0.466203
trainer/Policy Action Std               0.725351
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         43000
exploration/num paths total          1110
exploration/path length Mean           90.9091
exploration/path length Std            41.157
exploration/path length Max           148
exploration/path length Min            24
exploration/Rewards Mean                1.97428
exploration/Rewards Std                 0.587906
exploration/Rewards Max                 3.96424
exploration/Rewards Min                 0.918765
exploration/Returns Mean              179.48
exploration/Returns Std                95.433
exploration/Returns Max               314.671
exploration/Returns Min                35.9468
exploration/Actions Mean                0.188949
exploration/Actions Std                 0.687992
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  11
exploration/Average Returns           179.48
evaluation/num steps total          30391
evaluation/num paths total            453
evaluation/path length Mean           172.4
evaluation/path length Std             16.132
evaluation/path length Max            195
evaluation/path length Min            158
evaluation/Rewards Mean                 2.48761
evaluation/Rewards Std                  0.875244
evaluation/Rewards Max                  5.24788
evaluation/Rewards Min                  1.00422
evaluation/Returns Mean               428.864
evaluation/Returns Std                 67.4055
evaluation/Returns Max                516.943
evaluation/Returns Min                364.72
evaluation/Actions Mean                 0.189028
evaluation/Actions Std                  0.818744
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    5
evaluation/Average Returns            428.864
time/data storing (s)                   0.00614394
time/evaluation sampling (s)            0.497632
time/exploration sampling (s)           0.523941
time/logging (s)                        0.00398661
time/saving (s)                         0.00197656
time/training (s)                       7.63139
time/epoch (s)                          8.66507
time/total (s)                        252.985
Epoch                                  32
---------------------------------  ---------------
2021-07-02 23:40:41.936417 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 33 finished
---------------------------------  ---------------
replay_buffer/size                  44000
trainer/QF Loss                         0.00130079
trainer/Policy Loss                 -1512.79
trainer/Raw Policy Loss             -1512.79
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           1465.68
trainer/Q Predictions Std             428.461
trainer/Q Predictions Max            2000.89
trainer/Q Predictions Min             -23.1506
trainer/Q Targets Mean               1473.27
trainer/Q Targets Std                 438.108
trainer/Q Targets Max                1989.23
trainer/Q Targets Min                  -0.49435
trainer/Bellman Errors Mean          5638.52
trainer/Bellman Errors Std          27159.5
trainer/Bellman Errors Max         278647
trainer/Bellman Errors Min              0.00141358
trainer/Policy Action Mean              0.267612
trainer/Policy Action Std               0.805315
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         44000
exploration/num paths total          1120
exploration/path length Mean          100
exploration/path length Std            43.811
exploration/path length Max           147
exploration/path length Min            10
exploration/Rewards Mean                2.20444
exploration/Rewards Std                 0.859104
exploration/Rewards Max                 4.78207
exploration/Rewards Min                 0.254369
exploration/Returns Mean              220.444
exploration/Returns Std               126.359
exploration/Returns Max               391.74
exploration/Returns Min                 8.39895
exploration/Actions Mean                0.107698
exploration/Actions Std                 0.699832
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  10
exploration/Average Returns           220.444
evaluation/num steps total          30983
evaluation/num paths total            456
evaluation/path length Mean           197.333
evaluation/path length Std             65.1067
evaluation/path length Max            289
evaluation/path length Min            144
evaluation/Rewards Mean                 2.05697
evaluation/Rewards Std                  0.720849
evaluation/Rewards Max                  4.11035
evaluation/Rewards Min                  0.933216
evaluation/Returns Mean               405.909
evaluation/Returns Std                127.768
evaluation/Returns Max                585.909
evaluation/Returns Min                302.238
evaluation/Actions Mean                 0.139551
evaluation/Actions Std                  0.75971
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    3
evaluation/Average Returns            405.909
time/data storing (s)                   0.00623009
time/evaluation sampling (s)            0.387086
time/exploration sampling (s)           0.471079
time/logging (s)                        0.0034369
time/saving (s)                         0.001578
time/training (s)                       7.42599
time/epoch (s)                          8.29541
time/total (s)                        261.282
Epoch                                  33
---------------------------------  ---------------
2021-07-02 23:40:50.052252 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 34 finished
---------------------------------  ---------------
replay_buffer/size                  45000
trainer/QF Loss                         0.00142779
trainer/Policy Loss                 -1623.19
trainer/Raw Policy Loss             -1623.19
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           1550.24
trainer/Q Predictions Std             409.413
trainer/Q Predictions Max            2075.52
trainer/Q Predictions Min             -57.1789
trainer/Q Targets Mean               1566.42
trainer/Q Targets Std                 413.367
trainer/Q Targets Max                2034.53
trainer/Q Targets Min                  -0.732653
trainer/Bellman Errors Mean          6295.93
trainer/Bellman Errors Std          33187.7
trainer/Bellman Errors Max         479495
trainer/Bellman Errors Min              0.185998
trainer/Policy Action Mean              0.247844
trainer/Policy Action Std               0.815023
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         45000
exploration/num paths total          1131
exploration/path length Mean           90.9091
exploration/path length Std            64.2403
exploration/path length Max           211
exploration/path length Min             9
exploration/Rewards Mean                1.71238
exploration/Rewards Std                 0.592534
exploration/Rewards Max                 3.02695
exploration/Rewards Min                 0.30573
exploration/Returns Mean              155.671
exploration/Returns Std               127.469
exploration/Returns Max               359.439
exploration/Returns Min                 8.82826
exploration/Actions Mean                0.125549
exploration/Actions Std                 0.674409
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  11
exploration/Average Returns           155.671
evaluation/num steps total          31949
evaluation/num paths total            462
evaluation/path length Mean           161
evaluation/path length Std             33.8723
evaluation/path length Max            228
evaluation/path length Min            136
evaluation/Rewards Mean                 2.12389
evaluation/Rewards Std                  0.530782
evaluation/Rewards Max                  3.28866
evaluation/Rewards Min                  0.935002
evaluation/Returns Mean               341.946
evaluation/Returns Std                 59.4932
evaluation/Returns Max                465.377
evaluation/Returns Min                289.859
evaluation/Actions Mean                 0.00614371
evaluation/Actions Std                  0.769898
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    6
evaluation/Average Returns            341.946
time/data storing (s)                   0.00360598
time/evaluation sampling (s)            0.37992
time/exploration sampling (s)           0.404062
time/logging (s)                        0.00613213
time/saving (s)                         0.00203028
time/training (s)                       7.32064
time/epoch (s)                          8.11639
time/total (s)                        269.401
Epoch                                  34
---------------------------------  ---------------
2021-07-02 23:40:58.065940 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 35 finished
---------------------------------  ---------------
replay_buffer/size                  46000
trainer/QF Loss                         0.00135919
trainer/Policy Loss                 -1709.63
trainer/Raw Policy Loss             -1709.63
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           1634.89
trainer/Q Predictions Std             461.6
trainer/Q Predictions Max            2324.34
trainer/Q Predictions Min             -55.6518
trainer/Q Targets Mean               1629.29
trainer/Q Targets Std                 476.86
trainer/Q Targets Max                2206.93
trainer/Q Targets Min                  -0.550259
trainer/Bellman Errors Mean          7168.92
trainer/Bellman Errors Std          33015.9
trainer/Bellman Errors Max         386674
trainer/Bellman Errors Min              0.0186587
trainer/Policy Action Mean              0.24723
trainer/Policy Action Std               0.79745
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         46000
exploration/num paths total          1141
exploration/path length Mean          100
exploration/path length Std            65.3315
exploration/path length Max           209
exploration/path length Min             9
exploration/Rewards Mean                1.7776
exploration/Rewards Std                 0.674765
exploration/Rewards Max                 3.7262
exploration/Rewards Min                 0.365424
exploration/Returns Mean              177.76
exploration/Returns Std               127.634
exploration/Returns Max               343.178
exploration/Returns Min                 8.82899
exploration/Actions Mean                0.0786907
exploration/Actions Std                 0.6645
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  10
exploration/Average Returns           177.76
evaluation/num steps total          32820
evaluation/num paths total            468
evaluation/path length Mean           145.167
evaluation/path length Std              0.687184
evaluation/path length Max            146
evaluation/path length Min            144
evaluation/Rewards Mean                 2.12762
evaluation/Rewards Std                  0.53653
evaluation/Rewards Max                  3.1684
evaluation/Rewards Min                  0.903844
evaluation/Returns Mean               308.859
evaluation/Returns Std                  4.77154
evaluation/Returns Max                314.537
evaluation/Returns Min                300.276
evaluation/Actions Mean                 0.0506598
evaluation/Actions Std                  0.759515
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    6
evaluation/Average Returns            308.859
time/data storing (s)                   0.00357408
time/evaluation sampling (s)            0.449118
time/exploration sampling (s)           0.412558
time/logging (s)                        0.0038977
time/saving (s)                         0.00160098
time/training (s)                       7.13773
time/epoch (s)                          8.00847
time/total (s)                        277.411
Epoch                                  35
---------------------------------  ---------------
2021-07-02 23:41:06.130980 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 36 finished
---------------------------------  ---------------
replay_buffer/size                  47000
trainer/QF Loss                         0.00139778
trainer/Policy Loss                 -1768.62
trainer/Raw Policy Loss             -1768.62
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           1694.61
trainer/Q Predictions Std             518.416
trainer/Q Predictions Max            2268.7
trainer/Q Predictions Min             -61.6784
trainer/Q Targets Mean               1696.11
trainer/Q Targets Std                 527.376
trainer/Q Targets Max                2234.99
trainer/Q Targets Min                  -2.98183
trainer/Bellman Errors Mean          6975.71
trainer/Bellman Errors Std          34663.8
trainer/Bellman Errors Max         442829
trainer/Bellman Errors Min              0.0080719
trainer/Policy Action Mean              0.233637
trainer/Policy Action Std               0.816623
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         47000
exploration/num paths total          1149
exploration/path length Mean          125
exploration/path length Std            55.933
exploration/path length Max           231
exploration/path length Min            29
exploration/Rewards Mean                2.01228
exploration/Rewards Std                 0.62098
exploration/Rewards Max                 4.3005
exploration/Rewards Min                 0.824898
exploration/Returns Mean              251.535
exploration/Returns Std               134.412
exploration/Returns Max               520.255
exploration/Returns Min                44.9016
exploration/Actions Mean                0.0354814
exploration/Actions Std                 0.648724
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                   8
exploration/Average Returns           251.535
evaluation/num steps total          33681
evaluation/num paths total            474
evaluation/path length Mean           143.5
evaluation/path length Std             14.2097
evaluation/path length Max            172
evaluation/path length Min            128
evaluation/Rewards Mean                 2.15843
evaluation/Rewards Std                  0.617186
evaluation/Rewards Max                  3.89215
evaluation/Rewards Min                  0.890723
evaluation/Returns Mean               309.735
evaluation/Returns Std                 13.1988
evaluation/Returns Max                337.171
evaluation/Returns Min                298.605
evaluation/Actions Mean                 0.0412026
evaluation/Actions Std                  0.774667
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    6
evaluation/Average Returns            309.735
time/data storing (s)                   0.00348719
time/evaluation sampling (s)            0.372626
time/exploration sampling (s)           0.390362
time/logging (s)                        0.00377203
time/saving (s)                         0.00153531
time/training (s)                       7.29109
time/epoch (s)                          8.06287
time/total (s)                        285.476
Epoch                                  36
---------------------------------  ---------------
2021-07-02 23:41:14.448432 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 37 finished
---------------------------------  ---------------
replay_buffer/size                  48000
trainer/QF Loss                         0.00131649
trainer/Policy Loss                 -1828.46
trainer/Raw Policy Loss             -1828.46
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           1766.05
trainer/Q Predictions Std             569.751
trainer/Q Predictions Max            2340.27
trainer/Q Predictions Min             -55.1974
trainer/Q Targets Mean               1765.81
trainer/Q Targets Std                 564.94
trainer/Q Targets Max                2314.36
trainer/Q Targets Min                 -31.0526
trainer/Bellman Errors Mean          5772.52
trainer/Bellman Errors Std          24860.7
trainer/Bellman Errors Max         312287
trainer/Bellman Errors Min              0.00390625
trainer/Policy Action Mean              0.2199
trainer/Policy Action Std               0.822491
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         48000
exploration/num paths total          1158
exploration/path length Mean          111.111
exploration/path length Std            65.2524
exploration/path length Max           198
exploration/path length Min            11
exploration/Rewards Mean                2.08154
exploration/Rewards Std                 0.73952
exploration/Rewards Max                 4.63737
exploration/Rewards Min                 0.764622
exploration/Returns Mean              231.282
exploration/Returns Std               143.573
exploration/Returns Max               417.354
exploration/Returns Min                12.6781
exploration/Actions Mean                0.0709704
exploration/Actions Std                 0.67372
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                   9
exploration/Average Returns           231.282
evaluation/num steps total          34472
evaluation/num paths total            479
evaluation/path length Mean           158.2
evaluation/path length Std             12.6396
evaluation/path length Max            176
evaluation/path length Min            141
evaluation/Rewards Mean                 2.25222
evaluation/Rewards Std                  0.761386
evaluation/Rewards Max                  5.04017
evaluation/Rewards Min                  0.898775
evaluation/Returns Mean               356.301
evaluation/Returns Std                 56.1724
evaluation/Returns Max                439.324
evaluation/Returns Min                273.363
evaluation/Actions Mean                 0.0214197
evaluation/Actions Std                  0.796712
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    5
evaluation/Average Returns            356.301
time/data storing (s)                   0.0036631
time/evaluation sampling (s)            0.393561
time/exploration sampling (s)           0.412795
time/logging (s)                        0.00547582
time/saving (s)                         0.00207434
time/training (s)                       7.49949
time/epoch (s)                          8.31706
time/total (s)                        293.795
Epoch                                  37
---------------------------------  ---------------
2021-07-02 23:41:22.596327 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 38 finished
---------------------------------  ----------------
replay_buffer/size                  49000
trainer/QF Loss                         0.001392
trainer/Policy Loss                 -1779.18
trainer/Raw Policy Loss             -1779.18
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           1713.05
trainer/Q Predictions Std             644.082
trainer/Q Predictions Max            2385.48
trainer/Q Predictions Min             -57.9694
trainer/Q Targets Mean               1747.54
trainer/Q Targets Std                 650.251
trainer/Q Targets Max                2445.79
trainer/Q Targets Min                  -1.21339
trainer/Bellman Errors Mean          7328.04
trainer/Bellman Errors Std          34656.9
trainer/Bellman Errors Max         504234
trainer/Bellman Errors Min              0.000493586
trainer/Policy Action Mean              0.191523
trainer/Policy Action Std               0.850423
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         49000
exploration/num paths total          1166
exploration/path length Mean          125
exploration/path length Std            68.3319
exploration/path length Max           203
exploration/path length Min            11
exploration/Rewards Mean                1.8623
exploration/Rewards Std                 0.532687
exploration/Rewards Max                 3.79614
exploration/Rewards Min                 0.659528
exploration/Returns Mean              232.787
exploration/Returns Std               137.203
exploration/Returns Max               421.219
exploration/Returns Min                12.6691
exploration/Actions Mean                0.128083
exploration/Actions Std                 0.691744
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                   8
exploration/Average Returns           232.787
evaluation/num steps total          35450
evaluation/num paths total            483
evaluation/path length Mean           244.5
evaluation/path length Std             76.8163
evaluation/path length Max            345
evaluation/path length Min            165
evaluation/Rewards Mean                 1.58926
evaluation/Rewards Std                  0.86446
evaluation/Rewards Max                  4.55363
evaluation/Rewards Min                  0.0809152
evaluation/Returns Mean               388.574
evaluation/Returns Std                215.041
evaluation/Returns Max                651.521
evaluation/Returns Min                175.205
evaluation/Actions Mean                 0.179698
evaluation/Actions Std                  0.743427
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    4
evaluation/Average Returns            388.574
time/data storing (s)                   0.00350296
time/evaluation sampling (s)            0.380023
time/exploration sampling (s)           0.389821
time/logging (s)                        0.00427606
time/saving (s)                         0.00147785
time/training (s)                       7.36477
time/epoch (s)                          8.14387
time/total (s)                        301.94
Epoch                                  38
---------------------------------  ----------------
2021-07-02 23:41:31.098421 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 39 finished
---------------------------------  ----------------
replay_buffer/size                  50000
trainer/QF Loss                         0.00150708
trainer/Policy Loss                 -1872.59
trainer/Raw Policy Loss             -1872.59
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           1788.36
trainer/Q Predictions Std             604.213
trainer/Q Predictions Max            2522.82
trainer/Q Predictions Min             -25.8612
trainer/Q Targets Mean               1797.32
trainer/Q Targets Std                 611.368
trainer/Q Targets Max                2514.55
trainer/Q Targets Min                  -0.72273
trainer/Bellman Errors Mean         12884.9
trainer/Bellman Errors Std         113793
trainer/Bellman Errors Max              1.79908e+06
trainer/Bellman Errors Min              0.17165
trainer/Policy Action Mean              0.240024
trainer/Policy Action Std               0.839631
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         50000
exploration/num paths total          1175
exploration/path length Mean          111.111
exploration/path length Std            72.8155
exploration/path length Max           190
exploration/path length Min             8
exploration/Rewards Mean                2.01895
exploration/Rewards Std                 0.679624
exploration/Rewards Max                 4.39764
exploration/Rewards Min                 0.360618
exploration/Returns Mean              224.328
exploration/Returns Std               163.88
exploration/Returns Max               466.242
exploration/Returns Min                 7.32418
exploration/Actions Mean                0.10308
exploration/Actions Std                 0.683806
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                   9
exploration/Average Returns           224.328
evaluation/num steps total          36394
evaluation/num paths total            488
evaluation/path length Mean           188.8
evaluation/path length Std              3.54401
evaluation/path length Max            195
evaluation/path length Min            185
evaluation/Rewards Mean                 2.17339
evaluation/Rewards Std                  0.681683
evaluation/Rewards Max                  5.20423
evaluation/Rewards Min                  0.872568
evaluation/Returns Mean               410.337
evaluation/Returns Std                 39.7134
evaluation/Returns Max                462.795
evaluation/Returns Min                363.512
evaluation/Actions Mean                 0.112451
evaluation/Actions Std                  0.779403
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    5
evaluation/Average Returns            410.337
time/data storing (s)                   0.00628595
time/evaluation sampling (s)            0.382329
time/exploration sampling (s)           0.452648
time/logging (s)                        0.0059701
time/saving (s)                         0.00208329
time/training (s)                       7.65237
time/epoch (s)                          8.50169
time/total (s)                        310.444
Epoch                                  39
---------------------------------  ----------------
2021-07-02 23:41:39.786936 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 40 finished
---------------------------------  ---------------
replay_buffer/size                 51000
trainer/QF Loss                        0.00159135
trainer/Policy Loss                -2030.12
trainer/Raw Policy Loss            -2030.12
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean          1943.68
trainer/Q Predictions Std            601.333
trainer/Q Predictions Max           2640.74
trainer/Q Predictions Min            -39.6639
trainer/Q Targets Mean              1943.27
trainer/Q Targets Std                600.555
trainer/Q Targets Max               2570.67
trainer/Q Targets Min                 -0.302079
trainer/Bellman Errors Mean        10383.3
trainer/Bellman Errors Std         74361.8
trainer/Bellman Errors Max             1.09509e+06
trainer/Bellman Errors Min             0.000155032
trainer/Policy Action Mean             0.245124
trainer/Policy Action Std              0.822063
trainer/Policy Action Max              1
trainer/Policy Action Min             -1
exploration/num steps total        51000
exploration/num paths total         1189
exploration/path length Mean          71.4286
exploration/path length Std           53.2068
exploration/path length Max          185
exploration/path length Min            8
exploration/Rewards Mean               2.03843
exploration/Rewards Std                0.763176
exploration/Rewards Max                5.32732
exploration/Rewards Min                0.781702
exploration/Returns Mean             145.602
exploration/Returns Std              126.665
exploration/Returns Max              377.152
exploration/Returns Min                6.79748
exploration/Actions Mean               0.151033
exploration/Actions Std                0.699346
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                 14
exploration/Average Returns          145.602
evaluation/num steps total         37307
evaluation/num paths total           494
evaluation/path length Mean          152.167
evaluation/path length Std            19.4372
evaluation/path length Max           179
evaluation/path length Min           130
evaluation/Rewards Mean                2.33521
evaluation/Rewards Std                 0.818132
evaluation/Rewards Max                 4.78588
evaluation/Rewards Min                 0.862612
evaluation/Returns Mean              355.341
evaluation/Returns Std                65.8141
evaluation/Returns Max               458.332
evaluation/Returns Min               272.458
evaluation/Actions Mean                0.11013
evaluation/Actions Std                 0.800818
evaluation/Actions Max                 1
evaluation/Actions Min                -1
evaluation/Num Paths                   6
evaluation/Average Returns           355.341
time/data storing (s)                  0.00561673
time/evaluation sampling (s)           0.553345
time/exploration sampling (s)          0.432629
time/logging (s)                       0.00597805
time/saving (s)                        0.00213817
time/training (s)                      7.68591
time/epoch (s)                         8.68562
time/total (s)                       319.132
Epoch                                 40
---------------------------------  ---------------
2021-07-02 23:41:48.313264 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 41 finished
---------------------------------  ----------------
replay_buffer/size                  52000
trainer/QF Loss                         0.00161209
trainer/Policy Loss                 -2030.94
trainer/Raw Policy Loss             -2030.94
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           1964.21
trainer/Q Predictions Std             666.052
trainer/Q Predictions Max            2627.7
trainer/Q Predictions Min             -43.7884
trainer/Q Targets Mean               1952.78
trainer/Q Targets Std                 690.14
trainer/Q Targets Max                2674.66
trainer/Q Targets Min                 -16.3851
trainer/Bellman Errors Mean         19181
trainer/Bellman Errors Std         156714
trainer/Bellman Errors Max              2.36049e+06
trainer/Bellman Errors Min              0.00906587
trainer/Policy Action Mean              0.153489
trainer/Policy Action Std               0.841474
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         52000
exploration/num paths total          1199
exploration/path length Mean          100
exploration/path length Std            46.6819
exploration/path length Max           193
exploration/path length Min            30
exploration/Rewards Mean                1.66174
exploration/Rewards Std                 0.566113
exploration/Rewards Max                 4.03319
exploration/Rewards Min                 0.112722
exploration/Returns Mean              166.174
exploration/Returns Std                81.1061
exploration/Returns Max               294.516
exploration/Returns Min                29.2116
exploration/Actions Mean                0.107174
exploration/Actions Std                 0.694296
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  10
exploration/Average Returns           166.174
evaluation/num steps total          38228
evaluation/num paths total            500
evaluation/path length Mean           153.5
evaluation/path length Std             30.9987
evaluation/path length Max            190
evaluation/path length Min            121
evaluation/Rewards Mean                 2.35039
evaluation/Rewards Std                  0.975358
evaluation/Rewards Max                  6.41456
evaluation/Rewards Min                  0.783018
evaluation/Returns Mean               360.785
evaluation/Returns Std                110.375
evaluation/Returns Max                530.952
evaluation/Returns Min                241.1
evaluation/Actions Mean                 0.071705
evaluation/Actions Std                  0.839616
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    6
evaluation/Average Returns            360.785
time/data storing (s)                   0.0035149
time/evaluation sampling (s)            0.522153
time/exploration sampling (s)           0.459683
time/logging (s)                        0.00370255
time/saving (s)                         0.00147335
time/training (s)                       7.53072
time/epoch (s)                          8.52125
time/total (s)                        327.655
Epoch                                  41
---------------------------------  ----------------
2021-07-02 23:41:56.469864 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 42 finished
---------------------------------  ---------------
replay_buffer/size                  53000
trainer/QF Loss                         0.00101695
trainer/Policy Loss                 -2149.94
trainer/Raw Policy Loss             -2149.94
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2081.18
trainer/Q Predictions Std             626.53
trainer/Q Predictions Max            2738.2
trainer/Q Predictions Min             -22.9134
trainer/Q Targets Mean               2080.08
trainer/Q Targets Std                 640.335
trainer/Q Targets Max                2711.4
trainer/Q Targets Min                   0.0898771
trainer/Bellman Errors Mean         11819.4
trainer/Bellman Errors Std          67044.3
trainer/Bellman Errors Max         650177
trainer/Bellman Errors Min              0.00618005
trainer/Policy Action Mean              0.174554
trainer/Policy Action Std               0.820552
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         53000
exploration/num paths total          1207
exploration/path length Mean          125
exploration/path length Std           102.621
exploration/path length Max           365
exploration/path length Min            18
exploration/Rewards Mean                1.89351
exploration/Rewards Std                 0.70481
exploration/Rewards Max                 4.76654
exploration/Rewards Min                 0.46492
exploration/Returns Mean              236.689
exploration/Returns Std               186.764
exploration/Returns Max               617.014
exploration/Returns Min                27.1431
exploration/Actions Mean                0.104409
exploration/Actions Std                 0.695611
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                   8
exploration/Average Returns           236.689
evaluation/num steps total          39069
evaluation/num paths total            505
evaluation/path length Mean           168.2
evaluation/path length Std             21.4607
evaluation/path length Max            185
evaluation/path length Min            126
evaluation/Rewards Mean                 2.6419
evaluation/Rewards Std                  1.098
evaluation/Rewards Max                  5.52609
evaluation/Rewards Min                  0.857281
evaluation/Returns Mean               444.367
evaluation/Returns Std                 64.5754
evaluation/Returns Max                512.964
evaluation/Returns Min                323.529
evaluation/Actions Mean                 0.0533511
evaluation/Actions Std                  0.830326
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    5
evaluation/Average Returns            444.367
time/data storing (s)                   0.00350571
time/evaluation sampling (s)            0.397086
time/exploration sampling (s)           0.418746
time/logging (s)                        0.00576603
time/saving (s)                         0.00205267
time/training (s)                       7.32949
time/epoch (s)                          8.15665
time/total (s)                        335.813
Epoch                                  42
---------------------------------  ---------------
2021-07-02 23:42:05.331228 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 43 finished
---------------------------------  ---------------
replay_buffer/size                  54000
trainer/QF Loss                         0.0016924
trainer/Policy Loss                 -2135.26
trainer/Raw Policy Loss             -2135.26
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2068.22
trainer/Q Predictions Std             658.471
trainer/Q Predictions Max            2795.08
trainer/Q Predictions Min             -53.2434
trainer/Q Targets Mean               2083.13
trainer/Q Targets Std                 663.222
trainer/Q Targets Max                2777.77
trainer/Q Targets Min                   0.163915
trainer/Bellman Errors Mean          8120.23
trainer/Bellman Errors Std          33384.3
trainer/Bellman Errors Max         319298
trainer/Bellman Errors Min              1.13306
trainer/Policy Action Mean              0.194903
trainer/Policy Action Std               0.828457
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         54000
exploration/num paths total          1216
exploration/path length Mean          111.111
exploration/path length Std            61.9667
exploration/path length Max           208
exploration/path length Min             9
exploration/Rewards Mean                2.02612
exploration/Rewards Std                 0.95951
exploration/Rewards Max                 6.08738
exploration/Rewards Min                 0.389934
exploration/Returns Mean              225.124
exploration/Returns Std               162.719
exploration/Returns Max               517.398
exploration/Returns Min                 8.18572
exploration/Actions Mean                0.124017
exploration/Actions Std                 0.714996
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                   9
exploration/Average Returns           225.124
evaluation/num steps total          40011
evaluation/num paths total            510
evaluation/path length Mean           188.4
evaluation/path length Std             36.1087
evaluation/path length Max            235
evaluation/path length Min            155
evaluation/Rewards Mean                 2.30856
evaluation/Rewards Std                  0.922102
evaluation/Rewards Max                  6.08654
evaluation/Rewards Min                  0.858778
evaluation/Returns Mean               434.932
evaluation/Returns Std                 65.3509
evaluation/Returns Max                528.167
evaluation/Returns Min                361.934
evaluation/Actions Mean                 0.116234
evaluation/Actions Std                  0.813443
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    5
evaluation/Average Returns            434.932
time/data storing (s)                   0.00352148
time/evaluation sampling (s)            0.490933
time/exploration sampling (s)           0.488752
time/logging (s)                        0.00590335
time/saving (s)                         0.00206388
time/training (s)                       7.86738
time/epoch (s)                          8.85856
time/total (s)                        344.674
Epoch                                  43
---------------------------------  ---------------
2021-07-02 23:42:13.821481 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 44 finished
---------------------------------  ---------------
replay_buffer/size                  55000
trainer/QF Loss                         0.00133531
trainer/Policy Loss                 -2228.19
trainer/Raw Policy Loss             -2228.19
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2116.05
trainer/Q Predictions Std             689.115
trainer/Q Predictions Max            2921.41
trainer/Q Predictions Min             -34.0687
trainer/Q Targets Mean               2118.35
trainer/Q Targets Std                 697.109
trainer/Q Targets Max                2811.67
trainer/Q Targets Min                 -41.0588
trainer/Bellman Errors Mean         14436.4
trainer/Bellman Errors Std          68893
trainer/Bellman Errors Max         702443
trainer/Bellman Errors Min              0.0925372
trainer/Policy Action Mean              0.203305
trainer/Policy Action Std               0.821244
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         55000
exploration/num paths total          1224
exploration/path length Mean          125
exploration/path length Std            86.6934
exploration/path length Max           280
exploration/path length Min            13
exploration/Rewards Mean                1.87379
exploration/Rewards Std                 0.699731
exploration/Rewards Max                 4.53675
exploration/Rewards Min                 0.59209
exploration/Returns Mean              234.223
exploration/Returns Std               182.786
exploration/Returns Max               582.206
exploration/Returns Min                16.0152
exploration/Actions Mean                0.126179
exploration/Actions Std                 0.708527
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                   8
exploration/Average Returns           234.223
evaluation/num steps total          40875
evaluation/num paths total            513
evaluation/path length Mean           288
evaluation/path length Std             32.9545
evaluation/path length Max            333
evaluation/path length Min            255
evaluation/Rewards Mean                 2.01428
evaluation/Rewards Std                  0.81914
evaluation/Rewards Max                  4.68748
evaluation/Rewards Min                  0.443044
evaluation/Returns Mean               580.111
evaluation/Returns Std                 39.5301
evaluation/Returns Max                633.337
evaluation/Returns Min                538.692
evaluation/Actions Mean                 0.0204026
evaluation/Actions Std                  0.780134
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    3
evaluation/Average Returns            580.111
time/data storing (s)                   0.00453172
time/evaluation sampling (s)            0.386653
time/exploration sampling (s)           0.407821
time/logging (s)                        0.00401294
time/saving (s)                         0.00166814
time/training (s)                       7.6807
time/epoch (s)                          8.48539
time/total (s)                        353.162
Epoch                                  44
---------------------------------  ---------------
2021-07-02 23:42:22.689058 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 45 finished
---------------------------------  ---------------
replay_buffer/size                 56000
trainer/QF Loss                        0.00176513
trainer/Policy Loss                -2279.05
trainer/Raw Policy Loss            -2279.05
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean          2213.34
trainer/Q Predictions Std            744.936
trainer/Q Predictions Max           2946.89
trainer/Q Predictions Min            -35.3452
trainer/Q Targets Mean              2212.46
trainer/Q Targets Std                765.185
trainer/Q Targets Max               2891.57
trainer/Q Targets Min                  0.755427
trainer/Bellman Errors Mean        14590
trainer/Bellman Errors Std         93216.2
trainer/Bellman Errors Max             1.27099e+06
trainer/Bellman Errors Min             0.0402739
trainer/Policy Action Mean             0.240192
trainer/Policy Action Std              0.809565
trainer/Policy Action Max              1
trainer/Policy Action Min             -1
exploration/num steps total        56000
exploration/num paths total         1231
exploration/path length Mean         142.857
exploration/path length Std           54.5355
exploration/path length Max          250
exploration/path length Min           79
exploration/Rewards Mean               2.11724
exploration/Rewards Std                0.765668
exploration/Rewards Max                4.81574
exploration/Rewards Min                0.7576
exploration/Returns Mean             302.463
exploration/Returns Std              142.931
exploration/Returns Max              563.238
exploration/Returns Min              121.88
exploration/Actions Mean               0.082956
exploration/Actions Std                0.683523
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                  7
exploration/Average Returns          302.463
evaluation/num steps total         41728
evaluation/num paths total           517
evaluation/path length Mean          213.25
evaluation/path length Std             6.75925
evaluation/path length Max           223
evaluation/path length Min           206
evaluation/Rewards Mean                2.58468
evaluation/Rewards Std                 1.12342
evaluation/Rewards Max                 6.20803
evaluation/Rewards Min                 0.859705
evaluation/Returns Mean              551.182
evaluation/Returns Std                22.8873
evaluation/Returns Max               587.511
evaluation/Returns Min               524.258
evaluation/Actions Mean                0.0257463
evaluation/Actions Std                 0.793731
evaluation/Actions Max                 1
evaluation/Actions Min                -1
evaluation/Num Paths                   4
evaluation/Average Returns           551.182
time/data storing (s)                  0.00609894
time/evaluation sampling (s)           0.372569
time/exploration sampling (s)          0.45904
time/logging (s)                       0.00412385
time/saving (s)                        0.00157077
time/training (s)                      8.0219
time/epoch (s)                         8.86531
time/total (s)                       362.029
Epoch                                 45
---------------------------------  ---------------
2021-07-02 23:42:31.446058 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 46 finished
---------------------------------  ---------------
replay_buffer/size                  57000
trainer/QF Loss                         0.00136274
trainer/Policy Loss                 -2351.54
trainer/Raw Policy Loss             -2351.54
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2279.68
trainer/Q Predictions Std             741.921
trainer/Q Predictions Max            3006.56
trainer/Q Predictions Min             -33.0261
trainer/Q Targets Mean               2283.29
trainer/Q Targets Std                 752.123
trainer/Q Targets Max                2993.24
trainer/Q Targets Min                  -9.81282
trainer/Bellman Errors Mean          8350.99
trainer/Bellman Errors Std          54977.5
trainer/Bellman Errors Max         837780
trainer/Bellman Errors Min              0.0180304
trainer/Policy Action Mean              0.17457
trainer/Policy Action Std               0.833919
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         57000
exploration/num paths total          1241
exploration/path length Mean          100
exploration/path length Std            48.025
exploration/path length Max           183
exploration/path length Min            33
exploration/Rewards Mean                2.10471
exploration/Rewards Std                 0.889616
exploration/Rewards Max                 5.62136
exploration/Rewards Min                -0.250951
exploration/Returns Mean              210.471
exploration/Returns Std               148.986
exploration/Returns Max               510.304
exploration/Returns Min                25.9159
exploration/Actions Mean                0.0809901
exploration/Actions Std                 0.697388
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  10
exploration/Average Returns           210.471
evaluation/num steps total          42558
evaluation/num paths total            521
evaluation/path length Mean           207.5
evaluation/path length Std             15.5322
evaluation/path length Max            224
evaluation/path length Min            187
evaluation/Rewards Mean                 2.53494
evaluation/Rewards Std                  1.02156
evaluation/Rewards Max                  6.08825
evaluation/Rewards Min                  0.821528
evaluation/Returns Mean               526.001
evaluation/Returns Std                 35.1195
evaluation/Returns Max                563.658
evaluation/Returns Min                470.089
evaluation/Actions Mean                 0.053174
evaluation/Actions Std                  0.780867
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    4
evaluation/Average Returns            526.001
time/data storing (s)                   0.00366227
time/evaluation sampling (s)            0.409305
time/exploration sampling (s)           0.51498
time/logging (s)                        0.00400216
time/saving (s)                         0.00212681
time/training (s)                       7.82043
time/epoch (s)                          8.75451
time/total (s)                        370.785
Epoch                                  46
---------------------------------  ---------------
2021-07-02 23:42:39.663113 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 47 finished
---------------------------------  ---------------
replay_buffer/size                  58000
trainer/QF Loss                         0.00132064
trainer/Policy Loss                 -2372.33
trainer/Raw Policy Loss             -2372.33
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2283.46
trainer/Q Predictions Std             799.378
trainer/Q Predictions Max            3179.35
trainer/Q Predictions Min             -60.7965
trainer/Q Targets Mean               2293.5
trainer/Q Targets Std                 811.173
trainer/Q Targets Max                3214.12
trainer/Q Targets Min                 -34.5829
trainer/Bellman Errors Mean          8437.49
trainer/Bellman Errors Std          41738.8
trainer/Bellman Errors Max         634819
trainer/Bellman Errors Min              0.00911242
trainer/Policy Action Mean              0.214572
trainer/Policy Action Std               0.828795
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         58000
exploration/num paths total          1247
exploration/path length Mean          166.667
exploration/path length Std           102.12
exploration/path length Max           276
exploration/path length Min             3
exploration/Rewards Mean                2.018
exploration/Rewards Std                 0.574788
exploration/Rewards Max                 4.00931
exploration/Rewards Min                 0.667557
exploration/Returns Mean              336.334
exploration/Returns Std               218.182
exploration/Returns Max               571.153
exploration/Returns Min                 2.78747
exploration/Actions Mean                0.100997
exploration/Actions Std                 0.684897
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                   6
exploration/Average Returns           336.334
evaluation/num steps total          43463
evaluation/num paths total            526
evaluation/path length Mean           181
evaluation/path length Std             13.813
evaluation/path length Max            206
evaluation/path length Min            164
evaluation/Rewards Mean                 2.37446
evaluation/Rewards Std                  0.779552
evaluation/Rewards Max                  5.93782
evaluation/Rewards Min                  0.718726
evaluation/Returns Mean               429.778
evaluation/Returns Std                 48.5588
evaluation/Returns Max                522.263
evaluation/Returns Min                378.159
evaluation/Actions Mean                 0.00659112
evaluation/Actions Std                  0.769033
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    5
evaluation/Average Returns            429.778
time/data storing (s)                   0.0035071
time/evaluation sampling (s)            0.357612
time/exploration sampling (s)           0.399758
time/logging (s)                        0.00586935
time/saving (s)                         0.00205114
time/training (s)                       7.44811
time/epoch (s)                          8.21691
time/total (s)                        379.003
Epoch                                  47
---------------------------------  ---------------
2021-07-02 23:42:48.282574 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 48 finished
---------------------------------  ---------------
replay_buffer/size                  59000
trainer/QF Loss                         0.00118302
trainer/Policy Loss                 -2476.14
trainer/Raw Policy Loss             -2476.14
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2354.29
trainer/Q Predictions Std             829.38
trainer/Q Predictions Max            3222.64
trainer/Q Predictions Min              50.2011
trainer/Q Targets Mean               2369.93
trainer/Q Targets Std                 858.304
trainer/Q Targets Max                3190.14
trainer/Q Targets Min                  -0.320906
trainer/Bellman Errors Mean         13738.6
trainer/Bellman Errors Std          56519.6
trainer/Bellman Errors Max         737910
trainer/Bellman Errors Min              0.00987345
trainer/Policy Action Mean              0.0259654
trainer/Policy Action Std               0.851856
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         59000
exploration/num paths total          1259
exploration/path length Mean           83.3333
exploration/path length Std            69.5789
exploration/path length Max           254
exploration/path length Min            10
exploration/Rewards Mean                0.99951
exploration/Rewards Std                 0.832748
exploration/Rewards Max                 4.02976
exploration/Rewards Min                -0.0361278
exploration/Returns Mean               83.2925
exploration/Returns Std               121.365
exploration/Returns Max               457.954
exploration/Returns Min                 7.50454
exploration/Actions Mean                0.130128
exploration/Actions Std                 0.620651
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  12
exploration/Average Returns            83.2925
evaluation/num steps total          44082
evaluation/num paths total            528
evaluation/path length Mean           309.5
evaluation/path length Std              5.5
evaluation/path length Max            315
evaluation/path length Min            304
evaluation/Rewards Mean                 1.23528
evaluation/Rewards Std                  0.565022
evaluation/Rewards Max                  3.0639
evaluation/Rewards Min                 -0.0738959
evaluation/Returns Mean               382.32
evaluation/Returns Std                 12.1649
evaluation/Returns Max                394.485
evaluation/Returns Min                370.155
evaluation/Actions Mean                 0.191452
evaluation/Actions Std                  0.691154
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    2
evaluation/Average Returns            382.32
time/data storing (s)                   0.0035663
time/evaluation sampling (s)            0.535507
time/exploration sampling (s)           0.551221
time/logging (s)                        0.00363424
time/saving (s)                         0.001701
time/training (s)                       7.51868
time/epoch (s)                          8.61431
time/total (s)                        387.62
Epoch                                  48
---------------------------------  ---------------
2021-07-02 23:42:56.393114 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 49 finished
---------------------------------  ----------------
replay_buffer/size                  60000
trainer/QF Loss                         0.00143742
trainer/Policy Loss                 -2566.41
trainer/Raw Policy Loss             -2566.41
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2439.24
trainer/Q Predictions Std             836.027
trainer/Q Predictions Max            3558.72
trainer/Q Predictions Min            -154.204
trainer/Q Targets Mean               2428.77
trainer/Q Targets Std                 873.512
trainer/Q Targets Max                3537.69
trainer/Q Targets Min                -136.782
trainer/Bellman Errors Mean         25048.8
trainer/Bellman Errors Std         130546
trainer/Bellman Errors Max              1.40427e+06
trainer/Bellman Errors Min              0.0208188
trainer/Policy Action Mean             -0.0482474
trainer/Policy Action Std               0.840805
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         60000
exploration/num paths total          1273
exploration/path length Mean           71.4286
exploration/path length Std            62.1803
exploration/path length Max           199
exploration/path length Min             8
exploration/Rewards Mean                0.561303
exploration/Rewards Std                 0.374743
exploration/Rewards Max                 1.84211
exploration/Rewards Min                -0.394275
exploration/Returns Mean               40.0931
exploration/Returns Std                37.0474
exploration/Returns Max               119.229
exploration/Returns Min                 3.88452
exploration/Actions Mean                0.187863
exploration/Actions Std                 0.593822
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  14
exploration/Average Returns            40.0931
evaluation/num steps total          44929
evaluation/num paths total            530
evaluation/path length Mean           423.5
evaluation/path length Std             90.5
evaluation/path length Max            514
evaluation/path length Min            333
evaluation/Rewards Mean                 0.724409
evaluation/Rewards Std                  0.31437
evaluation/Rewards Max                  1.55735
evaluation/Rewards Min                 -0.129128
evaluation/Returns Mean               306.787
evaluation/Returns Std                 86.0574
evaluation/Returns Max                392.845
evaluation/Returns Min                220.73
evaluation/Actions Mean                 0.135882
evaluation/Actions Std                  0.601084
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    2
evaluation/Average Returns            306.787
time/data storing (s)                   0.0035404
time/evaluation sampling (s)            0.414665
time/exploration sampling (s)           0.417126
time/logging (s)                        0.00381484
time/saving (s)                         0.00152918
time/training (s)                       7.26778
time/epoch (s)                          8.10846
time/total (s)                        395.73
Epoch                                  49
---------------------------------  ----------------
2021-07-02 23:43:04.766283 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 50 finished
---------------------------------  ---------------
replay_buffer/size                  61000
trainer/QF Loss                         0.00143047
trainer/Policy Loss                 -2635.92
trainer/Raw Policy Loss             -2635.92
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2554.24
trainer/Q Predictions Std             861.701
trainer/Q Predictions Max            3651.82
trainer/Q Predictions Min             -36.667
trainer/Q Targets Mean               2544.87
trainer/Q Targets Std                 862.862
trainer/Q Targets Max                3616.57
trainer/Q Targets Min                  -0.349644
trainer/Bellman Errors Mean          7812.16
trainer/Bellman Errors Std          27356.2
trainer/Bellman Errors Max         269988
trainer/Bellman Errors Min              0.0292898
trainer/Policy Action Mean             -0.0974844
trainer/Policy Action Std               0.848655
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         61000
exploration/num paths total          1283
exploration/path length Mean          100
exploration/path length Std            66.7383
exploration/path length Max           238
exploration/path length Min             9
exploration/Rewards Mean                0.482661
exploration/Rewards Std                 0.324735
exploration/Rewards Max                 1.47156
exploration/Rewards Min                -0.284341
exploration/Returns Mean               48.2661
exploration/Returns Std                31.6179
exploration/Returns Max               111.494
exploration/Returns Min                 5.98797
exploration/Actions Mean                0.158769
exploration/Actions Std                 0.565381
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  10
exploration/Average Returns            48.2661
evaluation/num steps total          45703
evaluation/num paths total            532
evaluation/path length Mean           387
evaluation/path length Std             15
evaluation/path length Max            402
evaluation/path length Min            372
evaluation/Rewards Mean                 0.922749
evaluation/Rewards Std                  0.245849
evaluation/Rewards Max                  1.54745
evaluation/Rewards Min                  0.241128
evaluation/Returns Mean               357.104
evaluation/Returns Std                 14.9469
evaluation/Returns Max                372.051
evaluation/Returns Min                342.157
evaluation/Actions Mean                 0.395476
evaluation/Actions Std                  0.407842
evaluation/Actions Max                  0.999819
evaluation/Actions Min                 -0.999993
evaluation/Num Paths                    2
evaluation/Average Returns            357.104
time/data storing (s)                   0.00346629
time/evaluation sampling (s)            0.399107
time/exploration sampling (s)           0.403601
time/logging (s)                        0.00550866
time/saving (s)                         0.00204747
time/training (s)                       7.55894
time/epoch (s)                          8.37267
time/total (s)                        404.104
Epoch                                  50
---------------------------------  ---------------
2021-07-02 23:43:13.266065 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 51 finished
---------------------------------  ----------------
replay_buffer/size                  62000
trainer/QF Loss                         0.00127326
trainer/Policy Loss                 -2819.5
trainer/Raw Policy Loss             -2819.5
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2689.14
trainer/Q Predictions Std             890.824
trainer/Q Predictions Max            3731.92
trainer/Q Predictions Min             -43.8009
trainer/Q Targets Mean               2709.95
trainer/Q Targets Std                 911.483
trainer/Q Targets Max                3759.94
trainer/Q Targets Min                 -46.2913
trainer/Bellman Errors Mean         20407.8
trainer/Bellman Errors Std         104512
trainer/Bellman Errors Max              1.15222e+06
trainer/Bellman Errors Min              0.0591288
trainer/Policy Action Mean             -0.113355
trainer/Policy Action Std               0.852132
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         62000
exploration/num paths total          1295
exploration/path length Mean           83.3333
exploration/path length Std            65.1119
exploration/path length Max           164
exploration/path length Min             2
exploration/Rewards Mean                0.737946
exploration/Rewards Std                 0.367347
exploration/Rewards Max                 2.00683
exploration/Rewards Min                -0.29558
exploration/Returns Mean               61.4955
exploration/Returns Std                52.4108
exploration/Returns Max               137.297
exploration/Returns Min                 1.60073
exploration/Actions Mean                0.114898
exploration/Actions Std                 0.560012
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  12
exploration/Average Returns            61.4955
evaluation/num steps total          46703
evaluation/num paths total            533
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 0.978564
evaluation/Rewards Std                  0.0981508
evaluation/Rewards Max                  1.19789
evaluation/Rewards Min                  0.422625
evaluation/Returns Mean               978.564
evaluation/Returns Std                  0
evaluation/Returns Max                978.564
evaluation/Returns Min                978.564
evaluation/Actions Mean                 0.0771709
evaluation/Actions Std                  0.378469
evaluation/Actions Max                  0.907344
evaluation/Actions Min                 -0.999997
evaluation/Num Paths                    1
evaluation/Average Returns            978.564
time/data storing (s)                   0.00381807
time/evaluation sampling (s)            0.513256
time/exploration sampling (s)           0.445677
time/logging (s)                        0.00399419
time/saving (s)                         0.00151419
time/training (s)                       7.5271
time/epoch (s)                          8.49536
time/total (s)                        412.602
Epoch                                  51
---------------------------------  ----------------
2021-07-02 23:43:21.702545 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 52 finished
---------------------------------  ---------------
replay_buffer/size                 63000
trainer/QF Loss                        0.00126067
trainer/Policy Loss                -2774.12
trainer/Raw Policy Loss            -2774.12
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean          2653.21
trainer/Q Predictions Std            927.247
trainer/Q Predictions Max           3846.3
trainer/Q Predictions Min           -175.028
trainer/Q Targets Mean              2664.94
trainer/Q Targets Std                952.109
trainer/Q Targets Max               3940.6
trainer/Q Targets Min               -215.531
trainer/Bellman Errors Mean        18520.3
trainer/Bellman Errors Std         88614.8
trainer/Bellman Errors Max             1.14631e+06
trainer/Bellman Errors Min             0.040372
trainer/Policy Action Mean            -0.0374584
trainer/Policy Action Std              0.85207
trainer/Policy Action Max              1
trainer/Policy Action Min             -1
exploration/num steps total        63000
exploration/num paths total         1304
exploration/path length Mean         111.111
exploration/path length Std          109.976
exploration/path length Max          318
exploration/path length Min            9
exploration/Rewards Mean               1.17496
exploration/Rewards Std                0.92883
exploration/Rewards Max                5.29154
exploration/Rewards Min                0.0500207
exploration/Returns Mean             130.551
exploration/Returns Std              193.378
exploration/Returns Max              642.206
exploration/Returns Min                5.39503
exploration/Actions Mean               0.0699166
exploration/Actions Std                0.570339
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                  9
exploration/Average Returns          130.551
evaluation/num steps total         47703
evaluation/num paths total           534
evaluation/path length Mean         1000
evaluation/path length Std             0
evaluation/path length Max          1000
evaluation/path length Min          1000
evaluation/Rewards Mean                0.986114
evaluation/Rewards Std                 0.101128
evaluation/Rewards Max                 1.41361
evaluation/Rewards Min                 0.422332
evaluation/Returns Mean              986.114
evaluation/Returns Std                 0
evaluation/Returns Max               986.114
evaluation/Returns Min               986.114
evaluation/Actions Mean                0.0202135
evaluation/Actions Std                 0.159293
evaluation/Actions Max                 0.902956
evaluation/Actions Min                -0.999997
evaluation/Num Paths                   1
evaluation/Average Returns           986.114
time/data storing (s)                  0.00375726
time/evaluation sampling (s)           0.40277
time/exploration sampling (s)          0.433005
time/logging (s)                       0.00600349
time/saving (s)                        0.00206889
time/training (s)                      7.58859
time/epoch (s)                         8.43619
time/total (s)                       421.04
Epoch                                 52
---------------------------------  ---------------
2021-07-02 23:43:29.877960 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 53 finished
---------------------------------  ----------------
replay_buffer/size                  64000
trainer/QF Loss                         0.00167328
trainer/Policy Loss                 -3010.12
trainer/Raw Policy Loss             -3010.12
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2874.59
trainer/Q Predictions Std             889.974
trainer/Q Predictions Max            4053.09
trainer/Q Predictions Min             -64.3346
trainer/Q Targets Mean               2872.91
trainer/Q Targets Std                 902.954
trainer/Q Targets Max                3896.68
trainer/Q Targets Min                 -93.9936
trainer/Bellman Errors Mean         26403
trainer/Bellman Errors Std         223235
trainer/Bellman Errors Max              3.50576e+06
trainer/Bellman Errors Min              0.0224707
trainer/Policy Action Mean             -0.0427508
trainer/Policy Action Std               0.848083
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         64000
exploration/num paths total          1311
exploration/path length Mean          142.857
exploration/path length Std           124.134
exploration/path length Max           388
exploration/path length Min            35
exploration/Rewards Mean                1.08999
exploration/Rewards Std                 0.657583
exploration/Rewards Max                 3.94122
exploration/Rewards Min                 0.257551
exploration/Returns Mean              155.713
exploration/Returns Std               168.216
exploration/Returns Max               535.114
exploration/Returns Min                22.1059
exploration/Actions Mean                0.0378273
exploration/Actions Std                 0.575434
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                   7
exploration/Average Returns           155.713
evaluation/num steps total          48703
evaluation/num paths total            535
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 0.988436
evaluation/Rewards Std                  0.0702652
evaluation/Rewards Max                  1.11966
evaluation/Rewards Min                  0.484896
evaluation/Returns Mean               988.436
evaluation/Returns Std                  0
evaluation/Returns Max                988.436
evaluation/Returns Min                988.436
evaluation/Actions Mean                 0.0110264
evaluation/Actions Std                  0.0897711
evaluation/Actions Max                  0.857042
evaluation/Actions Min                 -0.999994
evaluation/Num Paths                    1
evaluation/Average Returns            988.436
time/data storing (s)                   0.00348428
time/evaluation sampling (s)            0.504568
time/exploration sampling (s)           0.426053
time/logging (s)                        0.00412701
time/saving (s)                         0.00159793
time/training (s)                       7.23062
time/epoch (s)                          8.17045
time/total (s)                        429.213
Epoch                                  53
---------------------------------  ----------------
2021-07-02 23:43:38.103961 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 54 finished
---------------------------------  ----------------
replay_buffer/size                  65000
trainer/QF Loss                         0.00162172
trainer/Policy Loss                 -2949.43
trainer/Raw Policy Loss             -2949.43
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2825.07
trainer/Q Predictions Std            1117.25
trainer/Q Predictions Max            4506.52
trainer/Q Predictions Min            -130.633
trainer/Q Targets Mean               2809.08
trainer/Q Targets Std                1128.98
trainer/Q Targets Max                4394.6
trainer/Q Targets Min                -138.173
trainer/Bellman Errors Mean         16950.7
trainer/Bellman Errors Std          68062.5
trainer/Bellman Errors Max         859178
trainer/Bellman Errors Min              0.000920571
trainer/Policy Action Mean              0.0307863
trainer/Policy Action Std               0.84958
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         65000
exploration/num paths total          1318
exploration/path length Mean          142.857
exploration/path length Std           121.801
exploration/path length Max           350
exploration/path length Min            24
exploration/Rewards Mean                0.97638
exploration/Rewards Std                 0.726529
exploration/Rewards Max                 4.1774
exploration/Rewards Min                -0.104239
exploration/Returns Mean              139.483
exploration/Returns Std               156.504
exploration/Returns Max               481.304
exploration/Returns Min                13.3769
exploration/Actions Mean                0.0617843
exploration/Actions Std                 0.572289
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                   7
exploration/Average Returns           139.483
evaluation/num steps total          49703
evaluation/num paths total            536
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 0.989977
evaluation/Rewards Std                  0.06569
evaluation/Rewards Max                  1.06648
evaluation/Rewards Min                  0.488969
evaluation/Returns Mean               989.977
evaluation/Returns Std                  0
evaluation/Returns Max                989.977
evaluation/Returns Min                989.977
evaluation/Actions Mean                 0.0108342
evaluation/Actions Std                  0.0883874
evaluation/Actions Max                  0.846381
evaluation/Actions Min                 -0.99999
evaluation/Num Paths                    1
evaluation/Average Returns            989.977
time/data storing (s)                   0.00620337
time/evaluation sampling (s)            0.400726
time/exploration sampling (s)           0.436578
time/logging (s)                        0.00414607
time/saving (s)                         0.00203382
time/training (s)                       7.3736
time/epoch (s)                          8.22329
time/total (s)                        437.438
Epoch                                  54
---------------------------------  ----------------
2021-07-02 23:43:46.178447 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 55 finished
---------------------------------  ----------------
replay_buffer/size                  66000
trainer/QF Loss                         0.00108685
trainer/Policy Loss                 -3157.52
trainer/Raw Policy Loss             -3157.52
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2996.67
trainer/Q Predictions Std            1029.56
trainer/Q Predictions Max            4287.54
trainer/Q Predictions Min             -80.7373
trainer/Q Targets Mean               2966.54
trainer/Q Targets Std                1062.15
trainer/Q Targets Max                4230.05
trainer/Q Targets Min                 -99.5591
trainer/Bellman Errors Mean         24277.8
trainer/Bellman Errors Std         175969
trainer/Bellman Errors Max              2.56944e+06
trainer/Bellman Errors Min              0.0920921
trainer/Policy Action Mean             -0.0425831
trainer/Policy Action Std               0.846646
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         66000
exploration/num paths total          1325
exploration/path length Mean          142.857
exploration/path length Std           126.044
exploration/path length Max           444
exploration/path length Min            58
exploration/Rewards Mean                0.886316
exploration/Rewards Std                 0.326153
exploration/Rewards Max                 2.12558
exploration/Rewards Min                 0.107884
exploration/Returns Mean              126.617
exploration/Returns Std               109.333
exploration/Returns Max               381.963
exploration/Returns Min                38.7391
exploration/Actions Mean                0.0699718
exploration/Actions Std                 0.5358
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                   7
exploration/Average Returns           126.617
evaluation/num steps total          50703
evaluation/num paths total            537
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 0.99393
evaluation/Rewards Std                  0.053891
evaluation/Rewards Max                  1.08187
evaluation/Rewards Min                  0.531696
evaluation/Returns Mean               993.93
evaluation/Returns Std                  0
evaluation/Returns Max                993.93
evaluation/Returns Min                993.93
evaluation/Actions Mean                 0.00575799
evaluation/Actions Std                  0.0869508
evaluation/Actions Max                  0.832622
evaluation/Actions Min                 -0.999974
evaluation/Num Paths                    1
evaluation/Average Returns            993.93
time/data storing (s)                   0.00353869
time/evaluation sampling (s)            0.441509
time/exploration sampling (s)           0.403123
time/logging (s)                        0.0059454
time/saving (s)                         0.00203775
time/training (s)                       7.21764
time/epoch (s)                          8.0738
time/total (s)                        445.514
Epoch                                  55
---------------------------------  ----------------
2021-07-02 23:43:54.403391 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 56 finished
---------------------------------  ----------------
replay_buffer/size                  67000
trainer/QF Loss                         0.00142937
trainer/Policy Loss                 -3289.36
trainer/Raw Policy Loss             -3289.36
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           3163.03
trainer/Q Predictions Std            1000.04
trainer/Q Predictions Max            4534.94
trainer/Q Predictions Min            -116.938
trainer/Q Targets Mean               3159.2
trainer/Q Targets Std                1035.64
trainer/Q Targets Max                4620.28
trainer/Q Targets Min                 -98.398
trainer/Bellman Errors Mean         36997.9
trainer/Bellman Errors Std         321831
trainer/Bellman Errors Max              4.91881e+06
trainer/Bellman Errors Min              0.0414584
trainer/Policy Action Mean             -0.0153499
trainer/Policy Action Std               0.83304
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         67000
exploration/num paths total          1334
exploration/path length Mean          111.111
exploration/path length Std           107.196
exploration/path length Max           315
exploration/path length Min             9
exploration/Rewards Mean                0.993051
exploration/Rewards Std                 0.563963
exploration/Rewards Max                 3.23318
exploration/Rewards Min                -0.516733
exploration/Returns Mean              110.339
exploration/Returns Std               112.216
exploration/Returns Max               318.209
exploration/Returns Min                 7.01911
exploration/Actions Mean                0.0929649
exploration/Actions Std                 0.579692
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                   9
exploration/Average Returns           110.339
evaluation/num steps total          51703
evaluation/num paths total            538
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 0.996211
evaluation/Rewards Std                  0.0557876
evaluation/Rewards Max                  1.11371
evaluation/Rewards Min                  0.555269
evaluation/Returns Mean               996.211
evaluation/Returns Std                  0
evaluation/Returns Max                996.211
evaluation/Returns Min                996.211
evaluation/Actions Mean                 0.00301631
evaluation/Actions Std                  0.092447
evaluation/Actions Max                  0.894297
evaluation/Actions Min                 -0.999979
evaluation/Num Paths                    1
evaluation/Average Returns            996.211
time/data storing (s)                   0.00359172
time/evaluation sampling (s)            0.460593
time/exploration sampling (s)           0.430498
time/logging (s)                        0.00685637
time/saving (s)                         0.00225789
time/training (s)                       7.31882
time/epoch (s)                          8.22262
time/total (s)                        453.739
Epoch                                  56
---------------------------------  ----------------
2021-07-02 23:44:02.733558 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 57 finished
---------------------------------  ---------------
replay_buffer/size                  68000
trainer/QF Loss                         0.00156837
trainer/Policy Loss                 -3278.96
trainer/Raw Policy Loss             -3278.96
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           3142.79
trainer/Q Predictions Std            1098.82
trainer/Q Predictions Max            4749.17
trainer/Q Predictions Min            -177.526
trainer/Q Targets Mean               3131.02
trainer/Q Targets Std                1101.5
trainer/Q Targets Max                4809.5
trainer/Q Targets Min                  -1.14013
trainer/Bellman Errors Mean         16969.1
trainer/Bellman Errors Std          72491.3
trainer/Bellman Errors Max         735309
trainer/Bellman Errors Min              0.065464
trainer/Policy Action Mean              0.0681099
trainer/Policy Action Std               0.832082
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         68000
exploration/num paths total          1342
exploration/path length Mean          125
exploration/path length Std           141.326
exploration/path length Max           479
exploration/path length Min            20
exploration/Rewards Mean                0.994519
exploration/Rewards Std                 0.396191
exploration/Rewards Max                 2.18024
exploration/Rewards Min                 0.0167343
exploration/Returns Mean              124.315
exploration/Returns Std               163.014
exploration/Returns Max               534.863
exploration/Returns Min                11.616
exploration/Actions Mean                0.112469
exploration/Actions Std                 0.593712
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                   8
exploration/Average Returns           124.315
evaluation/num steps total          52703
evaluation/num paths total            539
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 0.981613
evaluation/Rewards Std                  0.136964
evaluation/Rewards Max                  1.21211
evaluation/Rewards Min                  0.599703
evaluation/Returns Mean               981.613
evaluation/Returns Std                  0
evaluation/Returns Max                981.613
evaluation/Returns Min                981.613
evaluation/Actions Mean                 0.0335192
evaluation/Actions Std                  0.51121
evaluation/Actions Max                  0.968019
evaluation/Actions Min                 -0.999358
evaluation/Num Paths                    1
evaluation/Average Returns            981.613
time/data storing (s)                   0.00349967
time/evaluation sampling (s)            0.43878
time/exploration sampling (s)           0.413425
time/logging (s)                        0.0053201
time/saving (s)                         0.00187847
time/training (s)                       7.46232
time/epoch (s)                          8.32522
time/total (s)                        462.067
Epoch                                  57
---------------------------------  ---------------
2021-07-02 23:44:10.889738 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 58 finished
---------------------------------  ---------------
replay_buffer/size                 69000
trainer/QF Loss                        0.000944422
trainer/Policy Loss                -3406.82
trainer/Raw Policy Loss            -3406.82
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean          3279.37
trainer/Q Predictions Std           1018.63
trainer/Q Predictions Max           4804.29
trainer/Q Predictions Min           -160.961
trainer/Q Targets Mean              3319.35
trainer/Q Targets Std               1029.17
trainer/Q Targets Max               4851.48
trainer/Q Targets Min               -153.185
trainer/Bellman Errors Mean        15999.6
trainer/Bellman Errors Std         72138.9
trainer/Bellman Errors Max             1.03344e+06
trainer/Bellman Errors Min             0.105435
trainer/Policy Action Mean             0.072691
trainer/Policy Action Std              0.811951
trainer/Policy Action Max              1
trainer/Policy Action Min             -1
exploration/num steps total        69000
exploration/num paths total         1349
exploration/path length Mean         142.857
exploration/path length Std          130.652
exploration/path length Max          429
exploration/path length Min           10
exploration/Rewards Mean               1.24567
exploration/Rewards Std                0.746265
exploration/Rewards Max                4.95814
exploration/Rewards Min                0.0352751
exploration/Returns Mean             177.953
exploration/Returns Std              142.719
exploration/Returns Max              363.755
exploration/Returns Min                7.01517
exploration/Actions Mean               0.043867
exploration/Actions Std                0.617683
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                  7
exploration/Average Returns          177.953
evaluation/num steps total         53703
evaluation/num paths total           540
evaluation/path length Mean         1000
evaluation/path length Std             0
evaluation/path length Max          1000
evaluation/path length Min          1000
evaluation/Rewards Mean                0.965205
evaluation/Rewards Std                 0.192928
evaluation/Rewards Max                 1.73198
evaluation/Rewards Min                 0.383626
evaluation/Returns Mean              965.205
evaluation/Returns Std                 0
evaluation/Returns Max               965.205
evaluation/Returns Min               965.205
evaluation/Actions Mean                0.0798342
evaluation/Actions Std                 0.488997
evaluation/Actions Max                 0.995802
evaluation/Actions Min                -0.99892
evaluation/Num Paths                   1
evaluation/Average Returns           965.205
time/data storing (s)                  0.0060648
time/evaluation sampling (s)           0.430862
time/exploration sampling (s)          0.555162
time/logging (s)                       0.00418588
time/saving (s)                        0.00196608
time/training (s)                      7.15408
time/epoch (s)                         8.15232
time/total (s)                       470.221
Epoch                                 58
---------------------------------  ---------------
2021-07-02 23:44:19.260659 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 59 finished
---------------------------------  ----------------
replay_buffer/size                  70000
trainer/QF Loss                         0.00115847
trainer/Policy Loss                 -3508.74
trainer/Raw Policy Loss             -3508.74
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           3321.41
trainer/Q Predictions Std            1143.6
trainer/Q Predictions Max            4689.59
trainer/Q Predictions Min              37.8469
trainer/Q Targets Mean               3318.19
trainer/Q Targets Std                1169.56
trainer/Q Targets Max                4645.5
trainer/Q Targets Min                   0.11971
trainer/Bellman Errors Mean         36239.3
trainer/Bellman Errors Std         282387
trainer/Bellman Errors Max              3.42389e+06
trainer/Bellman Errors Min              0.215626
trainer/Policy Action Mean              0.0729117
trainer/Policy Action Std               0.814153
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         70000
exploration/num paths total          1356
exploration/path length Mean          142.857
exploration/path length Std           161.569
exploration/path length Max           504
exploration/path length Min            15
exploration/Rewards Mean                0.913201
exploration/Rewards Std                 0.62804
exploration/Rewards Max                 3.19274
exploration/Rewards Min                -0.228341
exploration/Returns Mean              130.457
exploration/Returns Std               142.523
exploration/Returns Max               349.077
exploration/Returns Min                 7.40725
exploration/Actions Mean                0.0676339
exploration/Actions Std                 0.584293
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                   7
exploration/Average Returns           130.457
evaluation/num steps total          54049
evaluation/num paths total            541
evaluation/path length Mean           346
evaluation/path length Std              0
evaluation/path length Max            346
evaluation/path length Min            346
evaluation/Rewards Mean                 1.85183
evaluation/Rewards Std                  1.43473
evaluation/Rewards Max                  6.50669
evaluation/Rewards Min                  0.357703
evaluation/Returns Mean               640.732
evaluation/Returns Std                  0
evaluation/Returns Max                640.732
evaluation/Returns Min                640.732
evaluation/Actions Mean                 0.0484611
evaluation/Actions Std                  0.706059
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    1
evaluation/Average Returns            640.732
time/data storing (s)                   0.00345936
time/evaluation sampling (s)            0.40149
time/exploration sampling (s)           0.404432
time/logging (s)                        0.00437657
time/saving (s)                         0.00203641
time/training (s)                       7.55298
time/epoch (s)                          8.36877
time/total (s)                        478.591
Epoch                                  59
---------------------------------  ----------------
2021-07-02 23:44:27.272777 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 60 finished
---------------------------------  ----------------
replay_buffer/size                  71000
trainer/QF Loss                         0.000930846
trainer/Policy Loss                 -3609.36
trainer/Raw Policy Loss             -3609.36
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           3485.67
trainer/Q Predictions Std            1170.18
trainer/Q Predictions Max            4936.18
trainer/Q Predictions Min            -121.691
trainer/Q Targets Mean               3480.88
trainer/Q Targets Std                1196.1
trainer/Q Targets Max                4931.66
trainer/Q Targets Min                  -0.968997
trainer/Bellman Errors Mean         26775.2
trainer/Bellman Errors Std         195565
trainer/Bellman Errors Max              3.03354e+06
trainer/Bellman Errors Min              0.00925279
trainer/Policy Action Mean              0.0403479
trainer/Policy Action Std               0.821497
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         71000
exploration/num paths total          1365
exploration/path length Mean          111.111
exploration/path length Std            76.841
exploration/path length Max           264
exploration/path length Min            11
exploration/Rewards Mean                1.35751
exploration/Rewards Std                 0.739501
exploration/Rewards Max                 5.44167
exploration/Rewards Min                 0.131867
exploration/Returns Mean              150.835
exploration/Returns Std               142.934
exploration/Returns Max               452.037
exploration/Returns Min                 9.83949
exploration/Actions Mean                0.0704182
exploration/Actions Std                 0.625187
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                   9
exploration/Average Returns           150.835
evaluation/num steps total          54846
evaluation/num paths total            544
evaluation/path length Mean           265.667
evaluation/path length Std             12.6579
evaluation/path length Max            281
evaluation/path length Min            250
evaluation/Rewards Mean                 1.88678
evaluation/Rewards Std                  1.01663
evaluation/Rewards Max                  4.61449
evaluation/Rewards Min                  0.438699
evaluation/Returns Mean               501.254
evaluation/Returns Std                 18.4512
evaluation/Returns Max                515.382
evaluation/Returns Min                475.19
evaluation/Actions Mean                -0.0525944
evaluation/Actions Std                  0.725501
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    3
evaluation/Average Returns            501.254
time/data storing (s)                   0.00353006
time/evaluation sampling (s)            0.481416
time/exploration sampling (s)           0.486394
time/logging (s)                        0.00554795
time/saving (s)                         0.0020481
time/training (s)                       7.03164
time/epoch (s)                          8.01058
time/total (s)                        486.604
Epoch                                  60
---------------------------------  ----------------
2021-07-02 23:44:35.494146 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 61 finished
---------------------------------  ----------------
replay_buffer/size                  72000
trainer/QF Loss                         0.00106246
trainer/Policy Loss                 -3816.25
trainer/Raw Policy Loss             -3816.25
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           3647.9
trainer/Q Predictions Std            1102.67
trainer/Q Predictions Max            4968.23
trainer/Q Predictions Min             -61.2031
trainer/Q Targets Mean               3642.86
trainer/Q Targets Std                1134.84
trainer/Q Targets Max                5099.37
trainer/Q Targets Min                 -50.4136
trainer/Bellman Errors Mean         31764.7
trainer/Bellman Errors Std         246834
trainer/Bellman Errors Max              3.85821e+06
trainer/Bellman Errors Min              0.0518852
trainer/Policy Action Mean              0.045187
trainer/Policy Action Std               0.838215
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         72000
exploration/num paths total          1375
exploration/path length Mean          100
exploration/path length Std            89.8321
exploration/path length Max           253
exploration/path length Min             9
exploration/Rewards Mean                1.51089
exploration/Rewards Std                 0.640037
exploration/Rewards Max                 3.30655
exploration/Rewards Min                 0.157746
exploration/Returns Mean              151.089
exploration/Returns Std               155.837
exploration/Returns Max               507.553
exploration/Returns Min                 4.89607
exploration/Actions Mean                0.138066
exploration/Actions Std                 0.652105
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  10
exploration/Average Returns           151.089
evaluation/num steps total          55780
evaluation/num paths total            547
evaluation/path length Mean           311.333
evaluation/path length Std             25.8242
evaluation/path length Max            337
evaluation/path length Min            276
evaluation/Rewards Mean                 2.61269
evaluation/Rewards Std                  0.903649
evaluation/Rewards Max                  6.5042
evaluation/Rewards Min                  0.660602
evaluation/Returns Mean               813.418
evaluation/Returns Std                 68.1136
evaluation/Returns Max                893.247
evaluation/Returns Min                726.816
evaluation/Actions Mean                 0.00718218
evaluation/Actions Std                  0.750223
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    3
evaluation/Average Returns            813.418
time/data storing (s)                   0.00611697
time/evaluation sampling (s)            0.450491
time/exploration sampling (s)           0.424089
time/logging (s)                        0.00392374
time/saving (s)                         0.00183344
time/training (s)                       7.3302
time/epoch (s)                          8.21665
time/total (s)                        494.823
Epoch                                  61
---------------------------------  ----------------
2021-07-02 23:44:43.589423 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 62 finished
---------------------------------  ----------------
replay_buffer/size                  73000
trainer/QF Loss                         0.0010046
trainer/Policy Loss                 -3864.41
trainer/Raw Policy Loss             -3864.41
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           3728.19
trainer/Q Predictions Std            1188.97
trainer/Q Predictions Max            5125.3
trainer/Q Predictions Min            -105.357
trainer/Q Targets Mean               3729.06
trainer/Q Targets Std                1212.79
trainer/Q Targets Max                5126.45
trainer/Q Targets Min                -103.619
trainer/Bellman Errors Mean         47879.7
trainer/Bellman Errors Std         538486
trainer/Bellman Errors Max              8.56197e+06
trainer/Bellman Errors Min              0.020468
trainer/Policy Action Mean             -0.0698613
trainer/Policy Action Std               0.829574
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         73000
exploration/num paths total          1387
exploration/path length Mean           83.3333
exploration/path length Std            71.5685
exploration/path length Max           262
exploration/path length Min            17
exploration/Rewards Mean                0.952891
exploration/Rewards Std                 0.405383
exploration/Rewards Max                 2.21711
exploration/Rewards Min                -0.0148654
exploration/Returns Mean               79.4075
exploration/Returns Std                71.1479
exploration/Returns Max               265.244
exploration/Returns Min                14.2662
exploration/Actions Mean                0.0988099
exploration/Actions Std                 0.637024
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  12
exploration/Average Returns            79.4075
evaluation/num steps total          56780
evaluation/num paths total            548
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 0.77171
evaluation/Rewards Std                  0.397124
evaluation/Rewards Max                  1.82355
evaluation/Rewards Min                 -0.0543424
evaluation/Returns Mean               771.71
evaluation/Returns Std                  0
evaluation/Returns Max                771.71
evaluation/Returns Min                771.71
evaluation/Actions Mean                 0.0455933
evaluation/Actions Std                  0.545906
evaluation/Actions Max                  1
evaluation/Actions Min                 -0.999996
evaluation/Num Paths                    1
evaluation/Average Returns            771.71
time/data storing (s)                   0.00359246
time/evaluation sampling (s)            0.421964
time/exploration sampling (s)           0.413722
time/logging (s)                        0.00405249
time/saving (s)                         0.00152173
time/training (s)                       7.24825
time/epoch (s)                          8.0931
time/total (s)                        502.918
Epoch                                  62
---------------------------------  ----------------
2021-07-02 23:44:51.570644 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 63 finished
---------------------------------  ---------------
replay_buffer/size                  74000
trainer/QF Loss                         0.00121141
trainer/Policy Loss                 -3863.22
trainer/Raw Policy Loss             -3863.22
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           3688.6
trainer/Q Predictions Std            1216.15
trainer/Q Predictions Max            5066.03
trainer/Q Predictions Min            -117.804
trainer/Q Targets Mean               3680.44
trainer/Q Targets Std                1221.91
trainer/Q Targets Max                5051.36
trainer/Q Targets Min                 -70.779
trainer/Bellman Errors Mean         25757
trainer/Bellman Errors Std         160924
trainer/Bellman Errors Max              2.1723e+06
trainer/Bellman Errors Min              0.221102
trainer/Policy Action Mean              0.0647138
trainer/Policy Action Std               0.832943
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         74000
exploration/num paths total          1394
exploration/path length Mean          142.857
exploration/path length Std           148.304
exploration/path length Max           483
exploration/path length Min             9
exploration/Rewards Mean                1.07567
exploration/Rewards Std                 0.748111
exploration/Rewards Max                 3.78148
exploration/Rewards Min                -0.472175
exploration/Returns Mean              153.668
exploration/Returns Std               156.086
exploration/Returns Max               448.308
exploration/Returns Min                 7.66704
exploration/Actions Mean                0.115287
exploration/Actions Std                 0.63255
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                   7
exploration/Average Returns           153.668
evaluation/num steps total          57721
evaluation/num paths total            550
evaluation/path length Mean           470.5
evaluation/path length Std            106.5
evaluation/path length Max            577
evaluation/path length Min            364
evaluation/Rewards Mean                 1.82151
evaluation/Rewards Std                  1.08434
evaluation/Rewards Max                  6.5414
evaluation/Rewards Min                  0.121504
evaluation/Returns Mean               857.02
evaluation/Returns Std                104.6
evaluation/Returns Max                961.62
evaluation/Returns Min                752.419
evaluation/Actions Mean                -0.0213112
evaluation/Actions Std                  0.711587
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    2
evaluation/Average Returns            857.02
time/data storing (s)                   0.00620593
time/evaluation sampling (s)            0.419785
time/exploration sampling (s)           0.392884
time/logging (s)                        0.00588981
time/saving (s)                         0.00205487
time/training (s)                       7.15391
time/epoch (s)                          7.98073
time/total (s)                        510.9
Epoch                                  63
---------------------------------  ---------------
2021-07-02 23:45:00.059858 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 64 finished
---------------------------------  ----------------
replay_buffer/size                  75000
trainer/QF Loss                         0.00084242
trainer/Policy Loss                 -4054.64
trainer/Raw Policy Loss             -4054.64
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           3886.62
trainer/Q Predictions Std            1280.27
trainer/Q Predictions Max            5526.9
trainer/Q Predictions Min             -20.9749
trainer/Q Targets Mean               3871.77
trainer/Q Targets Std                1325.79
trainer/Q Targets Max                5474.16
trainer/Q Targets Min                 -29.9832
trainer/Bellman Errors Mean         98267.1
trainer/Bellman Errors Std         801200
trainer/Bellman Errors Max              1.09797e+07
trainer/Bellman Errors Min              0.00197434
trainer/Policy Action Mean             -0.0219699
trainer/Policy Action Std               0.83569
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         75000
exploration/num paths total          1404
exploration/path length Mean          100
exploration/path length Std            78.6753
exploration/path length Max           273
exploration/path length Min             9
exploration/Rewards Mean                1.31791
exploration/Rewards Std                 0.648013
exploration/Rewards Max                 3.06336
exploration/Rewards Min                -0.21012
exploration/Returns Mean              131.791
exploration/Returns Std               119.454
exploration/Returns Max               369.043
exploration/Returns Min                 6.61586
exploration/Actions Mean                0.111466
exploration/Actions Std                 0.662526
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  10
exploration/Average Returns           131.791
evaluation/num steps total          58407
evaluation/num paths total            552
evaluation/path length Mean           343
evaluation/path length Std            138
evaluation/path length Max            481
evaluation/path length Min            205
evaluation/Rewards Mean                 1.95021
evaluation/Rewards Std                  0.990664
evaluation/Rewards Max                  4.76074
evaluation/Rewards Min                  0.142562
evaluation/Returns Mean               668.921
evaluation/Returns Std                236.426
evaluation/Returns Max                905.347
evaluation/Returns Min                432.495
evaluation/Actions Mean                 0.0252788
evaluation/Actions Std                  0.742802
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    2
evaluation/Average Returns            668.921
time/data storing (s)                   0.00349938
time/evaluation sampling (s)            0.471804
time/exploration sampling (s)           0.399063
time/logging (s)                        0.00350289
time/saving (s)                         0.00162211
time/training (s)                       7.6042
time/epoch (s)                          8.4837
time/total (s)                        519.386
Epoch                                  64
---------------------------------  ----------------
2021-07-02 23:45:08.557015 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 65 finished
---------------------------------  ----------------
replay_buffer/size                  76000
trainer/QF Loss                         0.00100293
trainer/Policy Loss                 -4149.88
trainer/Raw Policy Loss             -4149.88
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           4029.26
trainer/Q Predictions Std            1179.43
trainer/Q Predictions Max            5420.63
trainer/Q Predictions Min             -63.7095
trainer/Q Targets Mean               4020.45
trainer/Q Targets Std                1235.56
trainer/Q Targets Max                5388.33
trainer/Q Targets Min                 -75.1203
trainer/Bellman Errors Mean         82613.4
trainer/Bellman Errors Std         585896
trainer/Bellman Errors Max              8.14676e+06
trainer/Bellman Errors Min              0.000515521
trainer/Policy Action Mean              0.0155985
trainer/Policy Action Std               0.820265
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         76000
exploration/num paths total          1411
exploration/path length Mean          142.857
exploration/path length Std            51.927
exploration/path length Max           211
exploration/path length Min            59
exploration/Rewards Mean                1.12306
exploration/Rewards Std                 1.04818
exploration/Rewards Max                 6.27548
exploration/Rewards Min                -0.285279
exploration/Returns Mean              160.437
exploration/Returns Std               132.656
exploration/Returns Max               474.155
exploration/Returns Min                48.0764
exploration/Actions Mean                0.0687172
exploration/Actions Std                 0.639544
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                   7
exploration/Average Returns           160.437
evaluation/num steps total          59394
evaluation/num paths total            561
evaluation/path length Mean           109.667
evaluation/path length Std              2.90593
evaluation/path length Max            115
evaluation/path length Min            106
evaluation/Rewards Mean                 1.28567
evaluation/Rewards Std                  0.677126
evaluation/Rewards Max                  3.0579
evaluation/Rewards Min                  0.103135
evaluation/Returns Mean               140.995
evaluation/Returns Std                  5.9657
evaluation/Returns Max                153.256
evaluation/Returns Min                134.617
evaluation/Actions Mean                 0.154024
evaluation/Actions Std                  0.723989
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    9
evaluation/Average Returns            140.995
time/data storing (s)                   0.00359091
time/evaluation sampling (s)            0.469378
time/exploration sampling (s)           0.516996
time/logging (s)                        0.00597182
time/saving (s)                         0.0020468
time/training (s)                       7.49949
time/epoch (s)                          8.49747
time/total (s)                        527.885
Epoch                                  65
---------------------------------  ----------------
2021-07-02 23:45:16.175101 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 66 finished
---------------------------------  ----------------
replay_buffer/size                  77000
trainer/QF Loss                         0.00127388
trainer/Policy Loss                 -4246.37
trainer/Raw Policy Loss             -4246.37
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           4083.16
trainer/Q Predictions Std            1340.52
trainer/Q Predictions Max            5695.73
trainer/Q Predictions Min            -118.417
trainer/Q Targets Mean               4079.76
trainer/Q Targets Std                1369.73
trainer/Q Targets Max                5599.21
trainer/Q Targets Min                -132.655
trainer/Bellman Errors Mean         27235.5
trainer/Bellman Errors Std         156550
trainer/Bellman Errors Max              2.06154e+06
trainer/Bellman Errors Min              0.00110245
trainer/Policy Action Mean             -0.0315374
trainer/Policy Action Std               0.833433
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         77000
exploration/num paths total          1420
exploration/path length Mean          111.111
exploration/path length Std           115.875
exploration/path length Max           399
exploration/path length Min             6
exploration/Rewards Mean                0.809694
exploration/Rewards Std                 0.413351
exploration/Rewards Max                 2.2496
exploration/Rewards Min                -0.31705
exploration/Returns Mean               89.966
exploration/Returns Std                95.3877
exploration/Returns Max               326.71
exploration/Returns Min                 4.39174
exploration/Actions Mean                0.107204
exploration/Actions Std                 0.594974
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                   9
exploration/Average Returns            89.966
evaluation/num steps total          59661
evaluation/num paths total            562
evaluation/path length Mean           267
evaluation/path length Std              0
evaluation/path length Max            267
evaluation/path length Min            267
evaluation/Rewards Mean                 2.05357
evaluation/Rewards Std                  0.987171
evaluation/Rewards Max                  4.09982
evaluation/Rewards Min                  0.0940939
evaluation/Returns Mean               548.304
evaluation/Returns Std                  0
evaluation/Returns Max                548.304
evaluation/Returns Min                548.304
evaluation/Actions Mean                -0.0192744
evaluation/Actions Std                  0.751799
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    1
evaluation/Average Returns            548.304
time/data storing (s)                   0.00352109
time/evaluation sampling (s)            0.376069
time/exploration sampling (s)           0.400716
time/logging (s)                        0.00366831
time/saving (s)                         0.00154619
time/training (s)                       6.82718
time/epoch (s)                          7.6127
time/total (s)                        535.5
Epoch                                  66
---------------------------------  ----------------
2021-07-02 23:45:24.122536 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 67 finished
---------------------------------  ----------------
replay_buffer/size                  78000
trainer/QF Loss                         0.00174779
trainer/Policy Loss                 -4098.17
trainer/Raw Policy Loss             -4098.17
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           3937.64
trainer/Q Predictions Std            1451.9
trainer/Q Predictions Max            5662.51
trainer/Q Predictions Min            -217.268
trainer/Q Targets Mean               3959.89
trainer/Q Targets Std                1451.11
trainer/Q Targets Max                5919.66
trainer/Q Targets Min                 -45.8057
trainer/Bellman Errors Mean         31045.2
trainer/Bellman Errors Std         127470
trainer/Bellman Errors Max              1.22848e+06
trainer/Bellman Errors Min              0.0508615
trainer/Policy Action Mean              0.0687214
trainer/Policy Action Std               0.830148
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         78000
exploration/num paths total          1427
exploration/path length Mean          142.857
exploration/path length Std            94.0417
exploration/path length Max           347
exploration/path length Min            28
exploration/Rewards Mean                1.3974
exploration/Rewards Std                 0.75831
exploration/Rewards Max                 4.7674
exploration/Rewards Min                -0.347648
exploration/Returns Mean              199.628
exploration/Returns Std               171.893
exploration/Returns Max               544.527
exploration/Returns Min                29.9584
exploration/Actions Mean                0.100859
exploration/Actions Std                 0.658864
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                   7
exploration/Average Returns           199.628
evaluation/num steps total          60516
evaluation/num paths total            567
evaluation/path length Mean           171
evaluation/path length Std             60.0566
evaluation/path length Max            287
evaluation/path length Min            112
evaluation/Rewards Mean                 1.90889
evaluation/Rewards Std                  0.973299
evaluation/Rewards Max                  4.84486
evaluation/Rewards Min                  0.247282
evaluation/Returns Mean               326.419
evaluation/Returns Std                188.167
evaluation/Returns Max                694.4
evaluation/Returns Min                160.237
evaluation/Actions Mean                 0.098282
evaluation/Actions Std                  0.778509
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    5
evaluation/Average Returns            326.419
time/data storing (s)                   0.00353893
time/evaluation sampling (s)            0.37475
time/exploration sampling (s)           0.391862
time/logging (s)                        0.00377466
time/saving (s)                         0.00154955
time/training (s)                       7.16878
time/epoch (s)                          7.94426
time/total (s)                        543.447
Epoch                                  67
---------------------------------  ----------------
2021-07-02 23:45:32.029986 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 68 finished
---------------------------------  ----------------
replay_buffer/size                  79000
trainer/QF Loss                         0.000973415
trainer/Policy Loss                 -4333.57
trainer/Raw Policy Loss             -4333.57
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           4168.9
trainer/Q Predictions Std            1397.11
trainer/Q Predictions Max            6216.92
trainer/Q Predictions Min             -29.7993
trainer/Q Targets Mean               4158.35
trainer/Q Targets Std                1420.7
trainer/Q Targets Max                6143.88
trainer/Q Targets Min                 -40.8824
trainer/Bellman Errors Mean         26150.8
trainer/Bellman Errors Std         124696
trainer/Bellman Errors Max              1.78628e+06
trainer/Bellman Errors Min              0.00107026
trainer/Policy Action Mean              0.152035
trainer/Policy Action Std               0.81662
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         79000
exploration/num paths total          1433
exploration/path length Mean          166.667
exploration/path length Std           159.778
exploration/path length Max           466
exploration/path length Min            15
exploration/Rewards Mean                1.31172
exploration/Rewards Std                 0.932721
exploration/Rewards Max                 5.3097
exploration/Rewards Min                -0.278111
exploration/Returns Mean              218.62
exploration/Returns Std               235.303
exploration/Returns Max               695.265
exploration/Returns Min                15.8007
exploration/Actions Mean                0.0782683
exploration/Actions Std                 0.645054
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                   6
exploration/Average Returns           218.62
evaluation/num steps total          61375
evaluation/num paths total            572
evaluation/path length Mean           171.8
evaluation/path length Std             19.8535
evaluation/path length Max            208
evaluation/path length Min            147
evaluation/Rewards Mean                 1.95605
evaluation/Rewards Std                  1.11606
evaluation/Rewards Max                  6.02133
evaluation/Rewards Min                  0.218374
evaluation/Returns Mean               336.049
evaluation/Returns Std                 73.0334
evaluation/Returns Max                434.831
evaluation/Returns Min                242.972
evaluation/Actions Mean                 0.0940782
evaluation/Actions Std                  0.778732
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    5
evaluation/Average Returns            336.049
time/data storing (s)                   0.00342629
time/evaluation sampling (s)            0.410909
time/exploration sampling (s)           0.392161
time/logging (s)                        0.00558338
time/saving (s)                         0.00204976
time/training (s)                       7.09307
time/epoch (s)                          7.9072
time/total (s)                        551.355
Epoch                                  68
---------------------------------  ----------------
2021-07-02 23:45:39.850942 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 69 finished
---------------------------------  ---------------
replay_buffer/size                 80000
trainer/QF Loss                        0.00130082
trainer/Policy Loss                -4364.2
trainer/Raw Policy Loss            -4364.2
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean          4216.2
trainer/Q Predictions Std           1426.65
trainer/Q Predictions Max           6051.99
trainer/Q Predictions Min           -191.74
trainer/Q Targets Mean              4216.08
trainer/Q Targets Std               1415.32
trainer/Q Targets Max               6059.84
trainer/Q Targets Min                -45.1346
trainer/Bellman Errors Mean        16697.8
trainer/Bellman Errors Std         82647.8
trainer/Bellman Errors Max             1.22167e+06
trainer/Bellman Errors Min             0.181498
trainer/Policy Action Mean             0.165976
trainer/Policy Action Std              0.797345
trainer/Policy Action Max              1
trainer/Policy Action Min             -1
exploration/num steps total        80000
exploration/num paths total         1441
exploration/path length Mean         125
exploration/path length Std           64
exploration/path length Max          219
exploration/path length Min           22
exploration/Rewards Mean               1.36724
exploration/Rewards Std                0.585728
exploration/Rewards Max                3.13847
exploration/Rewards Min               -0.112426
exploration/Returns Mean             170.906
exploration/Returns Std               98.2637
exploration/Returns Max              315.427
exploration/Returns Min               12.6233
exploration/Actions Mean               0.106742
exploration/Actions Std                0.635969
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                  8
exploration/Average Returns          170.906
evaluation/num steps total         61916
evaluation/num paths total           574
evaluation/path length Mean          270.5
evaluation/path length Std            14.5
evaluation/path length Max           285
evaluation/path length Min           256
evaluation/Rewards Mean                1.84547
evaluation/Rewards Std                 0.732954
evaluation/Rewards Max                 4.59051
evaluation/Rewards Min                 0.116374
evaluation/Returns Mean              499.199
evaluation/Returns Std                82.4512
evaluation/Returns Max               581.65
evaluation/Returns Min               416.748
evaluation/Actions Mean                0.0822039
evaluation/Actions Std                 0.726152
evaluation/Actions Max                 1
evaluation/Actions Min                -1
evaluation/Num Paths                   2
evaluation/Average Returns           499.199
time/data storing (s)                  0.00339463
time/evaluation sampling (s)           0.484198
time/exploration sampling (s)          0.378941
time/logging (s)                       0.00309221
time/saving (s)                        0.00148123
time/training (s)                      6.94438
time/epoch (s)                         7.81549
time/total (s)                       559.173
Epoch                                 69
---------------------------------  ---------------
2021-07-02 23:45:48.068509 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 70 finished
---------------------------------  ----------------
replay_buffer/size                  81000
trainer/QF Loss                         0.000961197
trainer/Policy Loss                 -4700.11
trainer/Raw Policy Loss             -4700.11
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           4542.19
trainer/Q Predictions Std            1129.37
trainer/Q Predictions Max            5862.39
trainer/Q Predictions Min            -149.828
trainer/Q Targets Mean               4507.73
trainer/Q Targets Std                1161.46
trainer/Q Targets Max                5865.61
trainer/Q Targets Min                -121.958
trainer/Bellman Errors Mean         32566.9
trainer/Bellman Errors Std         221967
trainer/Bellman Errors Max              3.45052e+06
trainer/Bellman Errors Min              0.0254939
trainer/Policy Action Mean             -0.0482046
trainer/Policy Action Std               0.807099
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         81000
exploration/num paths total          1454
exploration/path length Mean           76.9231
exploration/path length Std            69.2403
exploration/path length Max           232
exploration/path length Min             9
exploration/Rewards Mean                0.985692
exploration/Rewards Std                 0.541929
exploration/Rewards Max                 2.53707
exploration/Rewards Min                -0.270946
exploration/Returns Mean               75.8225
exploration/Returns Std                73.7727
exploration/Returns Max               219.698
exploration/Returns Min                 7.44407
exploration/Actions Mean                0.119765
exploration/Actions Std                 0.657761
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  13
exploration/Average Returns            75.8225
evaluation/num steps total          62539
evaluation/num paths total            575
evaluation/path length Mean           623
evaluation/path length Std              0
evaluation/path length Max            623
evaluation/path length Min            623
evaluation/Rewards Mean                 1.13478
evaluation/Rewards Std                  0.576651
evaluation/Rewards Max                  2.71357
evaluation/Rewards Min                 -0.342768
evaluation/Returns Mean               706.965
evaluation/Returns Std                  0
evaluation/Returns Max                706.965
evaluation/Returns Min                706.965
evaluation/Actions Mean                 0.0769348
evaluation/Actions Std                  0.63181
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    1
evaluation/Average Returns            706.965
time/data storing (s)                   0.00350616
time/evaluation sampling (s)            0.370945
time/exploration sampling (s)           0.44986
time/logging (s)                        0.00344736
time/saving (s)                         0.00151766
time/training (s)                       7.38658
time/epoch (s)                          8.21586
time/total (s)                        567.39
Epoch                                  70
---------------------------------  ----------------
2021-07-02 23:45:55.871404 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 71 finished
---------------------------------  ---------------
replay_buffer/size                 82000
trainer/QF Loss                        0.0011875
trainer/Policy Loss                -4689.93
trainer/Raw Policy Loss            -4689.93
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean          4584.2
trainer/Q Predictions Std           1307.25
trainer/Q Predictions Max           6776.18
trainer/Q Predictions Min            -59.111
trainer/Q Targets Mean              4572.94
trainer/Q Targets Std               1355.03
trainer/Q Targets Max               6897.59
trainer/Q Targets Min                -25.4035
trainer/Bellman Errors Mean        26039
trainer/Bellman Errors Std         85425.7
trainer/Bellman Errors Max             1.05093e+06
trainer/Bellman Errors Min             0.00152588
trainer/Policy Action Mean             0.0299442
trainer/Policy Action Std              0.822424
trainer/Policy Action Max              1
trainer/Policy Action Min             -1
exploration/num steps total        82000
exploration/num paths total         1466
exploration/path length Mean          83.3333
exploration/path length Std           68.7911
exploration/path length Max          204
exploration/path length Min            8
exploration/Rewards Mean               1.32483
exploration/Rewards Std                0.643956
exploration/Rewards Max                3.3882
exploration/Rewards Min               -1.11372
exploration/Returns Mean             110.403
exploration/Returns Std              114.63
exploration/Returns Max              363.027
exploration/Returns Min                4.46467
exploration/Actions Mean               0.121767
exploration/Actions Std                0.663407
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                 12
exploration/Average Returns          110.403
evaluation/num steps total         63426
evaluation/num paths total           576
evaluation/path length Mean          887
evaluation/path length Std             0
evaluation/path length Max           887
evaluation/path length Min           887
evaluation/Rewards Mean                1.82874
evaluation/Rewards Std                 0.882208
evaluation/Rewards Max                 5.0753
evaluation/Rewards Min                 0.0474612
evaluation/Returns Mean             1622.09
evaluation/Returns Std                 0
evaluation/Returns Max              1622.09
evaluation/Returns Min              1622.09
evaluation/Actions Mean                0.149463
evaluation/Actions Std                 0.707573
evaluation/Actions Max                 1
evaluation/Actions Min                -1
evaluation/Num Paths                   1
evaluation/Average Returns          1622.09
time/data storing (s)                  0.00351294
time/evaluation sampling (s)           0.414192
time/exploration sampling (s)          0.409776
time/logging (s)                       0.00377393
time/saving (s)                        0.00153361
time/training (s)                      6.96809
time/epoch (s)                         7.80088
time/total (s)                       575.193
Epoch                                 71
---------------------------------  ---------------
2021-07-02 23:46:04.370412 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 72 finished
---------------------------------  ---------------
replay_buffer/size                 83000
trainer/QF Loss                        0.00106097
trainer/Policy Loss                -4666.33
trainer/Raw Policy Loss            -4666.33
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean          4531.32
trainer/Q Predictions Std           1423.42
trainer/Q Predictions Max           6081.6
trainer/Q Predictions Min            -20.3707
trainer/Q Targets Mean              4523.17
trainer/Q Targets Std               1434.56
trainer/Q Targets Max               6193.08
trainer/Q Targets Min               -148.374
trainer/Bellman Errors Mean        24693.2
trainer/Bellman Errors Std         97001.1
trainer/Bellman Errors Max             1.36107e+06
trainer/Bellman Errors Min             0.379716
trainer/Policy Action Mean             0.167607
trainer/Policy Action Std              0.807448
trainer/Policy Action Max              1
trainer/Policy Action Min             -1
exploration/num steps total        83000
exploration/num paths total         1477
exploration/path length Mean          90.9091
exploration/path length Std           49.3806
exploration/path length Max          189
exploration/path length Min           12
exploration/Rewards Mean               1.68548
exploration/Rewards Std                0.644512
exploration/Rewards Max                3.72245
exploration/Rewards Min                0.468226
exploration/Returns Mean             153.226
exploration/Returns Std               92.1277
exploration/Returns Max              302.487
exploration/Returns Min               10.3931
exploration/Actions Mean               0.140918
exploration/Actions Std                0.672003
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                 11
exploration/Average Returns          153.226
evaluation/num steps total         64365
evaluation/num paths total           580
evaluation/path length Mean          234.75
evaluation/path length Std            70.8498
evaluation/path length Max           344
evaluation/path length Min           148
evaluation/Rewards Mean                2.18084
evaluation/Rewards Std                 0.822919
evaluation/Rewards Max                 6.29028
evaluation/Rewards Min                 0.664478
evaluation/Returns Mean              511.953
evaluation/Returns Std               182.921
evaluation/Returns Max               729.803
evaluation/Returns Min               257.594
evaluation/Actions Mean                0.187513
evaluation/Actions Std                 0.773594
evaluation/Actions Max                 1
evaluation/Actions Min                -1
evaluation/Num Paths                   4
evaluation/Average Returns           511.953
time/data storing (s)                  0.00346289
time/evaluation sampling (s)           0.385442
time/exploration sampling (s)          0.403222
time/logging (s)                       0.00682589
time/saving (s)                        0.00236895
time/training (s)                      7.69864
time/epoch (s)                         8.49997
time/total (s)                       583.694
Epoch                                 72
---------------------------------  ---------------
2021-07-02 23:46:12.983277 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 73 finished
---------------------------------  ----------------
replay_buffer/size                  84000
trainer/QF Loss                         0.000882028
trainer/Policy Loss                 -4783.64
trainer/Raw Policy Loss             -4783.64
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           4647.73
trainer/Q Predictions Std            1512.63
trainer/Q Predictions Max            6238.16
trainer/Q Predictions Min            -101.231
trainer/Q Targets Mean               4639.63
trainer/Q Targets Std                1519.57
trainer/Q Targets Max                6221.31
trainer/Q Targets Min                 -44.8651
trainer/Bellman Errors Mean         50151.5
trainer/Bellman Errors Std         244554
trainer/Bellman Errors Max              2.44432e+06
trainer/Bellman Errors Min              0.22851
trainer/Policy Action Mean              0.131209
trainer/Policy Action Std               0.812083
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         84000
exploration/num paths total          1486
exploration/path length Mean          111.111
exploration/path length Std            63.8321
exploration/path length Max           216
exploration/path length Min            37
exploration/Rewards Mean                1.86881
exploration/Rewards Std                 0.88542
exploration/Rewards Max                 5.33347
exploration/Rewards Min                 0.0286058
exploration/Returns Mean              207.645
exploration/Returns Std               156.163
exploration/Returns Max               563.716
exploration/Returns Min                18.8415
exploration/Actions Mean                0.122676
exploration/Actions Std                 0.668479
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                   9
exploration/Average Returns           207.645
evaluation/num steps total          65280
evaluation/num paths total            586
evaluation/path length Mean           152.5
evaluation/path length Std              2.5
evaluation/path length Max            157
evaluation/path length Min            150
evaluation/Rewards Mean                 1.83528
evaluation/Rewards Std                  0.415003
evaluation/Rewards Max                  3.03605
evaluation/Rewards Min                  0.697355
evaluation/Returns Mean               279.881
evaluation/Returns Std                  2.52586
evaluation/Returns Max                284.455
evaluation/Returns Min                277.031
evaluation/Actions Mean                 0.211647
evaluation/Actions Std                  0.77421
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    6
evaluation/Average Returns            279.881
time/data storing (s)                   0.00350974
time/evaluation sampling (s)            0.540108
time/exploration sampling (s)           0.51504
time/logging (s)                        0.00419
time/saving (s)                         0.00180043
time/training (s)                       7.54193
time/epoch (s)                          8.60658
time/total (s)                        592.303
Epoch                                  73
---------------------------------  ----------------
2021-07-02 23:46:20.618944 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 74 finished
---------------------------------  ----------------
replay_buffer/size                  85000
trainer/QF Loss                         0.000889983
trainer/Policy Loss                 -4773.11
trainer/Raw Policy Loss             -4773.11
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           4602.43
trainer/Q Predictions Std            1523.91
trainer/Q Predictions Max            6387.32
trainer/Q Predictions Min             -12.6944
trainer/Q Targets Mean               4588.57
trainer/Q Targets Std                1567.08
trainer/Q Targets Max                6357.27
trainer/Q Targets Min                 -91.2142
trainer/Bellman Errors Mean         39917.9
trainer/Bellman Errors Std         260691
trainer/Bellman Errors Max              3.75349e+06
trainer/Bellman Errors Min              0.488907
trainer/Policy Action Mean              0.0780237
trainer/Policy Action Std               0.828998
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         85000
exploration/num paths total          1494
exploration/path length Mean          125
exploration/path length Std           150.321
exploration/path length Max           514
exploration/path length Min            29
exploration/Rewards Mean                1.14935
exploration/Rewards Std                 0.544138
exploration/Rewards Max                 2.66279
exploration/Rewards Min                -0.627277
exploration/Returns Mean              143.669
exploration/Returns Std               183.513
exploration/Returns Max               605.535
exploration/Returns Min                16.9526
exploration/Actions Mean                0.20835
exploration/Actions Std                 0.644972
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                   8
exploration/Average Returns           143.669
evaluation/num steps total          66280
evaluation/num paths total            587
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 0.840953
evaluation/Rewards Std                  0.439016
evaluation/Rewards Max                  2.0065
evaluation/Rewards Min                 -0.305102
evaluation/Returns Mean               840.953
evaluation/Returns Std                  0
evaluation/Returns Max                840.953
evaluation/Returns Min                840.953
evaluation/Actions Mean                 0.165992
evaluation/Actions Std                  0.59565
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    1
evaluation/Average Returns            840.953
time/data storing (s)                   0.00356582
time/evaluation sampling (s)            0.402451
time/exploration sampling (s)           0.41049
time/logging (s)                        0.0040455
time/saving (s)                         0.00165209
time/training (s)                       6.81106
time/epoch (s)                          7.63327
time/total (s)                        599.938
Epoch                                  74
---------------------------------  ----------------
2021-07-02 23:46:29.268123 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 75 finished
---------------------------------  ----------------
replay_buffer/size                  86000
trainer/QF Loss                         0.00133877
trainer/Policy Loss                 -4863.36
trainer/Raw Policy Loss             -4863.36
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           4708.47
trainer/Q Predictions Std            1677.55
trainer/Q Predictions Max            6637.81
trainer/Q Predictions Min            -171.379
trainer/Q Targets Mean               4705.09
trainer/Q Targets Std                1724.8
trainer/Q Targets Max                6577.04
trainer/Q Targets Min                -234.543
trainer/Bellman Errors Mean         75066.4
trainer/Bellman Errors Std         619628
trainer/Bellman Errors Max              9.62147e+06
trainer/Bellman Errors Min              0.071337
trainer/Policy Action Mean              0.111009
trainer/Policy Action Std               0.797653
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         86000
exploration/num paths total          1501
exploration/path length Mean          142.857
exploration/path length Std            71.7205
exploration/path length Max           231
exploration/path length Min            38
exploration/Rewards Mean                1.50142
exploration/Rewards Std                 0.704271
exploration/Rewards Max                 3.45264
exploration/Rewards Min                -0.26278
exploration/Returns Mean              214.489
exploration/Returns Std               126.617
exploration/Returns Max               377.155
exploration/Returns Min                29.549
exploration/Actions Mean                0.14079
exploration/Actions Std                 0.664779
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                   7
exploration/Average Returns           214.489
evaluation/num steps total          67280
evaluation/num paths total            588
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 1.09683
evaluation/Rewards Std                  0.619683
evaluation/Rewards Max                  2.75527
evaluation/Rewards Min                 -0.270502
evaluation/Returns Mean              1096.83
evaluation/Returns Std                  0
evaluation/Returns Max               1096.83
evaluation/Returns Min               1096.83
evaluation/Actions Mean                 0.16803
evaluation/Actions Std                  0.597318
evaluation/Actions Max                  1
evaluation/Actions Min                 -0.999998
evaluation/Num Paths                    1
evaluation/Average Returns           1096.83
time/data storing (s)                   0.00357615
time/evaluation sampling (s)            0.38987
time/exploration sampling (s)           0.404488
time/logging (s)                        0.00600988
time/saving (s)                         0.0021159
time/training (s)                       7.84293
time/epoch (s)                          8.64899
time/total (s)                        608.589
Epoch                                  75
---------------------------------  ----------------
2021-07-02 23:46:37.759130 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 76 finished
---------------------------------  ----------------
replay_buffer/size                  87000
trainer/QF Loss                         0.00137347
trainer/Policy Loss                 -4926.39
trainer/Raw Policy Loss             -4926.39
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           4743.92
trainer/Q Predictions Std            1678.25
trainer/Q Predictions Max            6836.21
trainer/Q Predictions Min            -385.438
trainer/Q Targets Mean               4732.62
trainer/Q Targets Std                1695.35
trainer/Q Targets Max                6953.7
trainer/Q Targets Min                 -31.8137
trainer/Bellman Errors Mean         82582.7
trainer/Bellman Errors Std         886622
trainer/Bellman Errors Max              1.41849e+07
trainer/Bellman Errors Min              0.999024
trainer/Policy Action Mean              0.059223
trainer/Policy Action Std               0.81138
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         87000
exploration/num paths total          1510
exploration/path length Mean          111.111
exploration/path length Std            82.3899
exploration/path length Max           289
exploration/path length Min            12
exploration/Rewards Mean                1.42292
exploration/Rewards Std                 0.586937
exploration/Rewards Max                 3.15418
exploration/Rewards Min                -0.580813
exploration/Returns Mean              158.102
exploration/Returns Std               135.509
exploration/Returns Max               408.501
exploration/Returns Min                 5.24265
exploration/Actions Mean                0.0956756
exploration/Actions Std                 0.650207
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                   9
exploration/Average Returns           158.102
evaluation/num steps total          68280
evaluation/num paths total            589
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 1.07332
evaluation/Rewards Std                  0.47971
evaluation/Rewards Max                  2.1413
evaluation/Rewards Min                  0.00776638
evaluation/Returns Mean              1073.32
evaluation/Returns Std                  0
evaluation/Returns Max               1073.32
evaluation/Returns Min               1073.32
evaluation/Actions Mean                 0.0707542
evaluation/Actions Std                  0.541709
evaluation/Actions Max                  1
evaluation/Actions Min                 -0.999999
evaluation/Num Paths                    1
evaluation/Average Returns           1073.32
time/data storing (s)                   0.00358482
time/evaluation sampling (s)            0.433924
time/exploration sampling (s)           0.437667
time/logging (s)                        0.00414418
time/saving (s)                         0.00167315
time/training (s)                       7.60459
time/epoch (s)                          8.48558
time/total (s)                        617.077
Epoch                                  76
---------------------------------  ----------------
2021-07-02 23:46:45.938136 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 77 finished
---------------------------------  ----------------
replay_buffer/size                  88000
trainer/QF Loss                         0.000921839
trainer/Policy Loss                 -5200.43
trainer/Raw Policy Loss             -5200.43
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           5016.46
trainer/Q Predictions Std            1444
trainer/Q Predictions Max            6878.26
trainer/Q Predictions Min             -76.0327
trainer/Q Targets Mean               5003.87
trainer/Q Targets Std                1488.18
trainer/Q Targets Max                6866.34
trainer/Q Targets Min                 -98.7474
trainer/Bellman Errors Mean         54677
trainer/Bellman Errors Std         447698
trainer/Bellman Errors Max              6.1271e+06
trainer/Bellman Errors Min              0.00299072
trainer/Policy Action Mean              0.0946252
trainer/Policy Action Std               0.801991
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         88000
exploration/num paths total          1526
exploration/path length Mean           62.5
exploration/path length Std            53.7238
exploration/path length Max           212
exploration/path length Min            10
exploration/Rewards Mean                1.4752
exploration/Rewards Std                 0.542491
exploration/Rewards Max                 3.024
exploration/Rewards Min                 0.286703
exploration/Returns Mean               92.2002
exploration/Returns Std                98.0687
exploration/Returns Max               402.36
exploration/Returns Min                 7.47385
exploration/Actions Mean                0.116407
exploration/Actions Std                 0.662737
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  16
exploration/Average Returns            92.2002
evaluation/num steps total          69057
evaluation/num paths total            591
evaluation/path length Mean           388.5
evaluation/path length Std            204.5
evaluation/path length Max            593
evaluation/path length Min            184
evaluation/Rewards Mean                 1.60389
evaluation/Rewards Std                  0.392355
evaluation/Rewards Max                  2.60731
evaluation/Rewards Min                  0.42657
evaluation/Returns Mean               623.113
evaluation/Returns Std                351.158
evaluation/Returns Max                974.271
evaluation/Returns Min                271.955
evaluation/Actions Mean                 0.121491
evaluation/Actions Std                  0.706868
evaluation/Actions Max                  1
evaluation/Actions Min                 -0.999983
evaluation/Num Paths                    2
evaluation/Average Returns            623.113
time/data storing (s)                   0.00419295
time/evaluation sampling (s)            0.415455
time/exploration sampling (s)           0.422584
time/logging (s)                        0.00382035
time/saving (s)                         0.00157193
time/training (s)                       7.32779
time/epoch (s)                          8.17541
time/total (s)                        625.255
Epoch                                  77
---------------------------------  ----------------
2021-07-02 23:46:54.580025 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 78 finished
---------------------------------  ----------------
replay_buffer/size                  89000
trainer/QF Loss                         0.000968893
trainer/Policy Loss                 -5325.92
trainer/Raw Policy Loss             -5325.92
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           5163.83
trainer/Q Predictions Std            1472.77
trainer/Q Predictions Max            6999.38
trainer/Q Predictions Min             -91.35
trainer/Q Targets Mean               5165.58
trainer/Q Targets Std                1468.57
trainer/Q Targets Max                6870.94
trainer/Q Targets Min                  -0.361881
trainer/Bellman Errors Mean         23506.9
trainer/Bellman Errors Std          79563.4
trainer/Bellman Errors Max         660463
trainer/Bellman Errors Min              0.127402
trainer/Policy Action Mean              0.146439
trainer/Policy Action Std               0.797935
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         89000
exploration/num paths total          1535
exploration/path length Mean          111.111
exploration/path length Std           102.628
exploration/path length Max           342
exploration/path length Min            13
exploration/Rewards Mean                1.27574
exploration/Rewards Std                 0.593329
exploration/Rewards Max                 3.01142
exploration/Rewards Min                -0.262747
exploration/Returns Mean              141.749
exploration/Returns Std               151.045
exploration/Returns Max               507.478
exploration/Returns Min                13.8001
exploration/Actions Mean                0.18342
exploration/Actions Std                 0.65115
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                   9
exploration/Average Returns           141.749
evaluation/num steps total          70057
evaluation/num paths total            592
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 1.56669
evaluation/Rewards Std                  0.574353
evaluation/Rewards Max                  3.08624
evaluation/Rewards Min                 -0.118124
evaluation/Returns Mean              1566.69
evaluation/Returns Std                  0
evaluation/Returns Max               1566.69
evaluation/Returns Min               1566.69
evaluation/Actions Mean                 0.147539
evaluation/Actions Std                  0.678556
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    1
evaluation/Average Returns           1566.69
time/data storing (s)                   0.00361944
time/evaluation sampling (s)            0.45483
time/exploration sampling (s)           0.408615
time/logging (s)                        0.00604146
time/saving (s)                         0.00205082
time/training (s)                       7.7664
time/epoch (s)                          8.64156
time/total (s)                        633.898
Epoch                                  78
---------------------------------  ----------------
2021-07-02 23:47:02.638092 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 79 finished
---------------------------------  ----------------
replay_buffer/size                  90000
trainer/QF Loss                         0.000963648
trainer/Policy Loss                 -5288.76
trainer/Raw Policy Loss             -5288.76
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           5035.99
trainer/Q Predictions Std            1590.78
trainer/Q Predictions Max            7154.03
trainer/Q Predictions Min             -85.8369
trainer/Q Targets Mean               5042.71
trainer/Q Targets Std                1606.9
trainer/Q Targets Max                7123.94
trainer/Q Targets Min                -107.047
trainer/Bellman Errors Mean         46080.9
trainer/Bellman Errors Std         178855
trainer/Bellman Errors Max              2.02846e+06
trainer/Bellman Errors Min              0.575767
trainer/Policy Action Mean              0.182997
trainer/Policy Action Std               0.787782
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         90000
exploration/num paths total          1544
exploration/path length Mean          111.111
exploration/path length Std            60.0008
exploration/path length Max           188
exploration/path length Min            25
exploration/Rewards Mean                1.51429
exploration/Rewards Std                 0.673071
exploration/Rewards Max                 3.42902
exploration/Rewards Min                 0.0492491
exploration/Returns Mean              168.255
exploration/Returns Std               119.584
exploration/Returns Max               379.184
exploration/Returns Min                20.1959
exploration/Actions Mean                0.179366
exploration/Actions Std                 0.665349
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                   9
exploration/Average Returns           168.255
evaluation/num steps total          71057
evaluation/num paths total            593
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 1.83799
evaluation/Rewards Std                  0.40783
evaluation/Rewards Max                  2.89776
evaluation/Rewards Min                  0.702399
evaluation/Returns Mean              1837.99
evaluation/Returns Std                  0
evaluation/Returns Max               1837.99
evaluation/Returns Min               1837.99
evaluation/Actions Mean                 0.144962
evaluation/Actions Std                  0.706751
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    1
evaluation/Average Returns           1837.99
time/data storing (s)                   0.00594949
time/evaluation sampling (s)            0.380807
time/exploration sampling (s)           0.424556
time/logging (s)                        0.0041108
time/saving (s)                         0.00163926
time/training (s)                       7.23588
time/epoch (s)                          8.05294
time/total (s)                        641.953
Epoch                                  79
---------------------------------  ----------------
2021-07-02 23:47:10.853591 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 80 finished
---------------------------------  ----------------
replay_buffer/size                  91000
trainer/QF Loss                         0.00103522
trainer/Policy Loss                 -5151.55
trainer/Raw Policy Loss             -5151.55
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           4975.79
trainer/Q Predictions Std            1699.98
trainer/Q Predictions Max            7099.67
trainer/Q Predictions Min             -99.2179
trainer/Q Targets Mean               4971.22
trainer/Q Targets Std                1705.59
trainer/Q Targets Max                6951.33
trainer/Q Targets Min                 -89.9306
trainer/Bellman Errors Mean         41849.9
trainer/Bellman Errors Std         176539
trainer/Bellman Errors Max              2.24099e+06
trainer/Bellman Errors Min              0.16663
trainer/Policy Action Mean              0.103779
trainer/Policy Action Std               0.817753
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         91000
exploration/num paths total          1557
exploration/path length Mean           76.9231
exploration/path length Std            79.8821
exploration/path length Max           241
exploration/path length Min             3
exploration/Rewards Mean                1.5328
exploration/Rewards Std                 0.791505
exploration/Rewards Max                 3.89599
exploration/Rewards Min                -0.0859451
exploration/Returns Mean              117.907
exploration/Returns Std               147.659
exploration/Returns Max               436.944
exploration/Returns Min                 2.54583
exploration/Actions Mean                0.136535
exploration/Actions Std                 0.687874
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  13
exploration/Average Returns           117.907
evaluation/num steps total          71930
evaluation/num paths total            596
evaluation/path length Mean           291
evaluation/path length Std             90.0259
evaluation/path length Max            415
evaluation/path length Min            204
evaluation/Rewards Mean                 2.13935
evaluation/Rewards Std                  0.907393
evaluation/Rewards Max                  5.22057
evaluation/Rewards Min                  0.304188
evaluation/Returns Mean               622.55
evaluation/Returns Std                228.662
evaluation/Returns Max                905.237
evaluation/Returns Min                345.208
evaluation/Actions Mean                 0.10195
evaluation/Actions Std                  0.770518
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    3
evaluation/Average Returns            622.55
time/data storing (s)                   0.00358767
time/evaluation sampling (s)            0.375033
time/exploration sampling (s)           0.404398
time/logging (s)                        0.00418578
time/saving (s)                         0.00164256
time/training (s)                       7.42379
time/epoch (s)                          8.21264
time/total (s)                        650.168
Epoch                                  80
---------------------------------  ----------------
2021-07-02 23:47:19.559119 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 81 finished
---------------------------------  ----------------
replay_buffer/size                  92000
trainer/QF Loss                         0.00124966
trainer/Policy Loss                 -5615.72
trainer/Raw Policy Loss             -5615.72
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           5444.52
trainer/Q Predictions Std            1556.84
trainer/Q Predictions Max            7368.42
trainer/Q Predictions Min            -140.056
trainer/Q Targets Mean               5470.9
trainer/Q Targets Std                1582.32
trainer/Q Targets Max                7572.92
trainer/Q Targets Min                 -26.9246
trainer/Bellman Errors Mean         30675.6
trainer/Bellman Errors Std          88686.9
trainer/Bellman Errors Max         783492
trainer/Bellman Errors Min              2.14577e-06
trainer/Policy Action Mean              0.0999509
trainer/Policy Action Std               0.793317
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         92000
exploration/num paths total          1567
exploration/path length Mean          100
exploration/path length Std            89.7742
exploration/path length Max           236
exploration/path length Min            10
exploration/Rewards Mean                1.6877
exploration/Rewards Std                 0.99522
exploration/Rewards Max                 4.83304
exploration/Rewards Min                -0.0439542
exploration/Returns Mean              168.77
exploration/Returns Std               201.049
exploration/Returns Max               562.502
exploration/Returns Min                 6.54304
exploration/Actions Mean                0.124001
exploration/Actions Std                 0.690758
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  10
exploration/Average Returns           168.77
evaluation/num steps total          72298
evaluation/num paths total            597
evaluation/path length Mean           368
evaluation/path length Std              0
evaluation/path length Max            368
evaluation/path length Min            368
evaluation/Rewards Mean                 2.33192
evaluation/Rewards Std                  1.08897
evaluation/Rewards Max                  4.92588
evaluation/Rewards Min                  0.23551
evaluation/Returns Mean               858.147
evaluation/Returns Std                  0
evaluation/Returns Max                858.147
evaluation/Returns Min                858.147
evaluation/Actions Mean                 0.0498491
evaluation/Actions Std                  0.763574
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    1
evaluation/Average Returns            858.147
time/data storing (s)                   0.00361061
time/evaluation sampling (s)            0.423305
time/exploration sampling (s)           0.410914
time/logging (s)                        0.00320447
time/saving (s)                         0.00168764
time/training (s)                       7.85907
time/epoch (s)                          8.70179
time/total (s)                        658.872
Epoch                                  81
---------------------------------  ----------------
2021-07-02 23:47:27.916752 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 82 finished
---------------------------------  ---------------
replay_buffer/size                  93000
trainer/QF Loss                         0.00122402
trainer/Policy Loss                 -5330.47
trainer/Raw Policy Loss             -5330.47
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           5200.99
trainer/Q Predictions Std            1762.09
trainer/Q Predictions Max            7191.94
trainer/Q Predictions Min             -32.8424
trainer/Q Targets Mean               5220.18
trainer/Q Targets Std                1792.36
trainer/Q Targets Max                7223.99
trainer/Q Targets Min                -112.987
trainer/Bellman Errors Mean         50585.8
trainer/Bellman Errors Std         337004
trainer/Bellman Errors Max              4.6786e+06
trainer/Bellman Errors Min              0.423643
trainer/Policy Action Mean              0.1052
trainer/Policy Action Std               0.821874
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         93000
exploration/num paths total          1573
exploration/path length Mean          166.667
exploration/path length Std           112.318
exploration/path length Max           394
exploration/path length Min            60
exploration/Rewards Mean                1.10349
exploration/Rewards Std                 0.555944
exploration/Rewards Max                 2.89095
exploration/Rewards Min                 0.0603664
exploration/Returns Mean              183.915
exploration/Returns Std               169.873
exploration/Returns Max               544.303
exploration/Returns Min                54.9764
exploration/Actions Mean                0.195263
exploration/Actions Std                 0.658236
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                   6
exploration/Average Returns           183.915
evaluation/num steps total          73298
evaluation/num paths total            598
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 0.990669
evaluation/Rewards Std                  0.0612403
evaluation/Rewards Max                  1.64328
evaluation/Rewards Min                  0.493293
evaluation/Returns Mean               990.669
evaluation/Returns Std                  0
evaluation/Returns Max                990.669
evaluation/Returns Min                990.669
evaluation/Actions Mean                 0.252628
evaluation/Actions Std                  0.386897
evaluation/Actions Max                  0.999764
evaluation/Actions Min                 -0.999994
evaluation/Num Paths                    1
evaluation/Average Returns            990.669
time/data storing (s)                   0.00348866
time/evaluation sampling (s)            0.402111
time/exploration sampling (s)           0.405499
time/logging (s)                        0.00597006
time/saving (s)                         0.00206151
time/training (s)                       7.53909
time/epoch (s)                          8.35822
time/total (s)                        667.232
Epoch                                  82
---------------------------------  ---------------
2021-07-02 23:47:36.535932 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 83 finished
---------------------------------  ----------------
replay_buffer/size                  94000
trainer/QF Loss                         0.000993921
trainer/Policy Loss                 -5528.3
trainer/Raw Policy Loss             -5528.3
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           5378.34
trainer/Q Predictions Std            1722.93
trainer/Q Predictions Max            7297.95
trainer/Q Predictions Min             -65.2869
trainer/Q Targets Mean               5359.61
trainer/Q Targets Std                1750.08
trainer/Q Targets Max                7363.14
trainer/Q Targets Min                -127.884
trainer/Bellman Errors Mean         51162
trainer/Bellman Errors Std         279588
trainer/Bellman Errors Max              4.04131e+06
trainer/Bellman Errors Min              0.0572443
trainer/Policy Action Mean              0.133762
trainer/Policy Action Std               0.803689
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         94000
exploration/num paths total          1583
exploration/path length Mean          100
exploration/path length Std            74.6083
exploration/path length Max           253
exploration/path length Min             7
exploration/Rewards Mean                1.53883
exploration/Rewards Std                 0.795024
exploration/Rewards Max                 3.86075
exploration/Rewards Min                -0.017832
exploration/Returns Mean              153.883
exploration/Returns Std               136.964
exploration/Returns Max               448.889
exploration/Returns Min                 4.47785
exploration/Actions Mean                0.165921
exploration/Actions Std                 0.680048
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  10
exploration/Average Returns           153.883
evaluation/num steps total          74298
evaluation/num paths total            599
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 2.18729
evaluation/Rewards Std                  0.624832
evaluation/Rewards Max                  3.70367
evaluation/Rewards Min                  0.414598
evaluation/Returns Mean              2187.29
evaluation/Returns Std                  0
evaluation/Returns Max               2187.29
evaluation/Returns Min               2187.29
evaluation/Actions Mean                 0.142985
evaluation/Actions Std                  0.734866
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    1
evaluation/Average Returns           2187.29
time/data storing (s)                   0.00530731
time/evaluation sampling (s)            0.515769
time/exploration sampling (s)           0.536889
time/logging (s)                        0.00384872
time/saving (s)                         0.00146236
time/training (s)                       7.5504
time/epoch (s)                          8.61368
time/total (s)                        675.848
Epoch                                  83
---------------------------------  ----------------
2021-07-02 23:47:44.742087 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 84 finished
---------------------------------  ----------------
replay_buffer/size                  95000
trainer/QF Loss                         0.000991776
trainer/Policy Loss                 -5767.59
trainer/Raw Policy Loss             -5767.59
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           5567.85
trainer/Q Predictions Std            1584.77
trainer/Q Predictions Max            7405.45
trainer/Q Predictions Min             -31.3822
trainer/Q Targets Mean               5592.35
trainer/Q Targets Std                1605.99
trainer/Q Targets Max                7385.34
trainer/Q Targets Min                -164.168
trainer/Bellman Errors Mean         40711
trainer/Bellman Errors Std         196725
trainer/Bellman Errors Max              1.90755e+06
trainer/Bellman Errors Min              0.125665
trainer/Policy Action Mean              0.106368
trainer/Policy Action Std               0.802767
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         95000
exploration/num paths total          1596
exploration/path length Mean           76.9231
exploration/path length Std            83.4132
exploration/path length Max           292
exploration/path length Min             9
exploration/Rewards Mean                0.97133
exploration/Rewards Std                 0.891717
exploration/Rewards Max                 3.66503
exploration/Rewards Min                -0.767288
exploration/Returns Mean               74.7177
exploration/Returns Std               110.443
exploration/Returns Max               396.608
exploration/Returns Min                 2.47499
exploration/Actions Mean                0.228434
exploration/Actions Std                 0.661696
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  13
exploration/Average Returns            74.7177
evaluation/num steps total          74971
evaluation/num paths total            601
evaluation/path length Mean           336.5
evaluation/path length Std            111.5
evaluation/path length Max            448
evaluation/path length Min            225
evaluation/Rewards Mean                 1.19413
evaluation/Rewards Std                  0.605119
evaluation/Rewards Max                  2.62222
evaluation/Rewards Min                 -0.0170003
evaluation/Returns Mean               401.823
evaluation/Returns Std                235.682
evaluation/Returns Max                637.505
evaluation/Returns Min                166.142
evaluation/Actions Mean                 0.185872
evaluation/Actions Std                  0.68704
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    2
evaluation/Average Returns            401.823
time/data storing (s)                   0.00628214
time/evaluation sampling (s)            0.399985
time/exploration sampling (s)           0.42492
time/logging (s)                        0.00483085
time/saving (s)                         0.00156844
time/training (s)                       7.36727
time/epoch (s)                          8.20485
time/total (s)                        684.055
Epoch                                  84
---------------------------------  ----------------
2021-07-02 23:47:53.083092 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 85 finished
---------------------------------  ----------------
replay_buffer/size                  96000
trainer/QF Loss                         0.00109963
trainer/Policy Loss                 -5668.78
trainer/Raw Policy Loss             -5668.78
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           5520.71
trainer/Q Predictions Std            1702.89
trainer/Q Predictions Max            7537.9
trainer/Q Predictions Min            -272.565
trainer/Q Targets Mean               5517.94
trainer/Q Targets Std                1747.92
trainer/Q Targets Max                7637.4
trainer/Q Targets Min                -303.226
trainer/Bellman Errors Mean         67589.2
trainer/Bellman Errors Std         556673
trainer/Bellman Errors Max              8.11279e+06
trainer/Bellman Errors Min              0.685791
trainer/Policy Action Mean              0.121977
trainer/Policy Action Std               0.809115
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         96000
exploration/num paths total          1604
exploration/path length Mean          125
exploration/path length Std           104.456
exploration/path length Max           362
exploration/path length Min            11
exploration/Rewards Mean                1.55161
exploration/Rewards Std                 0.853362
exploration/Rewards Max                 3.70407
exploration/Rewards Min                -0.0485365
exploration/Returns Mean              193.951
exploration/Returns Std               257.056
exploration/Returns Max               845.297
exploration/Returns Min                 6.39072
exploration/Actions Mean                0.137741
exploration/Actions Std                 0.67093
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                   8
exploration/Average Returns           193.951
evaluation/num steps total          75971
evaluation/num paths total            602
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 2.0257
evaluation/Rewards Std                  0.718043
evaluation/Rewards Max                  3.40611
evaluation/Rewards Min                  0.131745
evaluation/Returns Mean              2025.7
evaluation/Returns Std                  0
evaluation/Returns Max               2025.7
evaluation/Returns Min               2025.7
evaluation/Actions Mean                 0.125818
evaluation/Actions Std                  0.737242
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    1
evaluation/Average Returns           2025.7
time/data storing (s)                   0.00347368
time/evaluation sampling (s)            0.397541
time/exploration sampling (s)           0.414159
time/logging (s)                        0.00687606
time/saving (s)                         0.0023631
time/training (s)                       7.51558
time/epoch (s)                          8.34
time/total (s)                        692.396
Epoch                                  85
---------------------------------  ----------------
2021-07-02 23:48:01.493162 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 86 finished
---------------------------------  ----------------
replay_buffer/size                  97000
trainer/QF Loss                         0.00093773
trainer/Policy Loss                 -5839.52
trainer/Raw Policy Loss             -5839.52
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           5655.81
trainer/Q Predictions Std            1651.59
trainer/Q Predictions Max            7737.36
trainer/Q Predictions Min            -181.695
trainer/Q Targets Mean               5644.48
trainer/Q Targets Std                1675.4
trainer/Q Targets Max                7644.64
trainer/Q Targets Min                  -1.1582
trainer/Bellman Errors Mean         43006.8
trainer/Bellman Errors Std         155784
trainer/Bellman Errors Max              2.07192e+06
trainer/Bellman Errors Min              0.0398829
trainer/Policy Action Mean              0.0864251
trainer/Policy Action Std               0.803848
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         97000
exploration/num paths total          1611
exploration/path length Mean          142.857
exploration/path length Std           124.802
exploration/path length Max           336
exploration/path length Min            17
exploration/Rewards Mean                1.32617
exploration/Rewards Std                 0.838973
exploration/Rewards Max                 4.22421
exploration/Rewards Min                -0.262872
exploration/Returns Mean              189.453
exploration/Returns Std               191.429
exploration/Returns Max               485.703
exploration/Returns Min                11.4497
exploration/Actions Mean                0.135132
exploration/Actions Std                 0.647833
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                   7
exploration/Average Returns           189.453
evaluation/num steps total          76622
evaluation/num paths total            603
evaluation/path length Mean           651
evaluation/path length Std              0
evaluation/path length Max            651
evaluation/path length Min            651
evaluation/Rewards Mean                 2.07437
evaluation/Rewards Std                  1.38843
evaluation/Rewards Max                  4.61178
evaluation/Rewards Min                  0.185392
evaluation/Returns Mean              1350.42
evaluation/Returns Std                  0
evaluation/Returns Max               1350.42
evaluation/Returns Min               1350.42
evaluation/Actions Mean                 0.122912
evaluation/Actions Std                  0.747098
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    1
evaluation/Average Returns           1350.42
time/data storing (s)                   0.00634204
time/evaluation sampling (s)            0.514611
time/exploration sampling (s)           0.56815
time/logging (s)                        0.00515639
time/saving (s)                         0.00209329
time/training (s)                       7.30819
time/epoch (s)                          8.40455
time/total (s)                        700.804
Epoch                                  86
---------------------------------  ----------------
2021-07-02 23:48:09.965593 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 87 finished
---------------------------------  ----------------
replay_buffer/size                  98000
trainer/QF Loss                         0.000851653
trainer/Policy Loss                 -5732.64
trainer/Raw Policy Loss             -5732.64
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           5619.08
trainer/Q Predictions Std            1785.56
trainer/Q Predictions Max            7932.22
trainer/Q Predictions Min             -69.4712
trainer/Q Targets Mean               5630.82
trainer/Q Targets Std                1800.09
trainer/Q Targets Max                7885.04
trainer/Q Targets Min                 -47.0107
trainer/Bellman Errors Mean         27082.4
trainer/Bellman Errors Std          88179.9
trainer/Bellman Errors Max         831515
trainer/Bellman Errors Min              0.0766489
trainer/Policy Action Mean              0.180613
trainer/Policy Action Std               0.790412
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         98000
exploration/num paths total          1622
exploration/path length Mean           90.9091
exploration/path length Std            60.6352
exploration/path length Max           220
exploration/path length Min            13
exploration/Rewards Mean                2.03225
exploration/Rewards Std                 1.11145
exploration/Rewards Max                 5.19942
exploration/Rewards Min                -0.0374932
exploration/Returns Mean              184.75
exploration/Returns Std               182.82
exploration/Returns Max               619.47
exploration/Returns Min                 8.71763
exploration/Actions Mean                0.190947
exploration/Actions Std                 0.669997
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  11
exploration/Average Returns           184.75
evaluation/num steps total          77605
evaluation/num paths total            606
evaluation/path length Mean           327.667
evaluation/path length Std             11.8415
evaluation/path length Max            342
evaluation/path length Min            313
evaluation/Rewards Mean                 2.857
evaluation/Rewards Std                  1.09129
evaluation/Rewards Max                  5.38706
evaluation/Rewards Min                  0.589825
evaluation/Returns Mean               936.143
evaluation/Returns Std                 52.8163
evaluation/Returns Max                974.932
evaluation/Returns Min                861.468
evaluation/Actions Mean                 0.16979
evaluation/Actions Std                  0.767363
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    3
evaluation/Average Returns            936.143
time/data storing (s)                   0.00623857
time/evaluation sampling (s)            0.409338
time/exploration sampling (s)           0.526478
time/logging (s)                        0.00593692
time/saving (s)                         0.00207023
time/training (s)                       7.51996
time/epoch (s)                          8.47002
time/total (s)                        709.276
Epoch                                  87
---------------------------------  ----------------
2021-07-02 23:48:18.549715 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 88 finished
---------------------------------  ----------------
replay_buffer/size                  99000
trainer/QF Loss                         0.00108552
trainer/Policy Loss                 -5866.39
trainer/Raw Policy Loss             -5866.39
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           5732
trainer/Q Predictions Std            1701.09
trainer/Q Predictions Max            7924.25
trainer/Q Predictions Min             -54.4627
trainer/Q Targets Mean               5740.9
trainer/Q Targets Std                1762.7
trainer/Q Targets Max                7896.98
trainer/Q Targets Min                -175.294
trainer/Bellman Errors Mean         67866.5
trainer/Bellman Errors Std         559671
trainer/Bellman Errors Max              8.81831e+06
trainer/Bellman Errors Min              0.157975
trainer/Policy Action Mean              0.195852
trainer/Policy Action Std               0.790114
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         99000
exploration/num paths total          1633
exploration/path length Mean           90.9091
exploration/path length Std            68.934
exploration/path length Max           210
exploration/path length Min             5
exploration/Rewards Mean                1.0555
exploration/Rewards Std                 0.764154
exploration/Rewards Max                 3.99625
exploration/Rewards Min                -0.108144
exploration/Returns Mean               95.9548
exploration/Returns Std                98.2488
exploration/Returns Max               369.192
exploration/Returns Min                 3.74263
exploration/Actions Mean                0.232806
exploration/Actions Std                 0.654674
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  11
exploration/Average Returns            95.9548
evaluation/num steps total          78468
evaluation/num paths total            610
evaluation/path length Mean           215.75
evaluation/path length Std             44.8407
evaluation/path length Max            276
evaluation/path length Min            155
evaluation/Rewards Mean                 2.34573
evaluation/Rewards Std                  1.27819
evaluation/Rewards Max                  4.8756
evaluation/Rewards Min                  0.168018
evaluation/Returns Mean               506.09
evaluation/Returns Std                168.167
evaluation/Returns Max                727.928
evaluation/Returns Min                288.385
evaluation/Actions Mean                 0.194562
evaluation/Actions Std                  0.810811
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    4
evaluation/Average Returns            506.09
time/data storing (s)                   0.00349645
time/evaluation sampling (s)            0.457397
time/exploration sampling (s)           0.425148
time/logging (s)                        0.0038
time/saving (s)                         0.00174325
time/training (s)                       7.68708
time/epoch (s)                          8.57866
time/total (s)                        717.856
Epoch                                  88
---------------------------------  ----------------
2021-07-02 23:48:26.216771 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 89 finished
---------------------------------  ----------------
replay_buffer/size                 100000
trainer/QF Loss                         0.000995554
trainer/Policy Loss                 -6021.29
trainer/Raw Policy Loss             -6021.29
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           5846.06
trainer/Q Predictions Std            1714.9
trainer/Q Predictions Max            8105.74
trainer/Q Predictions Min            -140.154
trainer/Q Targets Mean               5796.67
trainer/Q Targets Std                1805.35
trainer/Q Targets Max                8259.5
trainer/Q Targets Min                -312.377
trainer/Bellman Errors Mean        131983
trainer/Bellman Errors Std              1.22872e+06
trainer/Bellman Errors Max              1.8626e+07
trainer/Bellman Errors Min              3.98829
trainer/Policy Action Mean              0.053229
trainer/Policy Action Std               0.814213
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total        100000
exploration/num paths total          1639
exploration/path length Mean          166.667
exploration/path length Std           121.357
exploration/path length Max           395
exploration/path length Min            40
exploration/Rewards Mean                1.38678
exploration/Rewards Std                 0.787545
exploration/Rewards Max                 3.80691
exploration/Rewards Min                -0.194375
exploration/Returns Mean              231.129
exploration/Returns Std               223.679
exploration/Returns Max               665.281
exploration/Returns Min                44.7341
exploration/Actions Mean                0.131944
exploration/Actions Std                 0.658717
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                   6
exploration/Average Returns           231.129
evaluation/num steps total          79263
evaluation/num paths total            612
evaluation/path length Mean           397.5
evaluation/path length Std             48.5
evaluation/path length Max            446
evaluation/path length Min            349
evaluation/Rewards Mean                 2.36072
evaluation/Rewards Std                  1.36425
evaluation/Rewards Max                  5.55255
evaluation/Rewards Min                  0.299555
evaluation/Returns Mean               938.385
evaluation/Returns Std                117.786
evaluation/Returns Max               1056.17
evaluation/Returns Min                820.599
evaluation/Actions Mean                 0.14428
evaluation/Actions Std                  0.735469
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    2
evaluation/Average Returns            938.385
time/data storing (s)                   0.00348007
time/evaluation sampling (s)            0.393124
time/exploration sampling (s)           0.411397
time/logging (s)                        0.00546523
time/saving (s)                         0.00204947
time/training (s)                       6.85102
time/epoch (s)                          7.66654
time/total (s)                        725.525
Epoch                                  89
---------------------------------  ----------------
2021-07-02 23:48:34.094203 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 90 finished
---------------------------------  ----------------
replay_buffer/size                 101000
trainer/QF Loss                         0.00112636
trainer/Policy Loss                 -6147.4
trainer/Raw Policy Loss             -6147.4
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           5971.53
trainer/Q Predictions Std            1727.34
trainer/Q Predictions Max            8098.89
trainer/Q Predictions Min            -247.153
trainer/Q Targets Mean               5985.51
trainer/Q Targets Std                1751.9
trainer/Q Targets Max                8119.92
trainer/Q Targets Min                  -0.864292
trainer/Bellman Errors Mean         49687.6
trainer/Bellman Errors Std         212612
trainer/Bellman Errors Max              2.52484e+06
trainer/Bellman Errors Min              0.230381
trainer/Policy Action Mean              0.0362054
trainer/Policy Action Std               0.810725
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total        101000
exploration/num paths total          1650
exploration/path length Mean           90.9091
exploration/path length Std            89.1112
exploration/path length Max           290
exploration/path length Min             9
exploration/Rewards Mean                1.37311
exploration/Rewards Std                 0.814899
exploration/Rewards Max                 3.8628
exploration/Rewards Min                -0.0180869
exploration/Returns Mean              124.828
exploration/Returns Std               132.733
exploration/Returns Max               330.278
exploration/Returns Min                 6.0207
exploration/Actions Mean                0.130591
exploration/Actions Std                 0.682056
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  11
exploration/Average Returns           124.828
evaluation/num steps total          80102
evaluation/num paths total            615
evaluation/path length Mean           279.667
evaluation/path length Std             62.4731
evaluation/path length Max            357
evaluation/path length Min            204
evaluation/Rewards Mean                 2.65544
evaluation/Rewards Std                  1.30361
evaluation/Rewards Max                  5.98815
evaluation/Rewards Min                  0.531748
evaluation/Returns Mean               742.638
evaluation/Returns Std                161.949
evaluation/Returns Max                916.359
evaluation/Returns Min                526.521
evaluation/Actions Mean                 0.153692
evaluation/Actions Std                  0.785847
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    3
evaluation/Average Returns            742.638
time/data storing (s)                   0.00341996
time/evaluation sampling (s)            0.403698
time/exploration sampling (s)           0.391303
time/logging (s)                        0.00397886
time/saving (s)                         0.00199457
time/training (s)                       7.06839
time/epoch (s)                          7.87279
time/total (s)                        733.399
Epoch                                  90
---------------------------------  ----------------
2021-07-02 23:48:42.450652 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 91 finished
---------------------------------  ----------------
replay_buffer/size                 102000
trainer/QF Loss                         0.0012247
trainer/Policy Loss                 -6003.39
trainer/Raw Policy Loss             -6003.39
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           5819.68
trainer/Q Predictions Std            1941.7
trainer/Q Predictions Max            8282.06
trainer/Q Predictions Min            -203.428
trainer/Q Targets Mean               5820.18
trainer/Q Targets Std                2004.08
trainer/Q Targets Max                8283.91
trainer/Q Targets Min                  -0.98357
trainer/Bellman Errors Mean         90799.2
trainer/Bellman Errors Std         695735
trainer/Bellman Errors Max              9.94382e+06
trainer/Bellman Errors Min              0.0182936
trainer/Policy Action Mean             -0.0044956
trainer/Policy Action Std               0.829171
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total        102000
exploration/num paths total          1657
exploration/path length Mean          142.857
exploration/path length Std           150.52
exploration/path length Max           464
exploration/path length Min             8
exploration/Rewards Mean                1.08157
exploration/Rewards Std                 0.905622
exploration/Rewards Max                 4.30052
exploration/Rewards Min                -0.25005
exploration/Returns Mean              154.511
exploration/Returns Std               220.931
exploration/Returns Max               652.588
exploration/Returns Min                 5.63467
exploration/Actions Mean                0.14314
exploration/Actions Std                 0.643605
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                   7
exploration/Average Returns           154.511
evaluation/num steps total          80613
evaluation/num paths total            616
evaluation/path length Mean           511
evaluation/path length Std              0
evaluation/path length Max            511
evaluation/path length Min            511
evaluation/Rewards Mean                 1.81437
evaluation/Rewards Std                  1.3318
evaluation/Rewards Max                  5.33897
evaluation/Rewards Min                  0.101439
evaluation/Returns Mean               927.145
evaluation/Returns Std                  0
evaluation/Returns Max                927.145
evaluation/Returns Min                927.145
evaluation/Actions Mean                 0.119179
evaluation/Actions Std                  0.727081
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    1
evaluation/Average Returns            927.145
time/data storing (s)                   0.00599839
time/evaluation sampling (s)            0.385952
time/exploration sampling (s)           0.448906
time/logging (s)                        0.00473495
time/saving (s)                         0.00210697
time/training (s)                       7.50709
time/epoch (s)                          8.35479
time/total (s)                        741.756
Epoch                                  91
---------------------------------  ----------------
2021-07-02 23:48:50.315163 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 92 finished
---------------------------------  ----------------
replay_buffer/size                 103000
trainer/QF Loss                         0.000949923
trainer/Policy Loss                 -6320.91
trainer/Raw Policy Loss             -6320.91
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           6145.9
trainer/Q Predictions Std            1722.07
trainer/Q Predictions Max            8271.65
trainer/Q Predictions Min            -375.24
trainer/Q Targets Mean               6139.72
trainer/Q Targets Std                1757.33
trainer/Q Targets Max                8293.78
trainer/Q Targets Min                  -0.0976627
trainer/Bellman Errors Mean         96188.6
trainer/Bellman Errors Std              1.07984e+06
trainer/Bellman Errors Max              1.72717e+07
trainer/Bellman Errors Min              0.243693
trainer/Policy Action Mean             -0.0364589
trainer/Policy Action Std               0.800765
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total        103000
exploration/num paths total          1670
exploration/path length Mean           76.9231
exploration/path length Std            55.5081
exploration/path length Max           171
exploration/path length Min             9
exploration/Rewards Mean                0.924879
exploration/Rewards Std                 0.671958
exploration/Rewards Max                 3.65057
exploration/Rewards Min                -0.433039
exploration/Returns Mean               71.1446
exploration/Returns Std                79.1198
exploration/Returns Max               292.104
exploration/Returns Min                 5.27156
exploration/Actions Mean                0.156941
exploration/Actions Std                 0.646368
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  13
exploration/Average Returns            71.1446
evaluation/num steps total          81613
evaluation/num paths total            617
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 1.03549
evaluation/Rewards Std                  0.618047
evaluation/Rewards Max                  3.08634
evaluation/Rewards Min                 -0.226248
evaluation/Returns Mean              1035.49
evaluation/Returns Std                  0
evaluation/Returns Max               1035.49
evaluation/Returns Min               1035.49
evaluation/Actions Mean                 0.258256
evaluation/Actions Std                  0.625241
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    1
evaluation/Average Returns           1035.49
time/data storing (s)                   0.00349865
time/evaluation sampling (s)            0.459517
time/exploration sampling (s)           0.403426
time/logging (s)                        0.00597566
time/saving (s)                         0.00161009
time/training (s)                       6.9886
time/epoch (s)                          7.86263
time/total (s)                        749.621
Epoch                                  92
---------------------------------  ----------------
2021-07-02 23:48:58.290608 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 93 finished
---------------------------------  ----------------
replay_buffer/size                 104000
trainer/QF Loss                         0.000828881
trainer/Policy Loss                 -6451.57
trainer/Raw Policy Loss             -6451.57
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           6178.85
trainer/Q Predictions Std            1780.53
trainer/Q Predictions Max            8206.2
trainer/Q Predictions Min            -246.05
trainer/Q Targets Mean               6191.3
trainer/Q Targets Std                1798.13
trainer/Q Targets Max                8171.41
trainer/Q Targets Min                   0.100985
trainer/Bellman Errors Mean         34979.9
trainer/Bellman Errors Std          81754.2
trainer/Bellman Errors Max         725952
trainer/Bellman Errors Min              0.237466
trainer/Policy Action Mean             -0.0646324
trainer/Policy Action Std               0.806882
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total        104000
exploration/num paths total          1679
exploration/path length Mean          111.111
exploration/path length Std            73.2066
exploration/path length Max           236
exploration/path length Min             9
exploration/Rewards Mean                1.25836
exploration/Rewards Std                 0.739523
exploration/Rewards Max                 3.73249
exploration/Rewards Min                -0.0814136
exploration/Returns Mean              139.818
exploration/Returns Std               143.845
exploration/Returns Max               472.802
exploration/Returns Min                 7.51871
exploration/Actions Mean                0.160599
exploration/Actions Std                 0.666142
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                   9
exploration/Average Returns           139.818
evaluation/num steps total          82613
evaluation/num paths total            618
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 0.752768
evaluation/Rewards Std                  0.382893
evaluation/Rewards Max                  1.7546
evaluation/Rewards Min                 -0.0905748
evaluation/Returns Mean               752.768
evaluation/Returns Std                  0
evaluation/Returns Max                752.768
evaluation/Returns Min                752.768
evaluation/Actions Mean                 0.0819975
evaluation/Actions Std                  0.615555
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    1
evaluation/Average Returns            752.768
time/data storing (s)                   0.00593974
time/evaluation sampling (s)            0.392687
time/exploration sampling (s)           0.510654
time/logging (s)                        0.00398662
time/saving (s)                         0.00157831
time/training (s)                       7.05567
time/epoch (s)                          7.97051
time/total (s)                        757.593
Epoch                                  93
---------------------------------  ----------------
2021-07-02 23:49:06.072186 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 94 finished
---------------------------------  ----------------
replay_buffer/size                 105000
trainer/QF Loss                         0.00105255
trainer/Policy Loss                 -6213.75
trainer/Raw Policy Loss             -6213.75
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           6033.88
trainer/Q Predictions Std            1908.44
trainer/Q Predictions Max            8356.01
trainer/Q Predictions Min             -48.8015
trainer/Q Targets Mean               6044.68
trainer/Q Targets Std                1912.84
trainer/Q Targets Max                8460.74
trainer/Q Targets Min                -120.906
trainer/Bellman Errors Mean         33884.6
trainer/Bellman Errors Std         155396
trainer/Bellman Errors Max              2.01281e+06
trainer/Bellman Errors Min              0.914974
trainer/Policy Action Mean             -0.132081
trainer/Policy Action Std               0.814157
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total        105000
exploration/num paths total          1691
exploration/path length Mean           83.3333
exploration/path length Std            71.6733
exploration/path length Max           207
exploration/path length Min             9
exploration/Rewards Mean                0.518067
exploration/Rewards Std                 0.420429
exploration/Rewards Max                 1.619
exploration/Rewards Min                -0.579076
exploration/Returns Mean               43.1723
exploration/Returns Std                38.1236
exploration/Returns Max               102.791
exploration/Returns Min                 4.95029
exploration/Actions Mean                0.21383
exploration/Actions Std                 0.632272
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  12
exploration/Average Returns            43.1723
evaluation/num steps total          83613
evaluation/num paths total            619
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 0.442597
evaluation/Rewards Std                  0.369741
evaluation/Rewards Max                  1.22716
evaluation/Rewards Min                 -0.365901
evaluation/Returns Mean               442.597
evaluation/Returns Std                  0
evaluation/Returns Max                442.597
evaluation/Returns Min                442.597
evaluation/Actions Mean                 0.176328
evaluation/Actions Std                  0.585861
evaluation/Actions Max                  1
evaluation/Actions Min                 -0.999965
evaluation/Num Paths                    1
evaluation/Average Returns            442.597
time/data storing (s)                   0.00346644
time/evaluation sampling (s)            0.395787
time/exploration sampling (s)           0.39398
time/logging (s)                        0.00401584
time/saving (s)                         0.00154337
time/training (s)                       6.98019
time/epoch (s)                          7.77898
time/total (s)                        765.374
Epoch                                  94
---------------------------------  ----------------
2021-07-02 23:49:14.014443 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 95 finished
---------------------------------  ----------------
replay_buffer/size                 106000
trainer/QF Loss                         0.000991557
trainer/Policy Loss                 -6507.9
trainer/Raw Policy Loss             -6507.9
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           6300.34
trainer/Q Predictions Std            1929.18
trainer/Q Predictions Max            8975.36
trainer/Q Predictions Min            -178.197
trainer/Q Targets Mean               6254.6
trainer/Q Targets Std                2010.78
trainer/Q Targets Max                8664.37
trainer/Q Targets Min                -294.313
trainer/Bellman Errors Mean         96374.3
trainer/Bellman Errors Std         698780
trainer/Bellman Errors Max              8.59643e+06
trainer/Bellman Errors Min              0.00972843
trainer/Policy Action Mean             -0.0747933
trainer/Policy Action Std               0.805222
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total        106000
exploration/num paths total          1706
exploration/path length Mean           66.6667
exploration/path length Std            48.3455
exploration/path length Max           159
exploration/path length Min             9
exploration/Rewards Mean                0.65278
exploration/Rewards Std                 0.480154
exploration/Rewards Max                 2.41723
exploration/Rewards Min                -0.838442
exploration/Returns Mean               43.5187
exploration/Returns Std                43.2231
exploration/Returns Max               163.27
exploration/Returns Min                 7.4417
exploration/Actions Mean                0.147735
exploration/Actions Std                 0.649119
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  15
exploration/Average Returns            43.5187
evaluation/num steps total          84613
evaluation/num paths total            620
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 0.614116
evaluation/Rewards Std                  0.439429
evaluation/Rewards Max                  1.89817
evaluation/Rewards Min                 -0.580849
evaluation/Returns Mean               614.116
evaluation/Returns Std                  0
evaluation/Returns Max                614.116
evaluation/Returns Min                614.116
evaluation/Actions Mean                 0.117127
evaluation/Actions Std                  0.64944
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    1
evaluation/Average Returns            614.116
time/data storing (s)                   0.0035388
time/evaluation sampling (s)            0.38497
time/exploration sampling (s)           0.412819
time/logging (s)                        0.00411633
time/saving (s)                         0.00152992
time/training (s)                       7.13278
time/epoch (s)                          7.93976
time/total (s)                        773.316
Epoch                                  95
---------------------------------  ----------------
2021-07-02 23:49:21.715849 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 96 finished
---------------------------------  ----------------
replay_buffer/size                 107000
trainer/QF Loss                         0.000992241
trainer/Policy Loss                 -6621.97
trainer/Raw Policy Loss             -6621.97
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           6452.24
trainer/Q Predictions Std            1908.91
trainer/Q Predictions Max            8868.76
trainer/Q Predictions Min            -162.43
trainer/Q Targets Mean               6444.47
trainer/Q Targets Std                1939.93
trainer/Q Targets Max                8885.14
trainer/Q Targets Min                 -24.1771
trainer/Bellman Errors Mean         58021.4
trainer/Bellman Errors Std         223368
trainer/Bellman Errors Max              1.87245e+06
trainer/Bellman Errors Min              4.72978
trainer/Policy Action Mean             -0.123222
trainer/Policy Action Std               0.794118
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total        107000
exploration/num paths total          1727
exploration/path length Mean           47.619
exploration/path length Std            44.8335
exploration/path length Max           189
exploration/path length Min             7
exploration/Rewards Mean                0.673838
exploration/Rewards Std                 0.458324
exploration/Rewards Max                 2.18869
exploration/Rewards Min                -0.376583
exploration/Returns Mean               32.0875
exploration/Returns Std                30.2724
exploration/Returns Max               142.092
exploration/Returns Min                 4.41361
exploration/Actions Mean                0.199182
exploration/Actions Std                 0.630775
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  21
exploration/Average Returns            32.0875
evaluation/num steps total          85613
evaluation/num paths total            621
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 0.965391
evaluation/Rewards Std                  0.47555
evaluation/Rewards Max                  2.07731
evaluation/Rewards Min                 -0.0522859
evaluation/Returns Mean               965.391
evaluation/Returns Std                  0
evaluation/Returns Max                965.391
evaluation/Returns Min                965.391
evaluation/Actions Mean                 0.0517815
evaluation/Actions Std                  0.672921
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    1
evaluation/Average Returns            965.391
time/data storing (s)                   0.00354536
time/evaluation sampling (s)            0.3907
time/exploration sampling (s)           0.399302
time/logging (s)                        0.00430044
time/saving (s)                         0.00195449
time/training (s)                       6.8992
time/epoch (s)                          7.699
time/total (s)                        781.017
Epoch                                  96
---------------------------------  ----------------
2021-07-02 23:49:29.642824 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 97 finished
---------------------------------  ----------------
replay_buffer/size                 108000
trainer/QF Loss                         0.00151941
trainer/Policy Loss                 -6615.57
trainer/Raw Policy Loss             -6615.57
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           6367.98
trainer/Q Predictions Std            2070.96
trainer/Q Predictions Max            8653.95
trainer/Q Predictions Min            -112.801
trainer/Q Targets Mean               6337.08
trainer/Q Targets Std                2096.64
trainer/Q Targets Max                8783.1
trainer/Q Targets Min                -168.643
trainer/Bellman Errors Mean         50807.5
trainer/Bellman Errors Std         153252
trainer/Bellman Errors Max              1.49431e+06
trainer/Bellman Errors Min              0.217899
trainer/Policy Action Mean             -0.0563195
trainer/Policy Action Std               0.814778
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total        108000
exploration/num paths total          1735
exploration/path length Mean          125
exploration/path length Std            99.2509
exploration/path length Max           337
exploration/path length Min            11
exploration/Rewards Mean                0.747765
exploration/Rewards Std                 0.480555
exploration/Rewards Max                 2.84159
exploration/Rewards Min                -0.682579
exploration/Returns Mean               93.4706
exploration/Returns Std                59.3942
exploration/Returns Max               184.254
exploration/Returns Min                 9.75339
exploration/Actions Mean                0.187818
exploration/Actions Std                 0.643073
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                   8
exploration/Average Returns            93.4706
evaluation/num steps total          86613
evaluation/num paths total            622
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 1.13127
evaluation/Rewards Std                  0.527613
evaluation/Rewards Max                  2.71025
evaluation/Rewards Min                 -0.134375
evaluation/Returns Mean              1131.27
evaluation/Returns Std                  0
evaluation/Returns Max               1131.27
evaluation/Returns Min               1131.27
evaluation/Actions Mean                 0.0719092
evaluation/Actions Std                  0.690829
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    1
evaluation/Average Returns           1131.27
time/data storing (s)                   0.00352268
time/evaluation sampling (s)            0.376243
time/exploration sampling (s)           0.394312
time/logging (s)                        0.0046956
time/saving (s)                         0.00210096
time/training (s)                       7.1438
time/epoch (s)                          7.92467
time/total (s)                        788.943
Epoch                                  97
---------------------------------  ----------------
2021-07-02 23:49:38.194534 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 98 finished
---------------------------------  ----------------
replay_buffer/size                 109000
trainer/QF Loss                         0.000698464
trainer/Policy Loss                 -6832.19
trainer/Raw Policy Loss             -6832.19
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           6611.12
trainer/Q Predictions Std            1799.82
trainer/Q Predictions Max            8630.61
trainer/Q Predictions Min             -52.1365
trainer/Q Targets Mean               6639.46
trainer/Q Targets Std                1769.61
trainer/Q Targets Max                8559.29
trainer/Q Targets Min                -230.463
trainer/Bellman Errors Mean         51152.7
trainer/Bellman Errors Std         189733
trainer/Bellman Errors Max              1.56511e+06
trainer/Bellman Errors Min              0.119173
trainer/Policy Action Mean             -0.0632484
trainer/Policy Action Std               0.808797
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total        109000
exploration/num paths total          1749
exploration/path length Mean           71.4286
exploration/path length Std            54.4541
exploration/path length Max           209
exploration/path length Min             9
exploration/Rewards Mean                0.812901
exploration/Rewards Std                 0.44125
exploration/Rewards Max                 2.13776
exploration/Rewards Min                -0.826348
exploration/Returns Mean               58.0643
exploration/Returns Std                54.4414
exploration/Returns Max               206.982
exploration/Returns Min                 6.02552
exploration/Actions Mean                0.187077
exploration/Actions Std                 0.644848
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  14
exploration/Average Returns            58.0643
evaluation/num steps total          87613
evaluation/num paths total            623
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 0.963299
evaluation/Rewards Std                  0.421434
evaluation/Rewards Max                  2.15214
evaluation/Rewards Min                 -0.212533
evaluation/Returns Mean               963.299
evaluation/Returns Std                  0
evaluation/Returns Max                963.299
evaluation/Returns Min                963.299
evaluation/Actions Mean                 0.125993
evaluation/Actions Std                  0.66749
evaluation/Actions Max                  1
evaluation/Actions Min                 -0.999987
evaluation/Num Paths                    1
evaluation/Average Returns            963.299
time/data storing (s)                   0.00365945
time/evaluation sampling (s)            0.432521
time/exploration sampling (s)           0.530052
time/logging (s)                        0.0041486
time/saving (s)                         0.00152671
time/training (s)                       7.57648
time/epoch (s)                          8.54839
time/total (s)                        797.494
Epoch                                  98
---------------------------------  ----------------
2021-07-02 23:49:46.155414 CST | [Test ddpg based dsfpg on hopper env_2021_07_02_23_36_21_0000--s-0] Epoch 99 finished
---------------------------------  ----------------
replay_buffer/size                 110000
trainer/QF Loss                         0.000881379
trainer/Policy Loss                 -7015.85
trainer/Raw Policy Loss             -7015.85
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           6811.4
trainer/Q Predictions Std            1752.51
trainer/Q Predictions Max            8977.39
trainer/Q Predictions Min            -295.476
trainer/Q Targets Mean               6827.7
trainer/Q Targets Std                1762.46
trainer/Q Targets Max                9076.1
trainer/Q Targets Min                -206.652
trainer/Bellman Errors Mean         36072.5
trainer/Bellman Errors Std         119806
trainer/Bellman Errors Max              1.45859e+06
trainer/Bellman Errors Min              0.000105143
trainer/Policy Action Mean             -0.128244
trainer/Policy Action Std               0.813769
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total        110000
exploration/num paths total          1758
exploration/path length Mean          111.111
exploration/path length Std            57.4852
exploration/path length Max           209
exploration/path length Min            27
exploration/Rewards Mean                0.80524
exploration/Rewards Std                 0.475183
exploration/Rewards Max                 2.08591
exploration/Rewards Min                -0.669756
exploration/Returns Mean               89.4711
exploration/Returns Std                54.7864
exploration/Returns Max               177.021
exploration/Returns Min                 3.51688
exploration/Actions Mean                0.165147
exploration/Actions Std                 0.654932
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                   9
exploration/Average Returns            89.4711
evaluation/num steps total          88613
evaluation/num paths total            624
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 0.914166
evaluation/Rewards Std                  0.378008
evaluation/Rewards Max                  1.79429
evaluation/Rewards Min                  0.00488082
evaluation/Returns Mean               914.166
evaluation/Returns Std                  0
evaluation/Returns Max                914.166
evaluation/Returns Min                914.166
evaluation/Actions Mean                 0.114894
evaluation/Actions Std                  0.663267
evaluation/Actions Max                  1
evaluation/Actions Min                 -0.99992
evaluation/Num Paths                    1
evaluation/Average Returns            914.166
time/data storing (s)                   0.00341957
time/evaluation sampling (s)            0.408775
time/exploration sampling (s)           0.406937
time/logging (s)                        0.00605499
time/saving (s)                         0.00206481
time/training (s)                       7.1331
time/epoch (s)                          7.96035
time/total (s)                        805.456
Epoch                                  99
---------------------------------  ----------------
