2021-07-02 23:38:25.235183 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 0 finished
---------------------------------  ---------------
replay_buffer/size                 11000
trainer/QF Loss                        1.11728
trainer/Policy Loss                   -0.00146782
trainer/Raw Policy Loss               -0.00146782
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean             0.00147528
trainer/Q Predictions Std              0.000867932
trainer/Q Predictions Max              0.00475133
trainer/Q Predictions Min              0.00040274
trainer/Q Targets Mean                 0.88298
trainer/Q Targets Std                  0.58319
trainer/Q Targets Max                  2.40729
trainer/Q Targets Min                 -0.504848
trainer/Bellman Errors Mean            1.11728
trainer/Bellman Errors Std             1.17044
trainer/Bellman Errors Max             5.79217
trainer/Bellman Errors Min             8.74926e-05
trainer/Policy Action Mean             0.000329176
trainer/Policy Action Std              0.00179152
trainer/Policy Action Max              0.00732259
trainer/Policy Action Min             -0.00542517
exploration/num steps total        11000
exploration/num paths total          574
exploration/path length Mean          17.2414
exploration/path length Std            8.73619
exploration/path length Max           52
exploration/path length Min            8
exploration/Rewards Mean               0.738875
exploration/Rewards Std                0.522534
exploration/Rewards Max                2.03678
exploration/Rewards Min               -1.49658
exploration/Returns Mean              12.7392
exploration/Returns Std               11.392
exploration/Returns Max               57.9151
exploration/Returns Min                1.44981
exploration/Actions Mean               0.0131855
exploration/Actions Std                0.493689
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                 58
exploration/Average Returns           12.7392
evaluation/num steps total           970
evaluation/num paths total             7
evaluation/path length Mean          138.571
evaluation/path length Std            13.0915
evaluation/path length Max           163
evaluation/path length Min           127
evaluation/Rewards Mean                0.980817
evaluation/Rewards Std                 0.0341618
evaluation/Rewards Max                 1.02834
evaluation/Rewards Min                 0.773631
evaluation/Returns Mean              135.913
evaluation/Returns Std                13.755
evaluation/Returns Max               159.351
evaluation/Returns Min               118.081
evaluation/Actions Mean                0.000128571
evaluation/Actions Std                 0.000365768
evaluation/Actions Max                 0.000687975
evaluation/Actions Min                -0.000568682
evaluation/Num Paths                   7
evaluation/Average Returns           135.913
time/data storing (s)                  0.00353988
time/evaluation sampling (s)           0.24944
time/exploration sampling (s)          0.266115
time/logging (s)                       0.00409818
time/saving (s)                        0.0013369
time/training (s)                      6.38804
time/epoch (s)                         6.91257
time/total (s)                         9.8684
Epoch                                  0
---------------------------------  ---------------
2021-07-02 23:38:32.384411 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 1 finished
---------------------------------  ---------------
replay_buffer/size                 12000
trainer/QF Loss                        2.83414
trainer/Policy Loss                  -12.9941
trainer/Raw Policy Loss              -12.9941
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean            11.0024
trainer/Q Predictions Std              6.0559
trainer/Q Predictions Max             26.1607
trainer/Q Predictions Min             -2.87208
trainer/Q Targets Mean                11.1961
trainer/Q Targets Std                  6.13598
trainer/Q Targets Max                 26.0319
trainer/Q Targets Min                 -3.7186
trainer/Bellman Errors Mean            2.83414
trainer/Bellman Errors Std            25.0263
trainer/Bellman Errors Max           284.304
trainer/Bellman Errors Min             1.31815e-05
trainer/Policy Action Mean             0.999796
trainer/Policy Action Std              0.000511883
trainer/Policy Action Max              1
trainer/Policy Action Min              0.996918
exploration/num steps total        12000
exploration/num paths total          616
exploration/path length Mean          23.8095
exploration/path length Std            2.70131
exploration/path length Max           33
exploration/path length Min           17
exploration/Rewards Mean               1.73004
exploration/Rewards Std                0.473863
exploration/Rewards Max                2.72308
exploration/Rewards Min                1.00571
exploration/Returns Mean              41.1914
exploration/Returns Std                4.48826
exploration/Returns Max               57.1831
exploration/Returns Min               26.5342
exploration/Actions Mean               0.788433
exploration/Actions Std                0.30595
exploration/Actions Max                1
exploration/Actions Min               -0.929105
exploration/Num Paths                 42
exploration/Average Returns           41.1914
evaluation/num steps total          1961
evaluation/num paths total            52
evaluation/path length Mean           22.0222
evaluation/path length Std             0.576922
evaluation/path length Max            23
evaluation/path length Min            21
evaluation/Rewards Mean                1.75697
evaluation/Rewards Std                 0.464365
evaluation/Rewards Max                 2.41295
evaluation/Rewards Min                 1.01038
evaluation/Returns Mean               38.6923
evaluation/Returns Std                 0.96885
evaluation/Returns Max                40.721
evaluation/Returns Min                36.9011
evaluation/Actions Mean                0.999595
evaluation/Actions Std                 0.00073062
evaluation/Actions Max                 1
evaluation/Actions Min                 0.9964
evaluation/Num Paths                  45
evaluation/Average Returns            38.6923
time/data storing (s)                  0.00349141
time/evaluation sampling (s)           0.274145
time/exploration sampling (s)          0.268139
time/logging (s)                       0.00425676
time/saving (s)                        0.00131497
time/training (s)                      6.59624
time/epoch (s)                         7.14759
time/total (s)                        17.0175
Epoch                                  1
---------------------------------  ---------------
2021-07-02 23:38:39.447824 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 2 finished
---------------------------------  ---------------
replay_buffer/size                 13000
trainer/QF Loss                        2.08318
trainer/Policy Loss                  -34.904
trainer/Raw Policy Loss              -34.904
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean            31.3347
trainer/Q Predictions Std             12.2347
trainer/Q Predictions Max             59.7356
trainer/Q Predictions Min             -0.523917
trainer/Q Targets Mean                31.0648
trainer/Q Targets Std                 12.3734
trainer/Q Targets Max                 61.632
trainer/Q Targets Min                 -0.366958
trainer/Bellman Errors Mean            2.08318
trainer/Bellman Errors Std             5.03731
trainer/Bellman Errors Max            42.2615
trainer/Bellman Errors Min             6.35642e-07
trainer/Policy Action Mean             0.365139
trainer/Policy Action Std              0.860876
trainer/Policy Action Max              1
trainer/Policy Action Min             -0.999938
exploration/num steps total        13000
exploration/num paths total          642
exploration/path length Mean          38.4615
exploration/path length Std           13.5738
exploration/path length Max           67
exploration/path length Min           11
exploration/Rewards Mean               1.69739
exploration/Rewards Std                0.381942
exploration/Rewards Max                2.50365
exploration/Rewards Min                0.844432
exploration/Returns Mean              65.2842
exploration/Returns Std               26.017
exploration/Returns Max              120.991
exploration/Returns Min               13.4279
exploration/Actions Mean               0.439649
exploration/Actions Std                0.605469
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                 26
exploration/Average Returns           65.2842
evaluation/num steps total          2930
evaluation/num paths total            74
evaluation/path length Mean           44.0455
evaluation/path length Std             0.824471
evaluation/path length Max            45
evaluation/path length Min            43
evaluation/Rewards Mean                1.7416
evaluation/Rewards Std                 0.327189
evaluation/Rewards Max                 2.19447
evaluation/Rewards Min                 0.924187
evaluation/Returns Mean               76.7096
evaluation/Returns Std                 2.0396
evaluation/Returns Max                79.3306
evaluation/Returns Min                73.7783
evaluation/Actions Mean                0.498337
evaluation/Actions Std                 0.734727
evaluation/Actions Max                 1
evaluation/Actions Min                -0.981831
evaluation/Num Paths                  22
evaluation/Average Returns            76.7096
time/data storing (s)                  0.00344645
time/evaluation sampling (s)           0.245321
time/exploration sampling (s)          0.266167
time/logging (s)                       0.00462801
time/saving (s)                        0.00169701
time/training (s)                      6.54054
time/epoch (s)                         7.0618
time/total (s)                        24.0809
Epoch                                  2
---------------------------------  ---------------
2021-07-02 23:38:46.464457 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 3 finished
---------------------------------  ---------------
replay_buffer/size                 14000
trainer/QF Loss                        6.68199
trainer/Policy Loss                  -47.9836
trainer/Raw Policy Loss              -47.9836
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean            45.2713
trainer/Q Predictions Std             21.2965
trainer/Q Predictions Max             89.5059
trainer/Q Predictions Min             -1.03516
trainer/Q Targets Mean                45.5842
trainer/Q Targets Std                 21.0118
trainer/Q Targets Max                 88.1659
trainer/Q Targets Min                 -2.00627
trainer/Bellman Errors Mean            6.68199
trainer/Bellman Errors Std            20.1878
trainer/Bellman Errors Max           140.351
trainer/Bellman Errors Min             6.22409e-06
trainer/Policy Action Mean             0.364186
trainer/Policy Action Std              0.848589
trainer/Policy Action Max              1
trainer/Policy Action Min             -0.999992
exploration/num steps total        14000
exploration/num paths total          654
exploration/path length Mean          83.3333
exploration/path length Std            4.71405
exploration/path length Max           88
exploration/path length Min           70
exploration/Rewards Mean               2.09786
exploration/Rewards Std                0.610974
exploration/Rewards Max                3.36703
exploration/Rewards Min                0.764079
exploration/Returns Mean             174.822
exploration/Returns Std               15.0142
exploration/Returns Max              189.168
exploration/Returns Min              133.671
exploration/Actions Mean               0.3771
exploration/Actions Std                0.571768
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                 12
exploration/Average Returns          174.822
evaluation/num steps total          3882
evaluation/num paths total            85
evaluation/path length Mean           86.5455
evaluation/path length Std             0.49793
evaluation/path length Max            87
evaluation/path length Min            86
evaluation/Rewards Mean                2.12621
evaluation/Rewards Std                 0.628848
evaluation/Rewards Max                 3.40842
evaluation/Rewards Min                 0.803997
evaluation/Returns Mean              184.014
evaluation/Returns Std                 2.94993
evaluation/Returns Max               187.964
evaluation/Returns Min               177.783
evaluation/Actions Mean                0.454872
evaluation/Actions Std                 0.580422
evaluation/Actions Max                 1
evaluation/Actions Min                -0.99736
evaluation/Num Paths                  11
evaluation/Average Returns           184.014
time/data storing (s)                  0.00336943
time/evaluation sampling (s)           0.290585
time/exploration sampling (s)          0.259613
time/logging (s)                       0.00384433
time/saving (s)                        0.00124826
time/training (s)                      6.45496
time/epoch (s)                         7.01362
time/total (s)                        31.0965
Epoch                                  3
---------------------------------  ---------------
2021-07-02 23:38:53.475982 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 4 finished
---------------------------------  ---------------
replay_buffer/size                 15000
trainer/QF Loss                       11.4442
trainer/Policy Loss                  -75.9351
trainer/Raw Policy Loss              -75.9351
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean            71.0681
trainer/Q Predictions Std             32.6878
trainer/Q Predictions Max            148.543
trainer/Q Predictions Min             -7.14845
trainer/Q Targets Mean                70.2164
trainer/Q Targets Std                 31.751
trainer/Q Targets Max                150.049
trainer/Q Targets Min                 -2.84369
trainer/Bellman Errors Mean           11.4442
trainer/Bellman Errors Std            30.9826
trainer/Bellman Errors Max           222.157
trainer/Bellman Errors Min             3.95216e-05
trainer/Policy Action Mean             0.511811
trainer/Policy Action Std              0.804987
trainer/Policy Action Max              1
trainer/Policy Action Min             -0.999998
exploration/num steps total        15000
exploration/num paths total          667
exploration/path length Mean          76.9231
exploration/path length Std           16.4479
exploration/path length Max           94
exploration/path length Min           51
exploration/Rewards Mean               2.02244
exploration/Rewards Std                0.543186
exploration/Rewards Max                3.40475
exploration/Rewards Min                0.893127
exploration/Returns Mean             155.572
exploration/Returns Std               42.8537
exploration/Returns Max              208.728
exploration/Returns Min               88.9903
exploration/Actions Mean               0.382506
exploration/Actions Std                0.629743
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                 13
exploration/Average Returns          155.572
evaluation/num steps total          4863
evaluation/num paths total            96
evaluation/path length Mean           89.1818
evaluation/path length Std             0.935966
evaluation/path length Max            90
evaluation/path length Min            88
evaluation/Rewards Mean                2.02698
evaluation/Rewards Std                 0.472438
evaluation/Rewards Max                 3.06598
evaluation/Rewards Min                 0.912651
evaluation/Returns Mean              180.77
evaluation/Returns Std                 1.11795
evaluation/Returns Max               182.664
evaluation/Returns Min               178.648
evaluation/Actions Mean                0.392734
evaluation/Actions Std                 0.862948
evaluation/Actions Max                 1
evaluation/Actions Min                -0.999133
evaluation/Num Paths                  11
evaluation/Average Returns           180.77
time/data storing (s)                  0.00348921
time/evaluation sampling (s)           0.241569
time/exploration sampling (s)          0.263021
time/logging (s)                       0.00441342
time/saving (s)                        0.00137202
time/training (s)                      6.49626
time/epoch (s)                         7.01013
time/total (s)                        38.1082
Epoch                                  4
---------------------------------  ---------------
2021-07-02 23:39:00.231639 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 5 finished
---------------------------------  ---------------
replay_buffer/size                 16000
trainer/QF Loss                      176.606
trainer/Policy Loss                 -109.737
trainer/Raw Policy Loss             -109.737
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean           103.193
trainer/Q Predictions Std             47.0119
trainer/Q Predictions Max            198.761
trainer/Q Predictions Min             -3.45037
trainer/Q Targets Mean                98.7506
trainer/Q Targets Std                 47.8244
trainer/Q Targets Max                193.785
trainer/Q Targets Min                 -3.67511
trainer/Bellman Errors Mean          176.606
trainer/Bellman Errors Std          1144.71
trainer/Bellman Errors Max         10828.6
trainer/Bellman Errors Min             9.88255e-05
trainer/Policy Action Mean             0.327954
trainer/Policy Action Std              0.898587
trainer/Policy Action Max              1
trainer/Policy Action Min             -1
exploration/num steps total        16000
exploration/num paths total          683
exploration/path length Mean          62.5
exploration/path length Std           33.371
exploration/path length Max           94
exploration/path length Min            9
exploration/Rewards Mean               2.04452
exploration/Rewards Std                0.658251
exploration/Rewards Max                3.43797
exploration/Rewards Min                0.536692
exploration/Returns Mean             127.782
exploration/Returns Std               76.9632
exploration/Returns Max              216.069
exploration/Returns Min                7.61151
exploration/Actions Mean               0.366246
exploration/Actions Std                0.602709
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                 16
exploration/Average Returns          127.782
evaluation/num steps total          5795
evaluation/num paths total           107
evaluation/path length Mean           84.7273
evaluation/path length Std             2.5259
evaluation/path length Max            89
evaluation/path length Min            81
evaluation/Rewards Mean                1.98179
evaluation/Rewards Std                 0.453769
evaluation/Rewards Max                 2.79816
evaluation/Rewards Min                 0.617405
evaluation/Returns Mean              167.911
evaluation/Returns Std                 5.7279
evaluation/Returns Max               177.437
evaluation/Returns Min               159.748
evaluation/Actions Mean                0.471976
evaluation/Actions Std                 0.656381
evaluation/Actions Max                 1
evaluation/Actions Min                -0.999401
evaluation/Num Paths                  11
evaluation/Average Returns           167.911
time/data storing (s)                  0.00335072
time/evaluation sampling (s)           0.250178
time/exploration sampling (s)          0.257753
time/logging (s)                       0.00375313
time/saving (s)                        0.00127955
time/training (s)                      6.23685
time/epoch (s)                         6.75316
time/total (s)                        44.8629
Epoch                                  5
---------------------------------  ---------------
2021-07-02 23:39:06.921850 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 6 finished
---------------------------------  --------------
replay_buffer/size                 17000
trainer/QF Loss                      141.64
trainer/Policy Loss                 -145.575
trainer/Raw Policy Loss             -145.575
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean           137.334
trainer/Q Predictions Std             55.5601
trainer/Q Predictions Max            232.911
trainer/Q Predictions Min             -6.07765
trainer/Q Targets Mean               139.411
trainer/Q Targets Std                 56.6114
trainer/Q Targets Max                227.499
trainer/Q Targets Min                 -6.64703
trainer/Bellman Errors Mean          141.64
trainer/Bellman Errors Std          1164.82
trainer/Bellman Errors Max         13222.2
trainer/Bellman Errors Min             0.00973144
trainer/Policy Action Mean             0.217389
trainer/Policy Action Std              0.914397
trainer/Policy Action Max              1
trainer/Policy Action Min             -1
exploration/num steps total        17000
exploration/num paths total          695
exploration/path length Mean          83.3333
exploration/path length Std           23.4497
exploration/path length Max          109
exploration/path length Min           22
exploration/Rewards Mean               2.16389
exploration/Rewards Std                0.697427
exploration/Rewards Max                3.5849
exploration/Rewards Min                0.596552
exploration/Returns Mean             180.324
exploration/Returns Std               59.3321
exploration/Returns Max              241.567
exploration/Returns Min               33.7281
exploration/Actions Mean               0.267829
exploration/Actions Std                0.637352
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                 12
exploration/Average Returns          180.324
evaluation/num steps total          6717
evaluation/num paths total           117
evaluation/path length Mean           92.2
evaluation/path length Std             0.979796
evaluation/path length Max            94
evaluation/path length Min            91
evaluation/Rewards Mean                2.23904
evaluation/Rewards Std                 0.72577
evaluation/Rewards Max                 3.32886
evaluation/Rewards Min                 0.644704
evaluation/Returns Mean              206.439
evaluation/Returns Std                 3.02687
evaluation/Returns Max               211.813
evaluation/Returns Min               202.319
evaluation/Actions Mean                0.359255
evaluation/Actions Std                 0.692611
evaluation/Actions Max                 1
evaluation/Actions Min                -0.999922
evaluation/Num Paths                  10
evaluation/Average Returns           206.439
time/data storing (s)                  0.00342666
time/evaluation sampling (s)           0.258913
time/exploration sampling (s)          0.264648
time/logging (s)                       0.00393294
time/saving (s)                        0.00168307
time/training (s)                      6.15587
time/epoch (s)                         6.68848
time/total (s)                        51.553
Epoch                                  6
---------------------------------  --------------
2021-07-02 23:39:13.862123 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 7 finished
---------------------------------  --------------
replay_buffer/size                 18000
trainer/QF Loss                       67.4018
trainer/Policy Loss                 -171.721
trainer/Raw Policy Loss             -171.721
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean           158.412
trainer/Q Predictions Std             73.2146
trainer/Q Predictions Max            278.424
trainer/Q Predictions Min             -3.4438
trainer/Q Targets Mean               161.85
trainer/Q Targets Std                 73.3644
trainer/Q Targets Max                274.461
trainer/Q Targets Min                 -4.09695
trainer/Bellman Errors Mean           67.4018
trainer/Bellman Errors Std           214.389
trainer/Bellman Errors Max          2255.22
trainer/Bellman Errors Min             0.00144705
trainer/Policy Action Mean             0.27441
trainer/Policy Action Std              0.912927
trainer/Policy Action Max              1
trainer/Policy Action Min             -1
exploration/num steps total        18000
exploration/num paths total          707
exploration/path length Mean          83.3333
exploration/path length Std           21.5767
exploration/path length Max          127
exploration/path length Min           39
exploration/Rewards Mean               2.13512
exploration/Rewards Std                0.742876
exploration/Rewards Max                3.69035
exploration/Rewards Min                0.617378
exploration/Returns Mean             177.927
exploration/Returns Std               53.364
exploration/Returns Max              259.004
exploration/Returns Min               70.0586
exploration/Actions Mean               0.322491
exploration/Actions Std                0.652399
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                 12
exploration/Average Returns          177.927
evaluation/num steps total          7652
evaluation/num paths total           128
evaluation/path length Mean           85
evaluation/path length Std             0
evaluation/path length Max            85
evaluation/path length Min            85
evaluation/Rewards Mean                1.97627
evaluation/Rewards Std                 0.500291
evaluation/Rewards Max                 2.83857
evaluation/Rewards Min                 0.655813
evaluation/Returns Mean              167.983
evaluation/Returns Std                 0.720125
evaluation/Returns Max               169.059
evaluation/Returns Min               166.827
evaluation/Actions Mean                0.438075
evaluation/Actions Std                 0.717875
evaluation/Actions Max                 1
evaluation/Actions Min                -1
evaluation/Num Paths                  11
evaluation/Average Returns           167.983
time/data storing (s)                  0.00341024
time/evaluation sampling (s)           0.239258
time/exploration sampling (s)          0.255554
time/logging (s)                       0.00395978
time/saving (s)                        0.00201743
time/training (s)                      6.43411
time/epoch (s)                         6.93831
time/total (s)                        58.4929
Epoch                                  7
---------------------------------  --------------
2021-07-02 23:39:20.726863 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 8 finished
---------------------------------  ---------------
replay_buffer/size                 19000
trainer/QF Loss                       74.6815
trainer/Policy Loss                 -212.676
trainer/Raw Policy Loss             -212.676
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean           198.145
trainer/Q Predictions Std             81.0338
trainer/Q Predictions Max            386.972
trainer/Q Predictions Min             -5.23553
trainer/Q Targets Mean               200.734
trainer/Q Targets Std                 83.4267
trainer/Q Targets Max                394.397
trainer/Q Targets Min                 -7.1126
trainer/Bellman Errors Mean           74.6815
trainer/Bellman Errors Std           161.03
trainer/Bellman Errors Max          1007.64
trainer/Bellman Errors Min             1.65755e-08
trainer/Policy Action Mean             0.304121
trainer/Policy Action Std              0.901025
trainer/Policy Action Max              1
trainer/Policy Action Min             -1
exploration/num steps total        19000
exploration/num paths total          719
exploration/path length Mean          83.3333
exploration/path length Std           11.7568
exploration/path length Max           90
exploration/path length Min           46
exploration/Rewards Mean               2.16802
exploration/Rewards Std                0.716869
exploration/Rewards Max                3.85349
exploration/Rewards Min                0.630837
exploration/Returns Mean             180.668
exploration/Returns Std               32.9215
exploration/Returns Max              208
exploration/Returns Min               77.8863
exploration/Actions Mean               0.364824
exploration/Actions Std                0.612755
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                 12
exploration/Average Returns          180.668
evaluation/num steps total          8615
evaluation/num paths total           139
evaluation/path length Mean           87.5455
evaluation/path length Std             0.49793
evaluation/path length Max            88
evaluation/path length Min            87
evaluation/Rewards Mean                2.19223
evaluation/Rewards Std                 0.748206
evaluation/Rewards Max                 3.49484
evaluation/Rewards Min                 0.652035
evaluation/Returns Mean              191.92
evaluation/Returns Std                 2.42857
evaluation/Returns Max               194.545
evaluation/Returns Min               188.059
evaluation/Actions Mean                0.473786
evaluation/Actions Std                 0.674384
evaluation/Actions Max                 1
evaluation/Actions Min                -0.999967
evaluation/Num Paths                  11
evaluation/Average Returns           191.92
time/data storing (s)                  0.00476141
time/evaluation sampling (s)           0.293733
time/exploration sampling (s)          0.287425
time/logging (s)                       0.0038742
time/saving (s)                        0.00126256
time/training (s)                      6.27158
time/epoch (s)                         6.86264
time/total (s)                        65.3572
Epoch                                  8
---------------------------------  ---------------
2021-07-02 23:39:27.956511 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 9 finished
---------------------------------  ---------------
replay_buffer/size                 20000
trainer/QF Loss                       92.538
trainer/Policy Loss                 -243.265
trainer/Raw Policy Loss             -243.265
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean           225.136
trainer/Q Predictions Std             95.0284
trainer/Q Predictions Max            333.61
trainer/Q Predictions Min             -7.54868
trainer/Q Targets Mean               226.414
trainer/Q Targets Std                 97.5232
trainer/Q Targets Max                335.633
trainer/Q Targets Min                 -8.00854
trainer/Bellman Errors Mean           92.538
trainer/Bellman Errors Std           335.673
trainer/Bellman Errors Max          3230.4
trainer/Bellman Errors Min             1.67228e-05
trainer/Policy Action Mean             0.345883
trainer/Policy Action Std              0.880322
trainer/Policy Action Max              1
trainer/Policy Action Min             -1
exploration/num steps total        20000
exploration/num paths total          731
exploration/path length Mean          83.3333
exploration/path length Std           18.4225
exploration/path length Max           97
exploration/path length Min           26
exploration/Rewards Mean               2.19643
exploration/Rewards Std                0.737009
exploration/Rewards Max                4.03931
exploration/Rewards Min                0.646516
exploration/Returns Mean             183.036
exploration/Returns Std               50.2929
exploration/Returns Max              232.325
exploration/Returns Min               33.5246
exploration/Actions Mean               0.284874
exploration/Actions Std                0.641238
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                 12
exploration/Average Returns          183.036
evaluation/num steps total          9538
evaluation/num paths total           149
evaluation/path length Mean           92.3
evaluation/path length Std             0.9
evaluation/path length Max            93
evaluation/path length Min            90
evaluation/Rewards Mean                2.19716
evaluation/Rewards Std                 0.6657
evaluation/Rewards Max                 3.84302
evaluation/Rewards Min                 0.672917
evaluation/Returns Mean              202.798
evaluation/Returns Std                 2.2186
evaluation/Returns Max               206.171
evaluation/Returns Min               198.454
evaluation/Actions Mean                0.34835
evaluation/Actions Std                 0.653988
evaluation/Actions Max                 1
evaluation/Actions Min                -0.999924
evaluation/Num Paths                  10
evaluation/Average Returns           202.798
time/data storing (s)                  0.00337473
time/evaluation sampling (s)           0.253153
time/exploration sampling (s)          0.260126
time/logging (s)                       0.00436229
time/saving (s)                        0.00169994
time/training (s)                      6.70529
time/epoch (s)                         7.228
time/total (s)                        72.5871
Epoch                                  9
---------------------------------  ---------------
2021-07-02 23:39:34.860222 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 10 finished
---------------------------------  ---------------
replay_buffer/size                 21000
trainer/QF Loss                      206.207
trainer/Policy Loss                 -269.191
trainer/Raw Policy Loss             -269.191
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean           256.869
trainer/Q Predictions Std             93.5078
trainer/Q Predictions Max            358.239
trainer/Q Predictions Min             -6.08504
trainer/Q Targets Mean               258.606
trainer/Q Targets Std                 93.952
trainer/Q Targets Max                364.517
trainer/Q Targets Min                 -0.713435
trainer/Bellman Errors Mean          206.207
trainer/Bellman Errors Std          1279.41
trainer/Bellman Errors Max         14227.5
trainer/Bellman Errors Min             0.000181125
trainer/Policy Action Mean             0.30892
trainer/Policy Action Std              0.902778
trainer/Policy Action Max              1
trainer/Policy Action Min             -1
exploration/num steps total        21000
exploration/num paths total          744
exploration/path length Mean          76.9231
exploration/path length Std           17.6001
exploration/path length Max           93
exploration/path length Min           35
exploration/Rewards Mean               2.04502
exploration/Rewards Std                0.605694
exploration/Rewards Max                3.43714
exploration/Rewards Min                0.600309
exploration/Returns Mean             157.31
exploration/Returns Std               44.2597
exploration/Returns Max              196.871
exploration/Returns Min               60.281
exploration/Actions Mean               0.410161
exploration/Actions Std                0.613446
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                 13
exploration/Average Returns          157.31
evaluation/num steps total         10484
evaluation/num paths total           160
evaluation/path length Mean           86
evaluation/path length Std             0.603023
evaluation/path length Max            87
evaluation/path length Min            85
evaluation/Rewards Mean                2.07265
evaluation/Rewards Std                 0.577171
evaluation/Rewards Max                 3.19569
evaluation/Rewards Min                 0.652275
evaluation/Returns Mean              178.248
evaluation/Returns Std                 1.24207
evaluation/Returns Max               180.639
evaluation/Returns Min               176.622
evaluation/Actions Mean                0.593821
evaluation/Actions Std                 0.587285
evaluation/Actions Max                 1
evaluation/Actions Min                -0.999492
evaluation/Num Paths                  11
evaluation/Average Returns           178.248
time/data storing (s)                  0.00338328
time/evaluation sampling (s)           0.260194
time/exploration sampling (s)          0.255664
time/logging (s)                       0.00400739
time/saving (s)                        0.00133831
time/training (s)                      6.37601
time/epoch (s)                         6.90059
time/total (s)                        79.4898
Epoch                                 10
---------------------------------  ---------------
2021-07-02 23:39:41.695434 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 11 finished
---------------------------------  --------------
replay_buffer/size                 22000
trainer/QF Loss                      104.142
trainer/Policy Loss                 -262.294
trainer/Raw Policy Loss             -262.294
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean           247.671
trainer/Q Predictions Std            121.533
trainer/Q Predictions Max            388.959
trainer/Q Predictions Min            -11.0294
trainer/Q Targets Mean               250.309
trainer/Q Targets Std                121.435
trainer/Q Targets Max                390.374
trainer/Q Targets Min                -12.8601
trainer/Bellman Errors Mean          104.142
trainer/Bellman Errors Std           297.228
trainer/Bellman Errors Max          1908.67
trainer/Bellman Errors Min             0.00712339
trainer/Policy Action Mean             0.42777
trainer/Policy Action Std              0.840936
trainer/Policy Action Max              1
trainer/Policy Action Min             -1
exploration/num steps total        22000
exploration/num paths total          758
exploration/path length Mean          71.4286
exploration/path length Std           27.6734
exploration/path length Max          100
exploration/path length Min            9
exploration/Rewards Mean               1.93834
exploration/Rewards Std                0.694251
exploration/Rewards Max                4.28992
exploration/Rewards Min                0.555381
exploration/Returns Mean             138.453
exploration/Returns Std               67.2839
exploration/Returns Max              205.933
exploration/Returns Min                7.58085
exploration/Actions Mean               0.323127
exploration/Actions Std                0.688514
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                 14
exploration/Average Returns          138.453
evaluation/num steps total         11427
evaluation/num paths total           170
evaluation/path length Mean           94.3
evaluation/path length Std             1.00499
evaluation/path length Max            96
evaluation/path length Min            93
evaluation/Rewards Mean                2.1886
evaluation/Rewards Std                 0.76667
evaluation/Rewards Max                 4.57197
evaluation/Rewards Min                 0.640367
evaluation/Returns Mean              206.385
evaluation/Returns Std                 6.723
evaluation/Returns Max               216.798
evaluation/Returns Min               197.121
evaluation/Actions Mean                0.435607
evaluation/Actions Std                 0.741145
evaluation/Actions Max                 1
evaluation/Actions Min                -1
evaluation/Num Paths                  10
evaluation/Average Returns           206.385
time/data storing (s)                  0.00340906
time/evaluation sampling (s)           0.276492
time/exploration sampling (s)          0.255089
time/logging (s)                       0.00527309
time/saving (s)                        0.00161006
time/training (s)                      6.29255
time/epoch (s)                         6.83443
time/total (s)                        86.326
Epoch                                 11
---------------------------------  --------------
2021-07-02 23:39:48.562552 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 12 finished
---------------------------------  --------------
replay_buffer/size                 23000
trainer/QF Loss                       79.3589
trainer/Policy Loss                 -261.877
trainer/Raw Policy Loss             -261.877
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean           252.246
trainer/Q Predictions Std            120.325
trainer/Q Predictions Max            405.768
trainer/Q Predictions Min            -19.1828
trainer/Q Targets Mean               248.191
trainer/Q Targets Std                120.415
trainer/Q Targets Max                385.481
trainer/Q Targets Min                -26.233
trainer/Bellman Errors Mean           79.3589
trainer/Bellman Errors Std           151.34
trainer/Bellman Errors Max          1295.79
trainer/Bellman Errors Min             0.00102678
trainer/Policy Action Mean             0.358205
trainer/Policy Action Std              0.86489
trainer/Policy Action Max              1
trainer/Policy Action Min             -1
exploration/num steps total        23000
exploration/num paths total          769
exploration/path length Mean          90.9091
exploration/path length Std           22.7734
exploration/path length Max          110
exploration/path length Min           22
exploration/Rewards Mean               2.0475
exploration/Rewards Std                0.621216
exploration/Rewards Max                3.8855
exploration/Rewards Min                0.622335
exploration/Returns Mean             186.137
exploration/Returns Std               53.3408
exploration/Returns Max              231.635
exploration/Returns Min               27.0123
exploration/Actions Mean               0.264911
exploration/Actions Std                0.647565
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                 11
exploration/Average Returns          186.137
evaluation/num steps total         12410
evaluation/num paths total           180
evaluation/path length Mean           98.3
evaluation/path length Std             0.458258
evaluation/path length Max            99
evaluation/path length Min            98
evaluation/Rewards Mean                2.09997
evaluation/Rewards Std                 0.562364
evaluation/Rewards Max                 3.08954
evaluation/Rewards Min                 0.719385
evaluation/Returns Mean              206.427
evaluation/Returns Std                 1.08053
evaluation/Returns Max               208.249
evaluation/Returns Min               205.195
evaluation/Actions Mean                0.351297
evaluation/Actions Std                 0.709047
evaluation/Actions Max                 1
evaluation/Actions Min                -0.999988
evaluation/Num Paths                  10
evaluation/Average Returns           206.427
time/data storing (s)                  0.00340156
time/evaluation sampling (s)           0.284517
time/exploration sampling (s)          0.254647
time/logging (s)                       0.00395959
time/saving (s)                        0.00128384
time/training (s)                      6.31546
time/epoch (s)                         6.86327
time/total (s)                        93.1913
Epoch                                 12
---------------------------------  --------------
2021-07-02 23:39:55.509331 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 13 finished
---------------------------------  --------------
replay_buffer/size                 24000
trainer/QF Loss                      101.852
trainer/Policy Loss                 -261.442
trainer/Raw Policy Loss             -261.442
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean           248.671
trainer/Q Predictions Std            122.066
trainer/Q Predictions Max            395.398
trainer/Q Predictions Min            -10.3758
trainer/Q Targets Mean               249.823
trainer/Q Targets Std                123.75
trainer/Q Targets Max                394.929
trainer/Q Targets Min                -13.7275
trainer/Bellman Errors Mean          101.852
trainer/Bellman Errors Std           271.902
trainer/Bellman Errors Max          2233.64
trainer/Bellman Errors Min             0.00101724
trainer/Policy Action Mean             0.353576
trainer/Policy Action Std              0.85868
trainer/Policy Action Max              1
trainer/Policy Action Min             -1
exploration/num steps total        24000
exploration/num paths total          781
exploration/path length Mean          83.3333
exploration/path length Std           26.4837
exploration/path length Max          106
exploration/path length Min            9
exploration/Rewards Mean               1.98691
exploration/Rewards Std                0.550754
exploration/Rewards Max                3.90722
exploration/Rewards Min                0.804232
exploration/Returns Mean             165.576
exploration/Returns Std               59.4452
exploration/Returns Max              217.545
exploration/Returns Min                8.41062
exploration/Actions Mean               0.278662
exploration/Actions Std                0.663979
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                 12
exploration/Average Returns          165.576
evaluation/num steps total         13398
evaluation/num paths total           190
evaluation/path length Mean           98.8
evaluation/path length Std             0.6
evaluation/path length Max           100
evaluation/path length Min            98
evaluation/Rewards Mean                2.03651
evaluation/Rewards Std                 0.510208
evaluation/Rewards Max                 3.42278
evaluation/Rewards Min                 0.869595
evaluation/Returns Mean              201.208
evaluation/Returns Std                 1.54771
evaluation/Returns Max               203.483
evaluation/Returns Min               198.247
evaluation/Actions Mean                0.340455
evaluation/Actions Std                 0.751494
evaluation/Actions Max                 1
evaluation/Actions Min                -1
evaluation/Num Paths                  10
evaluation/Average Returns           201.208
time/data storing (s)                  0.00340437
time/evaluation sampling (s)           0.323539
time/exploration sampling (s)          0.258249
time/logging (s)                       0.00391521
time/saving (s)                        0.00125875
time/training (s)                      6.35415
time/epoch (s)                         6.94451
time/total (s)                       100.138
Epoch                                 13
---------------------------------  --------------
2021-07-02 23:40:02.659502 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 14 finished
---------------------------------  --------------
replay_buffer/size                 25000
trainer/QF Loss                       74.3074
trainer/Policy Loss                 -260.207
trainer/Raw Policy Loss             -260.207
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean           245.129
trainer/Q Predictions Std            116.162
trainer/Q Predictions Max            410.451
trainer/Q Predictions Min             -9.26147
trainer/Q Targets Mean               244.984
trainer/Q Targets Std                117.746
trainer/Q Targets Max                399.863
trainer/Q Targets Min                 -4.80229
trainer/Bellman Errors Mean           74.3074
trainer/Bellman Errors Std           225.143
trainer/Bellman Errors Max          2243.87
trainer/Bellman Errors Min             0.00849825
trainer/Policy Action Mean             0.00847852
trainer/Policy Action Std              0.910896
trainer/Policy Action Max              1
trainer/Policy Action Min             -1
exploration/num steps total        25000
exploration/num paths total          793
exploration/path length Mean          83.3333
exploration/path length Std           47.3697
exploration/path length Max          181
exploration/path length Min           36
exploration/Rewards Mean               0.642455
exploration/Rewards Std                0.357812
exploration/Rewards Max                1.74728
exploration/Rewards Min               -0.892968
exploration/Returns Mean              53.5379
exploration/Returns Std               44.1767
exploration/Returns Max              153.124
exploration/Returns Min                8.69224
exploration/Actions Mean               0.392822
exploration/Actions Std                0.568185
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                 12
exploration/Average Returns           53.5379
evaluation/num steps total         14282
evaluation/num paths total           197
evaluation/path length Mean          126.286
evaluation/path length Std            25.9048
evaluation/path length Max           147
evaluation/path length Min            84
evaluation/Rewards Mean                0.755013
evaluation/Rewards Std                 0.261168
evaluation/Rewards Max                 1.18721
evaluation/Rewards Min                 0.115017
evaluation/Returns Mean               95.3474
evaluation/Returns Std                26.7117
evaluation/Returns Max               116.475
evaluation/Returns Min                52.3518
evaluation/Actions Mean                0.597187
evaluation/Actions Std                 0.507763
evaluation/Actions Max                 1
evaluation/Actions Min                -0.994257
evaluation/Num Paths                   7
evaluation/Average Returns            95.3474
time/data storing (s)                  0.0033601
time/evaluation sampling (s)           0.27885
time/exploration sampling (s)          0.264053
time/logging (s)                       0.00431345
time/saving (s)                        0.00150997
time/training (s)                      6.59628
time/epoch (s)                         7.14837
time/total (s)                       107.288
Epoch                                 14
---------------------------------  --------------
2021-07-02 23:40:09.551454 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 15 finished
---------------------------------  ---------------
replay_buffer/size                 26000
trainer/QF Loss                      181.141
trainer/Policy Loss                 -332.534
trainer/Raw Policy Loss             -332.534
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean           313.018
trainer/Q Predictions Std            137.742
trainer/Q Predictions Max            518.39
trainer/Q Predictions Min             -2.54182
trainer/Q Targets Mean               316.227
trainer/Q Targets Std                141.579
trainer/Q Targets Max                548.913
trainer/Q Targets Min                 -5.20592
trainer/Bellman Errors Mean          181.141
trainer/Bellman Errors Std           520.264
trainer/Bellman Errors Max          4473.55
trainer/Bellman Errors Min             0.000108931
trainer/Policy Action Mean             0.259042
trainer/Policy Action Std              0.907735
trainer/Policy Action Max              1
trainer/Policy Action Min             -1
exploration/num steps total        26000
exploration/num paths total          799
exploration/path length Mean         166.667
exploration/path length Std           43.9798
exploration/path length Max          232
exploration/path length Min          122
exploration/Rewards Mean               1.50707
exploration/Rewards Std                0.534116
exploration/Rewards Max                3.33805
exploration/Rewards Min                0.280171
exploration/Returns Mean             251.178
exploration/Returns Std              109.707
exploration/Returns Max              424.284
exploration/Returns Min              160.283
exploration/Actions Mean               0.042946
exploration/Actions Std                0.683026
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                  6
exploration/Average Returns          251.178
evaluation/num steps total         15282
evaluation/num paths total           198
evaluation/path length Mean         1000
evaluation/path length Std             0
evaluation/path length Max          1000
evaluation/path length Min          1000
evaluation/Rewards Mean                1.08424
evaluation/Rewards Std                 0.143494
evaluation/Rewards Max                 1.7135
evaluation/Rewards Min                 0.724816
evaluation/Returns Mean             1084.24
evaluation/Returns Std                 0
evaluation/Returns Max              1084.24
evaluation/Returns Min              1084.24
evaluation/Actions Mean                0.0863874
evaluation/Actions Std                 0.762522
evaluation/Actions Max                 0.99999
evaluation/Actions Min                -0.999973
evaluation/Num Paths                   1
evaluation/Average Returns          1084.24
time/data storing (s)                  0.00342835
time/evaluation sampling (s)           0.302708
time/exploration sampling (s)          0.258537
time/logging (s)                       0.00544704
time/saving (s)                        0.00202883
time/training (s)                      6.31899
time/epoch (s)                         6.89114
time/total (s)                       114.181
Epoch                                 15
---------------------------------  ---------------
2021-07-02 23:40:16.583540 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 16 finished
---------------------------------  --------------
replay_buffer/size                 27000
trainer/QF Loss                      195.918
trainer/Policy Loss                 -420.998
trainer/Raw Policy Loss             -420.998
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean           397.827
trainer/Q Predictions Std            203.302
trainer/Q Predictions Max            766.841
trainer/Q Predictions Min             -3.08285
trainer/Q Targets Mean               399.716
trainer/Q Targets Std                204.825
trainer/Q Targets Max                773.365
trainer/Q Targets Min                 -4.56889
trainer/Bellman Errors Mean          195.918
trainer/Bellman Errors Std           656.125
trainer/Bellman Errors Max          6130.13
trainer/Bellman Errors Min             0.00303764
trainer/Policy Action Mean             0.418224
trainer/Policy Action Std              0.847735
trainer/Policy Action Max              1
trainer/Policy Action Min             -1
exploration/num steps total        27000
exploration/num paths total          816
exploration/path length Mean          58.8235
exploration/path length Std           16.3536
exploration/path length Max          105
exploration/path length Min           26
exploration/Rewards Mean               1.86804
exploration/Rewards Std                0.538086
exploration/Rewards Max                3.62925
exploration/Rewards Min                0.811346
exploration/Returns Mean             109.885
exploration/Returns Std               40.7568
exploration/Returns Max              239.817
exploration/Returns Min               42.8354
exploration/Actions Mean               0.171794
exploration/Actions Std                0.755332
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                 17
exploration/Average Returns          109.885
evaluation/num steps total         16256
evaluation/num paths total           219
evaluation/path length Mean           46.381
evaluation/path length Std             0.575383
evaluation/path length Max            47
evaluation/path length Min            45
evaluation/Rewards Mean                1.81839
evaluation/Rewards Std                 0.510454
evaluation/Rewards Max                 2.83453
evaluation/Rewards Min                 0.904701
evaluation/Returns Mean               84.3386
evaluation/Returns Std                 1.6412
evaluation/Returns Max                86.9721
evaluation/Returns Min                79.3969
evaluation/Actions Mean                0.266449
evaluation/Actions Std                 0.859563
evaluation/Actions Max                 1
evaluation/Actions Min                -1
evaluation/Num Paths                  21
evaluation/Average Returns            84.3386
time/data storing (s)                  0.00341939
time/evaluation sampling (s)           0.246972
time/exploration sampling (s)          0.259266
time/logging (s)                       0.00455018
time/saving (s)                        0.00137403
time/training (s)                      6.51366
time/epoch (s)                         7.02924
time/total (s)                       121.211
Epoch                                 16
---------------------------------  --------------
2021-07-02 23:40:23.337342 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 17 finished
---------------------------------  ---------------
replay_buffer/size                 28000
trainer/QF Loss                      480.695
trainer/Policy Loss                 -564.917
trainer/Raw Policy Loss             -564.917
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean           533.823
trainer/Q Predictions Std            228.212
trainer/Q Predictions Max           1049.97
trainer/Q Predictions Min              0.117025
trainer/Q Targets Mean               539.006
trainer/Q Targets Std                230.116
trainer/Q Targets Max               1033.84
trainer/Q Targets Min                  0.927504
trainer/Bellman Errors Mean          480.695
trainer/Bellman Errors Std          1684.32
trainer/Bellman Errors Max         13551
trainer/Bellman Errors Min             0.000641588
trainer/Policy Action Mean             0.072661
trainer/Policy Action Std              0.906553
trainer/Policy Action Max              1
trainer/Policy Action Min             -1
exploration/num steps total        28000
exploration/num paths total          822
exploration/path length Mean         166.667
exploration/path length Std           82.334
exploration/path length Max          282
exploration/path length Min           49
exploration/Rewards Mean               2.10012
exploration/Rewards Std                0.705047
exploration/Rewards Max                3.65595
exploration/Rewards Min                0.601971
exploration/Returns Mean             350.02
exploration/Returns Std              218.524
exploration/Returns Max              717.084
exploration/Returns Min               72.1466
exploration/Actions Mean               0.00326183
exploration/Actions Std                0.685689
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                  6
exploration/Average Returns          350.02
evaluation/num steps total         17173
evaluation/num paths total           224
evaluation/path length Mean          183.4
evaluation/path length Std            13.8651
evaluation/path length Max           206
evaluation/path length Min           166
evaluation/Rewards Mean                1.98243
evaluation/Rewards Std                 0.600164
evaluation/Rewards Max                 3.43796
evaluation/Rewards Min                 0.106909
evaluation/Returns Mean              363.577
evaluation/Returns Std                35.6521
evaluation/Returns Max               423.215
evaluation/Returns Min               324.039
evaluation/Actions Mean                0.163567
evaluation/Actions Std                 0.833641
evaluation/Actions Max                 1
evaluation/Actions Min                -1
evaluation/Num Paths                   5
evaluation/Average Returns           363.577
time/data storing (s)                  0.00334278
time/evaluation sampling (s)           0.286828
time/exploration sampling (s)          0.247696
time/logging (s)                       0.00383603
time/saving (s)                        0.00127458
time/training (s)                      6.2078
time/epoch (s)                         6.75078
time/total (s)                       127.964
Epoch                                 17
---------------------------------  ---------------
2021-07-02 23:40:30.316469 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 18 finished
---------------------------------  --------------
replay_buffer/size                 29000
trainer/QF Loss                     1610.32
trainer/Policy Loss                 -679.863
trainer/Raw Policy Loss             -679.863
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean           642.21
trainer/Q Predictions Std            265.317
trainer/Q Predictions Max            984.416
trainer/Q Predictions Min            -23.5905
trainer/Q Targets Mean               630.243
trainer/Q Targets Std                268.234
trainer/Q Targets Max                933.744
trainer/Q Targets Min                 -0.745078
trainer/Bellman Errors Mean         1610.32
trainer/Bellman Errors Std          9496.35
trainer/Bellman Errors Max         90641.3
trainer/Bellman Errors Min             0.00102288
trainer/Policy Action Mean             0.145125
trainer/Policy Action Std              0.882308
trainer/Policy Action Max              1
trainer/Policy Action Min             -1
exploration/num steps total        29000
exploration/num paths total          830
exploration/path length Mean         125
exploration/path length Std           50.2469
exploration/path length Max          167
exploration/path length Min           36
exploration/Rewards Mean               2.03003
exploration/Rewards Std                0.802206
exploration/Rewards Max                4.06601
exploration/Rewards Min                0.553006
exploration/Returns Mean             253.754
exploration/Returns Std              133.735
exploration/Returns Max              430.21
exploration/Returns Min               62.1492
exploration/Actions Mean               0.0636835
exploration/Actions Std                0.672773
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                  8
exploration/Average Returns          253.754
evaluation/num steps total         17934
evaluation/num paths total           227
evaluation/path length Mean          253.667
evaluation/path length Std            58.38
evaluation/path length Max           325
evaluation/path length Min           182
evaluation/Rewards Mean                2.47211
evaluation/Rewards Std                 0.832644
evaluation/Rewards Max                 4.88734
evaluation/Rewards Min                 0.428901
evaluation/Returns Mean              627.092
evaluation/Returns Std               187.474
evaluation/Returns Max               775.386
evaluation/Returns Min               362.612
evaluation/Actions Mean                0.0257849
evaluation/Actions Std                 0.808312
evaluation/Actions Max                 1
evaluation/Actions Min                -1
evaluation/Num Paths                   3
evaluation/Average Returns           627.092
time/data storing (s)                  0.00338772
time/evaluation sampling (s)           0.332225
time/exploration sampling (s)          0.261949
time/logging (s)                       0.00395489
time/saving (s)                        0.0015308
time/training (s)                      6.37407
time/epoch (s)                         6.97712
time/total (s)                       134.943
Epoch                                 18
---------------------------------  --------------
2021-07-02 23:40:37.242983 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 19 finished
---------------------------------  ---------------
replay_buffer/size                  30000
trainer/QF Loss                      3345.21
trainer/Policy Loss                  -729.089
trainer/Raw Policy Loss              -729.089
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean            700.899
trainer/Q Predictions Std             264.078
trainer/Q Predictions Max            1079.29
trainer/Q Predictions Min              11.4552
trainer/Q Targets Mean                704.699
trainer/Q Targets Std                 272.269
trainer/Q Targets Max                1090.55
trainer/Q Targets Min                   1.23156
trainer/Bellman Errors Mean          3345.21
trainer/Bellman Errors Std          30271.1
trainer/Bellman Errors Max         343770
trainer/Bellman Errors Min              3.8147e-06
trainer/Policy Action Mean             -0.0715165
trainer/Policy Action Std               0.914783
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         30000
exploration/num paths total           839
exploration/path length Mean          111.111
exploration/path length Std            75.3473
exploration/path length Max           246
exploration/path length Min            21
exploration/Rewards Mean                1.49817
exploration/Rewards Std                 0.5416
exploration/Rewards Max                 3.49934
exploration/Rewards Min                 0.358961
exploration/Returns Mean              166.463
exploration/Returns Std               111.947
exploration/Returns Max               373.818
exploration/Returns Min                27.9896
exploration/Actions Mean                0.096793
exploration/Actions Std                 0.704841
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                   9
exploration/Average Returns           166.463
evaluation/num steps total          18795
evaluation/num paths total            229
evaluation/path length Mean           430.5
evaluation/path length Std            147.5
evaluation/path length Max            578
evaluation/path length Min            283
evaluation/Rewards Mean                 1.49495
evaluation/Rewards Std                  0.493033
evaluation/Rewards Max                  2.95429
evaluation/Rewards Min                  0.500796
evaluation/Returns Mean               643.577
evaluation/Returns Std                280.408
evaluation/Returns Max                923.984
evaluation/Returns Min                363.169
evaluation/Actions Mean                 0.196026
evaluation/Actions Std                  0.777673
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    2
evaluation/Average Returns            643.577
time/data storing (s)                   0.00337349
time/evaluation sampling (s)            0.250816
time/exploration sampling (s)           0.250095
time/logging (s)                        0.00608684
time/saving (s)                         0.00183592
time/training (s)                       6.41398
time/epoch (s)                          6.92618
time/total (s)                        141.871
Epoch                                  19
---------------------------------  ---------------
2021-07-02 23:40:44.330252 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 20 finished
---------------------------------  ---------------
replay_buffer/size                 31000
trainer/QF Loss                      358.526
trainer/Policy Loss                 -759.567
trainer/Raw Policy Loss             -759.567
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean           734.835
trainer/Q Predictions Std            268.858
trainer/Q Predictions Max           1093.54
trainer/Q Predictions Min            -23.2839
trainer/Q Targets Mean               743.336
trainer/Q Targets Std                268.792
trainer/Q Targets Max               1104.39
trainer/Q Targets Min                 -0.413885
trainer/Bellman Errors Mean          358.526
trainer/Bellman Errors Std           607.868
trainer/Bellman Errors Max          3247.78
trainer/Bellman Errors Min             0.000225898
trainer/Policy Action Mean            -0.0896316
trainer/Policy Action Std              0.911639
trainer/Policy Action Max              1
trainer/Policy Action Min             -1
exploration/num steps total        31000
exploration/num paths total          849
exploration/path length Mean         100
exploration/path length Std          107.141
exploration/path length Max          311
exploration/path length Min           11
exploration/Rewards Mean               1.45156
exploration/Rewards Std                0.561494
exploration/Rewards Max                3.62515
exploration/Rewards Min                0.178333
exploration/Returns Mean             145.156
exploration/Returns Std              154.476
exploration/Returns Max              427.247
exploration/Returns Min               12.5081
exploration/Actions Mean               0.176362
exploration/Actions Std                0.717668
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                 10
exploration/Average Returns          145.156
evaluation/num steps total         19788
evaluation/num paths total           257
evaluation/path length Mean           35.4643
evaluation/path length Std             0.498723
evaluation/path length Max            36
evaluation/path length Min            35
evaluation/Rewards Mean                1.29208
evaluation/Rewards Std                 0.362722
evaluation/Rewards Max                 1.93582
evaluation/Rewards Min                 0.665997
evaluation/Returns Mean               45.8228
evaluation/Returns Std                 1.04584
evaluation/Returns Max                47.3138
evaluation/Returns Min                44.5188
evaluation/Actions Mean                0.193666
evaluation/Actions Std                 0.861953
evaluation/Actions Max                 1
evaluation/Actions Min                -1
evaluation/Num Paths                  28
evaluation/Average Returns            45.8228
time/data storing (s)                  0.00336709
time/evaluation sampling (s)           0.327025
time/exploration sampling (s)          0.252065
time/logging (s)                       0.00418202
time/saving (s)                        0.00129428
time/training (s)                      6.49457
time/epoch (s)                         7.0825
time/total (s)                       148.956
Epoch                                 20
---------------------------------  ---------------
2021-07-02 23:40:51.074170 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 21 finished
---------------------------------  --------------
replay_buffer/size                 32000
trainer/QF Loss                      503.977
trainer/Policy Loss                 -850.376
trainer/Raw Policy Loss             -850.376
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean           821.051
trainer/Q Predictions Std            305.157
trainer/Q Predictions Max           1263.81
trainer/Q Predictions Min            -33.2423
trainer/Q Targets Mean               825.68
trainer/Q Targets Std                302.746
trainer/Q Targets Max               1271.62
trainer/Q Targets Min                 -0.384073
trainer/Bellman Errors Mean          503.977
trainer/Bellman Errors Std          1174.09
trainer/Bellman Errors Max          7919.06
trainer/Bellman Errors Min             0.0178017
trainer/Policy Action Mean             0.0698454
trainer/Policy Action Std              0.905956
trainer/Policy Action Max              1
trainer/Policy Action Min             -1
exploration/num steps total        32000
exploration/num paths total          855
exploration/path length Mean         166.667
exploration/path length Std          104.256
exploration/path length Max          313
exploration/path length Min           31
exploration/Rewards Mean               1.59771
exploration/Rewards Std                0.725832
exploration/Rewards Max                3.53983
exploration/Rewards Min                0.138818
exploration/Returns Mean             266.284
exploration/Returns Std              190.343
exploration/Returns Max              555.083
exploration/Returns Min               44.3149
exploration/Actions Mean               0.214735
exploration/Actions Std                0.737021
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                  6
exploration/Average Returns          266.284
evaluation/num steps total         20730
evaluation/num paths total           261
evaluation/path length Mean          235.5
evaluation/path length Std             9.31397
evaluation/path length Max           246
evaluation/path length Min           222
evaluation/Rewards Mean                1.82411
evaluation/Rewards Std                 0.620019
evaluation/Rewards Max                 3.34947
evaluation/Rewards Min                 0.866102
evaluation/Returns Mean              429.578
evaluation/Returns Std                44.6854
evaluation/Returns Max               479.752
evaluation/Returns Min               366.8
evaluation/Actions Mean                0.0637568
evaluation/Actions Std                 0.833511
evaluation/Actions Max                 1
evaluation/Actions Min                -1
evaluation/Num Paths                   4
evaluation/Average Returns           429.578
time/data storing (s)                  0.00340643
time/evaluation sampling (s)           0.246418
time/exploration sampling (s)          0.254757
time/logging (s)                       0.00388469
time/saving (s)                        0.00125391
time/training (s)                      6.23181
time/epoch (s)                         6.74153
time/total (s)                       155.699
Epoch                                 21
---------------------------------  --------------
2021-07-02 23:40:58.214427 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 22 finished
---------------------------------  --------------
replay_buffer/size                 33000
trainer/QF Loss                     1450.87
trainer/Policy Loss                 -942.96
trainer/Raw Policy Loss             -942.96
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean           892.077
trainer/Q Predictions Std            381.744
trainer/Q Predictions Max           1407.87
trainer/Q Predictions Min            -34.2616
trainer/Q Targets Mean               889.367
trainer/Q Targets Std                382.535
trainer/Q Targets Max               1407.41
trainer/Q Targets Min                -72.4179
trainer/Bellman Errors Mean         1450.87
trainer/Bellman Errors Std          4760.93
trainer/Bellman Errors Max         33241.9
trainer/Bellman Errors Min             0.0138108
trainer/Policy Action Mean             0.107065
trainer/Policy Action Std              0.916768
trainer/Policy Action Max              1
trainer/Policy Action Min             -1
exploration/num steps total        33000
exploration/num paths total          864
exploration/path length Mean         111.111
exploration/path length Std           52.0437
exploration/path length Max          219
exploration/path length Min           11
exploration/Rewards Mean               1.63285
exploration/Rewards Std                0.478676
exploration/Rewards Max                3.02176
exploration/Rewards Min                0.428662
exploration/Returns Mean             181.428
exploration/Returns Std              119.336
exploration/Returns Max              475.855
exploration/Returns Min               12.8301
exploration/Actions Mean               0.164869
exploration/Actions Std                0.709145
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                  9
exploration/Average Returns          181.428
evaluation/num steps total         21709
evaluation/num paths total           269
evaluation/path length Mean          122.375
evaluation/path length Std             2.54644
evaluation/path length Max           127
evaluation/path length Min           119
evaluation/Rewards Mean                1.62578
evaluation/Rewards Std                 0.288357
evaluation/Rewards Max                 2.3472
evaluation/Rewards Min                 0.88019
evaluation/Returns Mean              198.955
evaluation/Returns Std                13.4879
evaluation/Returns Max               224.588
evaluation/Returns Min               185.483
evaluation/Actions Mean                0.20007
evaluation/Actions Std                 0.825066
evaluation/Actions Max                 1
evaluation/Actions Min                -1
evaluation/Num Paths                   8
evaluation/Average Returns           198.955
time/data storing (s)                  0.00349377
time/evaluation sampling (s)           0.237642
time/exploration sampling (s)          0.258849
time/logging (s)                       0.00412028
time/saving (s)                        0.00125348
time/training (s)                      6.63304
time/epoch (s)                         7.13839
time/total (s)                       162.839
Epoch                                 22
---------------------------------  --------------
2021-07-02 23:41:05.109261 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 23 finished
---------------------------------  --------------
replay_buffer/size                 34000
trainer/QF Loss                     1390.28
trainer/Policy Loss                -1064.87
trainer/Raw Policy Loss            -1064.87
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean          1010.94
trainer/Q Predictions Std            367.812
trainer/Q Predictions Max           1505.21
trainer/Q Predictions Min           -205.995
trainer/Q Targets Mean              1008.55
trainer/Q Targets Std                365.1
trainer/Q Targets Max               1444.91
trainer/Q Targets Min               -211.558
trainer/Bellman Errors Mean         1390.28
trainer/Bellman Errors Std          2748.96
trainer/Bellman Errors Max         14787.3
trainer/Bellman Errors Min             0.224212
trainer/Policy Action Mean             0.238706
trainer/Policy Action Std              0.909397
trainer/Policy Action Max              1
trainer/Policy Action Min             -1
exploration/num steps total        34000
exploration/num paths total          870
exploration/path length Mean         166.667
exploration/path length Std           59.1599
exploration/path length Max          274
exploration/path length Min          103
exploration/Rewards Mean               1.75028
exploration/Rewards Std                0.500212
exploration/Rewards Max                3.86409
exploration/Rewards Min                0.844221
exploration/Returns Mean             291.713
exploration/Returns Std               78.1198
exploration/Returns Max              395.172
exploration/Returns Min              184.26
exploration/Actions Mean               0.173622
exploration/Actions Std                0.679806
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                  6
exploration/Average Returns          291.713
evaluation/num steps total         22632
evaluation/num paths total           275
evaluation/path length Mean          153.833
evaluation/path length Std            36.7034
evaluation/path length Max           235
evaluation/path length Min           131
evaluation/Rewards Mean                2.03962
evaluation/Rewards Std                 0.596121
evaluation/Rewards Max                 3.5356
evaluation/Rewards Min                 0.866205
evaluation/Returns Mean              313.762
evaluation/Returns Std               129.381
evaluation/Returns Max               593.992
evaluation/Returns Min               214.502
evaluation/Actions Mean                0.216157
evaluation/Actions Std                 0.808533
evaluation/Actions Max                 1
evaluation/Actions Min                -1
evaluation/Num Paths                   6
evaluation/Average Returns           313.762
time/data storing (s)                  0.00332562
time/evaluation sampling (s)           0.239191
time/exploration sampling (s)          0.25282
time/logging (s)                       0.00379875
time/saving (s)                        0.00135206
time/training (s)                      6.3919
time/epoch (s)                         6.89239
time/total (s)                       169.733
Epoch                                 23
---------------------------------  --------------
2021-07-02 23:41:12.214286 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 24 finished
---------------------------------  --------------
replay_buffer/size                 35000
trainer/QF Loss                     1304.83
trainer/Policy Loss                -1086.57
trainer/Raw Policy Loss            -1086.57
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean          1033.7
trainer/Q Predictions Std            433.657
trainer/Q Predictions Max           1604.1
trainer/Q Predictions Min            -42.5267
trainer/Q Targets Mean              1043.13
trainer/Q Targets Std                431.538
trainer/Q Targets Max               1591.4
trainer/Q Targets Min                -42.8564
trainer/Bellman Errors Mean         1304.83
trainer/Bellman Errors Std          3192.83
trainer/Bellman Errors Max         28749.8
trainer/Bellman Errors Min             0.503351
trainer/Policy Action Mean             0.23705
trainer/Policy Action Std              0.903502
trainer/Policy Action Max              1
trainer/Policy Action Min             -1
exploration/num steps total        35000
exploration/num paths total          875
exploration/path length Mean         200
exploration/path length Std           92.5916
exploration/path length Max          332
exploration/path length Min           95
exploration/Rewards Mean               1.7832
exploration/Rewards Std                0.596049
exploration/Rewards Max                3.52148
exploration/Rewards Min                0.437617
exploration/Returns Mean             356.64
exploration/Returns Std              185.58
exploration/Returns Max              629.95
exploration/Returns Min              140.108
exploration/Actions Mean               0.105789
exploration/Actions Std                0.652489
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                  5
exploration/Average Returns          356.64
evaluation/num steps total         23632
evaluation/num paths total           276
evaluation/path length Mean         1000
evaluation/path length Std             0
evaluation/path length Max          1000
evaluation/path length Min          1000
evaluation/Rewards Mean                1.90748
evaluation/Rewards Std                 0.444846
evaluation/Rewards Max                 2.95359
evaluation/Rewards Min                 0.625483
evaluation/Returns Mean             1907.48
evaluation/Returns Std                 0
evaluation/Returns Max              1907.48
evaluation/Returns Min              1907.48
evaluation/Actions Mean                0.229834
evaluation/Actions Std                 0.686802
evaluation/Actions Max                 1
evaluation/Actions Min                -1
evaluation/Num Paths                   1
evaluation/Average Returns          1907.48
time/data storing (s)                  0.00338579
time/evaluation sampling (s)           0.234443
time/exploration sampling (s)          0.261204
time/logging (s)                       0.00386472
time/saving (s)                        0.00130195
time/training (s)                      6.5988
time/epoch (s)                         7.103
time/total (s)                       176.838
Epoch                                 24
---------------------------------  --------------
2021-07-02 23:41:19.247700 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 25 finished
---------------------------------  ---------------
replay_buffer/size                 36000
trainer/QF Loss                     2367.11
trainer/Policy Loss                -1119.74
trainer/Raw Policy Loss            -1119.74
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean          1066.49
trainer/Q Predictions Std            489.345
trainer/Q Predictions Max           1724.62
trainer/Q Predictions Min            -89.4825
trainer/Q Targets Mean              1060.82
trainer/Q Targets Std                488.393
trainer/Q Targets Max               1677.65
trainer/Q Targets Min                -39.8759
trainer/Bellman Errors Mean         2367.11
trainer/Bellman Errors Std          8155.99
trainer/Bellman Errors Max         74614.7
trainer/Bellman Errors Min             0.000504494
trainer/Policy Action Mean             0.0846864
trainer/Policy Action Std              0.934473
trainer/Policy Action Max              1
trainer/Policy Action Min             -1
exploration/num steps total        36000
exploration/num paths total          881
exploration/path length Mean         166.667
exploration/path length Std           94.9678
exploration/path length Max          282
exploration/path length Min           23
exploration/Rewards Mean               1.96986
exploration/Rewards Std                0.628773
exploration/Rewards Max                3.437
exploration/Rewards Min                0.756207
exploration/Returns Mean             328.31
exploration/Returns Std              196.782
exploration/Returns Max              564.25
exploration/Returns Min               27.3622
exploration/Actions Mean               0.0725977
exploration/Actions Std                0.659811
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                  6
exploration/Average Returns          328.31
evaluation/num steps total         24473
evaluation/num paths total           279
evaluation/path length Mean          280.333
evaluation/path length Std            39.8358
evaluation/path length Max           309
evaluation/path length Min           224
evaluation/Rewards Mean                2.13188
evaluation/Rewards Std                 0.818727
evaluation/Rewards Max                 4.0793
evaluation/Rewards Min                 0.867107
evaluation/Returns Mean              597.636
evaluation/Returns Std                73.2262
evaluation/Returns Max               650.039
evaluation/Returns Min               494.081
evaluation/Actions Mean                0.0516581
evaluation/Actions Std                 0.757112
evaluation/Actions Max                 1
evaluation/Actions Min                -1
evaluation/Num Paths                   3
evaluation/Average Returns           597.636
time/data storing (s)                  0.00342393
time/evaluation sampling (s)           0.24726
time/exploration sampling (s)          0.258878
time/logging (s)                       0.00480576
time/saving (s)                        0.00127217
time/training (s)                      6.51681
time/epoch (s)                         7.03245
time/total (s)                       183.872
Epoch                                 25
---------------------------------  ---------------
2021-07-02 23:41:26.452663 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 26 finished
---------------------------------  ----------------
replay_buffer/size                  37000
trainer/QF Loss                     13248.5
trainer/Policy Loss                 -1250.58
trainer/Raw Policy Loss             -1250.58
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           1210.14
trainer/Q Predictions Std             453.482
trainer/Q Predictions Max            1650.37
trainer/Q Predictions Min             -52.4085
trainer/Q Targets Mean               1208.03
trainer/Q Targets Std                 461.982
trainer/Q Targets Max                1662.4
trainer/Q Targets Min                -193.645
trainer/Bellman Errors Mean         13248.5
trainer/Bellman Errors Std         105755
trainer/Bellman Errors Max              1.18779e+06
trainer/Bellman Errors Min              0.00536889
trainer/Policy Action Mean              0.0681013
trainer/Policy Action Std               0.930212
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         37000
exploration/num paths total           888
exploration/path length Mean          142.857
exploration/path length Std           131.293
exploration/path length Max           432
exploration/path length Min            10
exploration/Rewards Mean                1.68548
exploration/Rewards Std                 0.631905
exploration/Rewards Max                 3.97439
exploration/Rewards Min                 0.572893
exploration/Returns Mean              240.782
exploration/Returns Std               191.631
exploration/Returns Max               593.72
exploration/Returns Min                10.7492
exploration/Actions Mean                0.159616
exploration/Actions Std                 0.656559
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                   7
exploration/Average Returns           240.782
evaluation/num steps total          24948
evaluation/num paths total            281
evaluation/path length Mean           237.5
evaluation/path length Std             47.5
evaluation/path length Max            285
evaluation/path length Min            190
evaluation/Rewards Mean                 2.12646
evaluation/Rewards Std                  0.640753
evaluation/Rewards Max                  3.36912
evaluation/Rewards Min                  0.871867
evaluation/Returns Mean               505.033
evaluation/Returns Std                101.936
evaluation/Returns Max                606.969
evaluation/Returns Min                403.097
evaluation/Actions Mean                 0.0339283
evaluation/Actions Std                  0.751249
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    2
evaluation/Average Returns            505.033
time/data storing (s)                   0.00354059
time/evaluation sampling (s)            0.303396
time/exploration sampling (s)           0.265506
time/logging (s)                        0.00361554
time/saving (s)                         0.00148694
time/training (s)                       6.62354
time/epoch (s)                          7.20109
time/total (s)                        191.075
Epoch                                  26
---------------------------------  ----------------
2021-07-02 23:41:33.932167 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 27 finished
---------------------------------  ----------------
replay_buffer/size                  38000
trainer/QF Loss                      2806.99
trainer/Policy Loss                 -1273.28
trainer/Raw Policy Loss             -1273.28
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           1222.28
trainer/Q Predictions Std             477.794
trainer/Q Predictions Max            1843.33
trainer/Q Predictions Min               7.1598
trainer/Q Targets Mean               1235.14
trainer/Q Targets Std                 483.189
trainer/Q Targets Max                1858.04
trainer/Q Targets Min                   0.428662
trainer/Bellman Errors Mean          2806.99
trainer/Bellman Errors Std          14993.1
trainer/Bellman Errors Max         168212
trainer/Bellman Errors Min              0.000376716
trainer/Policy Action Mean             -0.0522692
trainer/Policy Action Std               0.906462
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         38000
exploration/num paths total           892
exploration/path length Mean          250
exploration/path length Std           143.828
exploration/path length Max           480
exploration/path length Min            84
exploration/Rewards Mean                1.88507
exploration/Rewards Std                 0.701373
exploration/Rewards Max                 3.79075
exploration/Rewards Min                 0.537969
exploration/Returns Mean              471.267
exploration/Returns Std               309.776
exploration/Returns Max               940.935
exploration/Returns Min               133.103
exploration/Actions Mean                0.0421218
exploration/Actions Std                 0.656378
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                   4
exploration/Average Returns           471.267
evaluation/num steps total          25379
evaluation/num paths total            283
evaluation/path length Mean           215.5
evaluation/path length Std             67.5
evaluation/path length Max            283
evaluation/path length Min            148
evaluation/Rewards Mean                 2.17027
evaluation/Rewards Std                  0.56193
evaluation/Rewards Max                  3.29236
evaluation/Rewards Min                  0.886449
evaluation/Returns Mean               467.693
evaluation/Returns Std                158.055
evaluation/Returns Max                625.748
evaluation/Returns Min                309.638
evaluation/Actions Mean                -0.0857954
evaluation/Actions Std                  0.759453
evaluation/Actions Max                  0.99998
evaluation/Actions Min                 -1
evaluation/Num Paths                    2
evaluation/Average Returns            467.693
time/data storing (s)                   0.00345041
time/evaluation sampling (s)            0.277937
time/exploration sampling (s)           0.251573
time/logging (s)                        0.00330996
time/saving (s)                         0.00139156
time/training (s)                       6.93957
time/epoch (s)                          7.47723
time/total (s)                        198.554
Epoch                                  27
---------------------------------  ----------------
2021-07-02 23:41:41.085700 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 28 finished
---------------------------------  ---------------
replay_buffer/size                  39000
trainer/QF Loss                      4916.91
trainer/Policy Loss                 -1238.59
trainer/Raw Policy Loss             -1238.59
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           1190.12
trainer/Q Predictions Std             534.767
trainer/Q Predictions Max            1811.92
trainer/Q Predictions Min             -67.3628
trainer/Q Targets Mean               1187.49
trainer/Q Targets Std                 541.201
trainer/Q Targets Max                1783.09
trainer/Q Targets Min                 -82.6687
trainer/Bellman Errors Mean          4916.91
trainer/Bellman Errors Std          37634.3
trainer/Bellman Errors Max         426903
trainer/Bellman Errors Min              0.0017429
trainer/Policy Action Mean             -0.0607116
trainer/Policy Action Std               0.911422
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         39000
exploration/num paths total           898
exploration/path length Mean          166.667
exploration/path length Std           111.262
exploration/path length Max           376
exploration/path length Min             5
exploration/Rewards Mean                1.6354
exploration/Rewards Std                 0.840845
exploration/Rewards Max                 3.54794
exploration/Rewards Min                 0.0158532
exploration/Returns Mean              272.567
exploration/Returns Std               171.172
exploration/Returns Max               519.152
exploration/Returns Min                 3.84883
exploration/Actions Mean                0.022234
exploration/Actions Std                 0.662547
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                   6
exploration/Average Returns           272.567
evaluation/num steps total          26247
evaluation/num paths total            288
evaluation/path length Mean           173.6
evaluation/path length Std             25.7884
evaluation/path length Max            225
evaluation/path length Min            158
evaluation/Rewards Mean                 1.98811
evaluation/Rewards Std                  0.581794
evaluation/Rewards Max                  3.0462
evaluation/Rewards Min                  0.601425
evaluation/Returns Mean               345.136
evaluation/Returns Std                 23.7148
evaluation/Returns Max                392.28
evaluation/Returns Min                329.949
evaluation/Actions Mean                -0.115709
evaluation/Actions Std                  0.789451
evaluation/Actions Max                  0.999991
evaluation/Actions Min                 -1
evaluation/Num Paths                    5
evaluation/Average Returns            345.136
time/data storing (s)                   0.00348832
time/evaluation sampling (s)            0.258
time/exploration sampling (s)           0.255919
time/logging (s)                        0.00437425
time/saving (s)                         0.00148479
time/training (s)                       6.6295
time/epoch (s)                          7.15276
time/total (s)                        205.708
Epoch                                  28
---------------------------------  ---------------
2021-07-02 23:41:48.024669 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 29 finished
---------------------------------  --------------
replay_buffer/size                 40000
trainer/QF Loss                     1825.05
trainer/Policy Loss                -1352.07
trainer/Raw Policy Loss            -1352.07
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean          1300.63
trainer/Q Predictions Std            556.836
trainer/Q Predictions Max           2055.91
trainer/Q Predictions Min             -9.15825
trainer/Q Targets Mean              1288.27
trainer/Q Targets Std                556.161
trainer/Q Targets Max               2075.86
trainer/Q Targets Min                 -0.403632
trainer/Bellman Errors Mean         1825.05
trainer/Bellman Errors Std          6234.55
trainer/Bellman Errors Max         66350.7
trainer/Bellman Errors Min             0.00307137
trainer/Policy Action Mean            -0.204654
trainer/Policy Action Std              0.892101
trainer/Policy Action Max              1
trainer/Policy Action Min             -1
exploration/num steps total        40000
exploration/num paths total          902
exploration/path length Mean         250
exploration/path length Std          135.582
exploration/path length Max          441
exploration/path length Min           67
exploration/Rewards Mean               1.32792
exploration/Rewards Std                0.843574
exploration/Rewards Max                4.0045
exploration/Rewards Min               -0.347816
exploration/Returns Mean             331.981
exploration/Returns Std              182.368
exploration/Returns Max              534.212
exploration/Returns Min               36.5982
exploration/Actions Mean               0.0382022
exploration/Actions Std                0.63964
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                  4
exploration/Average Returns          331.981
evaluation/num steps total         27247
evaluation/num paths total           289
evaluation/path length Mean         1000
evaluation/path length Std             0
evaluation/path length Max          1000
evaluation/path length Min          1000
evaluation/Rewards Mean                0.960733
evaluation/Rewards Std                 0.197832
evaluation/Rewards Max                 1.78383
evaluation/Rewards Min                 0.0959683
evaluation/Returns Mean              960.733
evaluation/Returns Std                 0
evaluation/Returns Max               960.733
evaluation/Returns Min               960.733
evaluation/Actions Mean                0.0620242
evaluation/Actions Std                 0.54703
evaluation/Actions Max                 0.999936
evaluation/Actions Min                -0.999995
evaluation/Num Paths                   1
evaluation/Average Returns           960.733
time/data storing (s)                  0.00341157
time/evaluation sampling (s)           0.298933
time/exploration sampling (s)          0.26619
time/logging (s)                       0.00386438
time/saving (s)                        0.00137758
time/training (s)                      6.36256
time/epoch (s)                         6.93633
time/total (s)                       212.646
Epoch                                 29
---------------------------------  --------------
2021-07-02 23:41:55.016811 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 30 finished
---------------------------------  ---------------
replay_buffer/size                 41000
trainer/QF Loss                     3300.5
trainer/Policy Loss                -1404.07
trainer/Raw Policy Loss            -1404.07
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean          1318.69
trainer/Q Predictions Std            665.325
trainer/Q Predictions Max           2181.5
trainer/Q Predictions Min            -24.4645
trainer/Q Targets Mean              1300.41
trainer/Q Targets Std                668.079
trainer/Q Targets Max               2126.91
trainer/Q Targets Min                -65.2878
trainer/Bellman Errors Mean         3300.5
trainer/Bellman Errors Std          7964.8
trainer/Bellman Errors Max         75765.4
trainer/Bellman Errors Min             0.000721216
trainer/Policy Action Mean            -0.0989323
trainer/Policy Action Std              0.924985
trainer/Policy Action Max              1
trainer/Policy Action Min             -1
exploration/num steps total        41000
exploration/num paths total          908
exploration/path length Mean         166.667
exploration/path length Std          136.43
exploration/path length Max          432
exploration/path length Min           19
exploration/Rewards Mean               1.04776
exploration/Rewards Std                0.556131
exploration/Rewards Max                3.40366
exploration/Rewards Min                0.106762
exploration/Returns Mean             174.626
exploration/Returns Std              148.911
exploration/Returns Max              385.213
exploration/Returns Min               12.5517
exploration/Actions Mean               0.0713198
exploration/Actions Std                0.644884
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                  6
exploration/Average Returns          174.626
evaluation/num steps total         28247
evaluation/num paths total           290
evaluation/path length Mean         1000
evaluation/path length Std             0
evaluation/path length Max          1000
evaluation/path length Min          1000
evaluation/Rewards Mean                0.860629
evaluation/Rewards Std                 0.349867
evaluation/Rewards Max                 1.67146
evaluation/Rewards Min                 0.360071
evaluation/Returns Mean              860.629
evaluation/Returns Std                 0
evaluation/Returns Max               860.629
evaluation/Returns Min               860.629
evaluation/Actions Mean                0.114347
evaluation/Actions Std                 0.540764
evaluation/Actions Max                 0.999396
evaluation/Actions Min                -0.99996
evaluation/Num Paths                   1
evaluation/Average Returns           860.629
time/data storing (s)                  0.00344621
time/evaluation sampling (s)           0.263926
time/exploration sampling (s)          0.270685
time/logging (s)                       0.00393484
time/saving (s)                        0.00132338
time/training (s)                      6.44687
time/epoch (s)                         6.99019
time/total (s)                       219.638
Epoch                                 30
---------------------------------  ---------------
2021-07-02 23:42:02.141328 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 31 finished
---------------------------------  --------------
replay_buffer/size                 42000
trainer/QF Loss                     3093.38
trainer/Policy Loss                -1621.37
trainer/Raw Policy Loss            -1621.37
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean          1557.91
trainer/Q Predictions Std            648.327
trainer/Q Predictions Max           2298.26
trainer/Q Predictions Min            -43.5358
trainer/Q Targets Mean              1565.03
trainer/Q Targets Std                644.165
trainer/Q Targets Max               2295.72
trainer/Q Targets Min                 -0.702798
trainer/Bellman Errors Mean         3093.38
trainer/Bellman Errors Std          9658.52
trainer/Bellman Errors Max         61493.9
trainer/Bellman Errors Min             0.0464205
trainer/Policy Action Mean            -0.0412274
trainer/Policy Action Std              0.932614
trainer/Policy Action Max              1
trainer/Policy Action Min             -1
exploration/num steps total        42000
exploration/num paths total          914
exploration/path length Mean         166.667
exploration/path length Std          115.602
exploration/path length Max          375
exploration/path length Min           20
exploration/Rewards Mean               1.85248
exploration/Rewards Std                0.741237
exploration/Rewards Max                4.57288
exploration/Rewards Min                0.526847
exploration/Returns Mean             308.746
exploration/Returns Std              199.073
exploration/Returns Max              568.029
exploration/Returns Min               28.076
exploration/Actions Mean               0.09858
exploration/Actions Std                0.701584
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                  6
exploration/Average Returns          308.746
evaluation/num steps total         29000
evaluation/num paths total           293
evaluation/path length Mean          251
evaluation/path length Std             2.16025
evaluation/path length Max           253
evaluation/path length Min           248
evaluation/Rewards Mean                2.73163
evaluation/Rewards Std                 0.957924
evaluation/Rewards Max                 4.89109
evaluation/Rewards Min                 0.609237
evaluation/Returns Mean              685.64
evaluation/Returns Std                17.3776
evaluation/Returns Max               705.858
evaluation/Returns Min               663.432
evaluation/Actions Mean                0.133555
evaluation/Actions Std                 0.822493
evaluation/Actions Max                 1
evaluation/Actions Min                -1
evaluation/Num Paths                   3
evaluation/Average Returns           685.64
time/data storing (s)                  0.00333244
time/evaluation sampling (s)           0.247105
time/exploration sampling (s)          0.251789
time/logging (s)                       0.00648264
time/saving (s)                        0.00234001
time/training (s)                      6.614
time/epoch (s)                         7.12505
time/total (s)                       226.765
Epoch                                 31
---------------------------------  --------------
2021-07-02 23:42:09.289047 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 32 finished
---------------------------------  ---------------
replay_buffer/size                  43000
trainer/QF Loss                      5825.7
trainer/Policy Loss                 -1714.9
trainer/Raw Policy Loss             -1714.9
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           1610.97
trainer/Q Predictions Std             678.229
trainer/Q Predictions Max            2466.71
trainer/Q Predictions Min            -114.327
trainer/Q Targets Mean               1618.96
trainer/Q Targets Std                 687.079
trainer/Q Targets Max                2448.02
trainer/Q Targets Min                -178.512
trainer/Bellman Errors Mean          5825.7
trainer/Bellman Errors Std          24890.1
trainer/Bellman Errors Max         253905
trainer/Bellman Errors Min              0.875248
trainer/Policy Action Mean             -0.0705236
trainer/Policy Action Std               0.950979
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         43000
exploration/num paths total           919
exploration/path length Mean          200
exploration/path length Std           126.455
exploration/path length Max           381
exploration/path length Min            20
exploration/Rewards Mean                1.83527
exploration/Rewards Std                 0.753686
exploration/Rewards Max                 4.41243
exploration/Rewards Min                 0.39752
exploration/Returns Mean              367.053
exploration/Returns Std               261.047
exploration/Returns Max               757.318
exploration/Returns Min                23.6688
exploration/Actions Mean                0.0684205
exploration/Actions Std                 0.715033
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                   5
exploration/Average Returns           367.053
evaluation/num steps total          29984
evaluation/num paths total            297
evaluation/path length Mean           246
evaluation/path length Std              9.48683
evaluation/path length Max            254
evaluation/path length Min            230
evaluation/Rewards Mean                 2.35481
evaluation/Rewards Std                  0.842822
evaluation/Rewards Max                  4.32107
evaluation/Rewards Min                  0.685652
evaluation/Returns Mean               579.284
evaluation/Returns Std                 29.7894
evaluation/Returns Max                615.833
evaluation/Returns Min                545.475
evaluation/Actions Mean                 0.0740556
evaluation/Actions Std                  0.805239
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    4
evaluation/Average Returns            579.284
time/data storing (s)                   0.00345166
time/evaluation sampling (s)            0.278219
time/exploration sampling (s)           0.254403
time/logging (s)                        0.00574774
time/saving (s)                         0.00194623
time/training (s)                       6.60028
time/epoch (s)                          7.14405
time/total (s)                        233.911
Epoch                                  32
---------------------------------  ---------------
2021-07-02 23:42:16.498561 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 33 finished
---------------------------------  ---------------
replay_buffer/size                  44000
trainer/QF Loss                      4174.26
trainer/Policy Loss                 -1770.71
trainer/Raw Policy Loss             -1770.71
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           1717.28
trainer/Q Predictions Std             618.257
trainer/Q Predictions Max            2254.34
trainer/Q Predictions Min             -89.5795
trainer/Q Targets Mean               1725.39
trainer/Q Targets Std                 620.297
trainer/Q Targets Max                2356.39
trainer/Q Targets Min                  -1.21045
trainer/Bellman Errors Mean          4174.26
trainer/Bellman Errors Std          17027.7
trainer/Bellman Errors Max         138184
trainer/Bellman Errors Min              0.280543
trainer/Policy Action Mean             -0.0414348
trainer/Policy Action Std               0.920744
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         44000
exploration/num paths total           930
exploration/path length Mean           90.9091
exploration/path length Std            41.9295
exploration/path length Max           158
exploration/path length Min            13
exploration/Rewards Mean                0.9111
exploration/Rewards Std                 0.543489
exploration/Rewards Max                 2.38919
exploration/Rewards Min                -0.4313
exploration/Returns Mean               82.8273
exploration/Returns Std                35.3753
exploration/Returns Max               150.669
exploration/Returns Min                12.8542
exploration/Actions Mean                0.3444
exploration/Actions Std                 0.692791
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  11
exploration/Average Returns            82.8273
evaluation/num steps total          30896
evaluation/num paths total            302
evaluation/path length Mean           182.4
evaluation/path length Std             77.8784
evaluation/path length Max            333
evaluation/path length Min            130
evaluation/Rewards Mean                 0.793714
evaluation/Rewards Std                  0.549799
evaluation/Rewards Max                  2.10005
evaluation/Rewards Min                 -0.43715
evaluation/Returns Mean               144.773
evaluation/Returns Std                 84.8439
evaluation/Returns Max                310.657
evaluation/Returns Min                 86.5279
evaluation/Actions Mean                 0.44572
evaluation/Actions Std                  0.757378
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    5
evaluation/Average Returns            144.773
time/data storing (s)                   0.00344424
time/evaluation sampling (s)            0.275427
time/exploration sampling (s)           0.259296
time/logging (s)                        0.00518939
time/saving (s)                         0.00168564
time/training (s)                       6.66054
time/epoch (s)                          7.20559
time/total (s)                        241.119
Epoch                                  33
---------------------------------  ---------------
2021-07-02 23:42:24.062901 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 34 finished
---------------------------------  ---------------
replay_buffer/size                  45000
trainer/QF Loss                      4088.68
trainer/Policy Loss                 -1688.19
trainer/Raw Policy Loss             -1688.19
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           1599.55
trainer/Q Predictions Std             753.857
trainer/Q Predictions Max            2349.59
trainer/Q Predictions Min            -108.863
trainer/Q Targets Mean               1595.37
trainer/Q Targets Std                 760.558
trainer/Q Targets Max                2441.88
trainer/Q Targets Min                  -0.0285602
trainer/Bellman Errors Mean          4088.68
trainer/Bellman Errors Std          14178.2
trainer/Bellman Errors Max         115316
trainer/Bellman Errors Min              0.00189249
trainer/Policy Action Mean             -0.035588
trainer/Policy Action Std               0.924754
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         45000
exploration/num paths total           933
exploration/path length Mean          333.333
exploration/path length Std           163.416
exploration/path length Max           562
exploration/path length Min           190
exploration/Rewards Mean                1.17952
exploration/Rewards Std                 0.438244
exploration/Rewards Max                 2.61356
exploration/Rewards Min                 0.171721
exploration/Returns Mean              393.173
exploration/Returns Std               164.391
exploration/Returns Max               601.708
exploration/Returns Min               199.903
exploration/Actions Mean                0.0561362
exploration/Actions Std                 0.628505
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                   3
exploration/Average Returns           393.173
evaluation/num steps total          31896
evaluation/num paths total            303
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 0.978093
evaluation/Rewards Std                  0.154451
evaluation/Rewards Max                  1.5749
evaluation/Rewards Min                  0.618326
evaluation/Returns Mean               978.093
evaluation/Returns Std                  0
evaluation/Returns Max                978.093
evaluation/Returns Min                978.093
evaluation/Actions Mean                 0.120387
evaluation/Actions Std                  0.640977
evaluation/Actions Max                  0.999988
evaluation/Actions Min                 -0.99982
evaluation/Num Paths                    1
evaluation/Average Returns            978.093
time/data storing (s)                   0.00343824
time/evaluation sampling (s)            0.286859
time/exploration sampling (s)           0.255411
time/logging (s)                        0.00501335
time/saving (s)                         0.00134837
time/training (s)                       7.00953
time/epoch (s)                          7.5616
time/total (s)                        248.682
Epoch                                  34
---------------------------------  ---------------
2021-07-02 23:42:31.289208 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 35 finished
---------------------------------  ---------------
replay_buffer/size                  46000
trainer/QF Loss                      5934.11
trainer/Policy Loss                 -1762.26
trainer/Raw Policy Loss             -1762.26
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           1706.62
trainer/Q Predictions Std             732.667
trainer/Q Predictions Max            2444.76
trainer/Q Predictions Min              10.3952
trainer/Q Targets Mean               1718.63
trainer/Q Targets Std                 752.172
trainer/Q Targets Max                2474.81
trainer/Q Targets Min                 -51.344
trainer/Bellman Errors Mean          5934.11
trainer/Bellman Errors Std          27905.9
trainer/Bellman Errors Max         226743
trainer/Bellman Errors Min              0.0137902
trainer/Policy Action Mean             -0.054955
trainer/Policy Action Std               0.913874
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         46000
exploration/num paths total           938
exploration/path length Mean          200
exploration/path length Std           217.959
exploration/path length Max           581
exploration/path length Min            14
exploration/Rewards Mean                1.3141
exploration/Rewards Std                 0.625219
exploration/Rewards Max                 4.09371
exploration/Rewards Min                 0.271204
exploration/Returns Mean              262.819
exploration/Returns Std               307.308
exploration/Returns Max               816.142
exploration/Returns Min                10.804
exploration/Actions Mean                0.110698
exploration/Actions Std                 0.669957
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                   5
exploration/Average Returns           262.819
evaluation/num steps total          32896
evaluation/num paths total            304
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 1.01275
evaluation/Rewards Std                  0.24206
evaluation/Rewards Max                  1.80251
evaluation/Rewards Min                  0.560097
evaluation/Returns Mean              1012.75
evaluation/Returns Std                  0
evaluation/Returns Max               1012.75
evaluation/Returns Min               1012.75
evaluation/Actions Mean                 0.0119164
evaluation/Actions Std                  0.637044
evaluation/Actions Max                  0.999997
evaluation/Actions Min                 -0.999863
evaluation/Num Paths                    1
evaluation/Average Returns           1012.75
time/data storing (s)                   0.0034724
time/evaluation sampling (s)            0.247286
time/exploration sampling (s)           0.256761
time/logging (s)                        0.00384913
time/saving (s)                         0.00132541
time/training (s)                       6.71019
time/epoch (s)                          7.22288
time/total (s)                        255.907
Epoch                                  35
---------------------------------  ---------------
2021-07-02 23:42:38.269988 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 36 finished
---------------------------------  ---------------
replay_buffer/size                  47000
trainer/QF Loss                      3048.55
trainer/Policy Loss                 -1856.21
trainer/Raw Policy Loss             -1856.21
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           1767.96
trainer/Q Predictions Std             719.632
trainer/Q Predictions Max            2498.03
trainer/Q Predictions Min             -34.8938
trainer/Q Targets Mean               1778.42
trainer/Q Targets Std                 720.865
trainer/Q Targets Max                2494.17
trainer/Q Targets Min                  -6.25979
trainer/Bellman Errors Mean          3048.55
trainer/Bellman Errors Std          10663.8
trainer/Bellman Errors Max         106324
trainer/Bellman Errors Min              0.159532
trainer/Policy Action Mean              0.0929458
trainer/Policy Action Std               0.925355
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         47000
exploration/num paths total           943
exploration/path length Mean          200
exploration/path length Std            78.1614
exploration/path length Max           325
exploration/path length Min            79
exploration/Rewards Mean                1.46537
exploration/Rewards Std                 0.626427
exploration/Rewards Max                 3.26871
exploration/Rewards Min                 0.414878
exploration/Returns Mean              293.075
exploration/Returns Std               117.435
exploration/Returns Max               435.261
exploration/Returns Min               102.597
exploration/Actions Mean                0.11769
exploration/Actions Std                 0.63312
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                   5
exploration/Average Returns           293.075
evaluation/num steps total          33896
evaluation/num paths total            305
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 1.01199
evaluation/Rewards Std                  0.0769514
evaluation/Rewards Max                  1.62528
evaluation/Rewards Min                  0.564676
evaluation/Returns Mean              1011.99
evaluation/Returns Std                  0
evaluation/Returns Max               1011.99
evaluation/Returns Min               1011.99
evaluation/Actions Mean                 0.283572
evaluation/Actions Std                  0.59429
evaluation/Actions Max                  1
evaluation/Actions Min                 -0.998569
evaluation/Num Paths                    1
evaluation/Average Returns           1011.99
time/data storing (s)                   0.00353654
time/evaluation sampling (s)            0.275371
time/exploration sampling (s)           0.265497
time/logging (s)                        0.00434765
time/saving (s)                         0.00146658
time/training (s)                       6.42885
time/epoch (s)                          6.97907
time/total (s)                        262.888
Epoch                                  36
---------------------------------  ---------------
2021-07-02 23:42:45.421736 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 37 finished
---------------------------------  --------------
replay_buffer/size                 48000
trainer/QF Loss                     4039.9
trainer/Policy Loss                -1900.35
trainer/Raw Policy Loss            -1900.35
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean          1814.09
trainer/Q Predictions Std            779.402
trainer/Q Predictions Max           2631.7
trainer/Q Predictions Min             31.8197
trainer/Q Targets Mean              1801.53
trainer/Q Targets Std                776.594
trainer/Q Targets Max               2625.46
trainer/Q Targets Min                  0.292955
trainer/Bellman Errors Mean         4039.9
trainer/Bellman Errors Std          9924.39
trainer/Bellman Errors Max         83836.8
trainer/Bellman Errors Min             0.167228
trainer/Policy Action Mean            -0.0520337
trainer/Policy Action Std              0.926699
trainer/Policy Action Max              1
trainer/Policy Action Min             -1
exploration/num steps total        48000
exploration/num paths total          946
exploration/path length Mean         333.333
exploration/path length Std          381.101
exploration/path length Max          870
exploration/path length Min           22
exploration/Rewards Mean               1.21288
exploration/Rewards Std                0.425932
exploration/Rewards Max                2.40022
exploration/Rewards Min               -0.0075158
exploration/Returns Mean             404.294
exploration/Returns Std              461.283
exploration/Returns Max             1053.19
exploration/Returns Min               21.7956
exploration/Actions Mean               0.154953
exploration/Actions Std                0.661566
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                  3
exploration/Average Returns          404.294
evaluation/num steps total         34896
evaluation/num paths total           306
evaluation/path length Mean         1000
evaluation/path length Std             0
evaluation/path length Max          1000
evaluation/path length Min          1000
evaluation/Rewards Mean                1.24382
evaluation/Rewards Std                 0.468433
evaluation/Rewards Max                 2.70625
evaluation/Rewards Min                 0.262319
evaluation/Returns Mean             1243.82
evaluation/Returns Std                 0
evaluation/Returns Max              1243.82
evaluation/Returns Min              1243.82
evaluation/Actions Mean                0.133833
evaluation/Actions Std                 0.743362
evaluation/Actions Max                 1
evaluation/Actions Min                -1
evaluation/Num Paths                   1
evaluation/Average Returns          1243.82
time/data storing (s)                  0.0040306
time/evaluation sampling (s)           0.241916
time/exploration sampling (s)          0.25587
time/logging (s)                       0.00385044
time/saving (s)                        0.00150597
time/training (s)                      6.64147
time/epoch (s)                         7.14864
time/total (s)                       270.039
Epoch                                 37
---------------------------------  --------------
2021-07-02 23:42:52.523813 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 38 finished
---------------------------------  ----------------
replay_buffer/size                  49000
trainer/QF Loss                     12610.9
trainer/Policy Loss                 -1982.77
trainer/Raw Policy Loss             -1982.77
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           1921.77
trainer/Q Predictions Std             757.171
trainer/Q Predictions Max            2793.49
trainer/Q Predictions Min              55.186
trainer/Q Targets Mean               1909.25
trainer/Q Targets Std                 771.712
trainer/Q Targets Max                2754.08
trainer/Q Targets Min                  -8.36199
trainer/Bellman Errors Mean         12610.9
trainer/Bellman Errors Std         101558
trainer/Bellman Errors Max              1.13499e+06
trainer/Bellman Errors Min              6.20526e-05
trainer/Policy Action Mean             -0.0939144
trainer/Policy Action Std               0.924472
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         49000
exploration/num paths total           951
exploration/path length Mean          200
exploration/path length Std           173.474
exploration/path length Max           534
exploration/path length Min            55
exploration/Rewards Mean                1.74947
exploration/Rewards Std                 0.650829
exploration/Rewards Max                 4.10216
exploration/Rewards Min                 0.475991
exploration/Returns Mean              349.895
exploration/Returns Std               323.103
exploration/Returns Max               956.314
exploration/Returns Min                70.732
exploration/Actions Mean                0.101897
exploration/Actions Std                 0.676398
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                   5
exploration/Average Returns           349.895
evaluation/num steps total          35633
evaluation/num paths total            307
evaluation/path length Mean           737
evaluation/path length Std              0
evaluation/path length Max            737
evaluation/path length Min            737
evaluation/Rewards Mean                 1.60286
evaluation/Rewards Std                  0.683885
evaluation/Rewards Max                  4.37944
evaluation/Rewards Min                  0.41112
evaluation/Returns Mean              1181.31
evaluation/Returns Std                  0
evaluation/Returns Max               1181.31
evaluation/Returns Min               1181.31
evaluation/Actions Mean                 0.0443576
evaluation/Actions Std                  0.736875
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    1
evaluation/Average Returns           1181.31
time/data storing (s)                   0.00338287
time/evaluation sampling (s)            0.238269
time/exploration sampling (s)           0.25944
time/logging (s)                        0.00342534
time/saving (s)                         0.00127125
time/training (s)                       6.59368
time/epoch (s)                          7.09947
time/total (s)                        277.14
Epoch                                  38
---------------------------------  ----------------
2021-07-02 23:42:59.468453 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 39 finished
---------------------------------  ---------------
replay_buffer/size                  50000
trainer/QF Loss                      4419.13
trainer/Policy Loss                 -1952.63
trainer/Raw Policy Loss             -1952.63
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           1870.64
trainer/Q Predictions Std             787.83
trainer/Q Predictions Max            2631.77
trainer/Q Predictions Min             -90.0971
trainer/Q Targets Mean               1899.43
trainer/Q Targets Std                 794.828
trainer/Q Targets Max                2653.39
trainer/Q Targets Min                   0.0427861
trainer/Bellman Errors Mean          4419.13
trainer/Bellman Errors Std          14164.3
trainer/Bellman Errors Max         124627
trainer/Bellman Errors Min              0.563599
trainer/Policy Action Mean             -0.10075
trainer/Policy Action Std               0.916893
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         50000
exploration/num paths total           954
exploration/path length Mean          333.333
exploration/path length Std           116.82
exploration/path length Max           496
exploration/path length Min           227
exploration/Rewards Mean                1.36196
exploration/Rewards Std                 0.560812
exploration/Rewards Max                 3.2417
exploration/Rewards Min                 0.350598
exploration/Returns Mean              453.988
exploration/Returns Std               203.049
exploration/Returns Max               740.821
exploration/Returns Min               298.8
exploration/Actions Mean                0.198861
exploration/Actions Std                 0.666792
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                   3
exploration/Average Returns           453.988
evaluation/num steps total          36628
evaluation/num paths total            308
evaluation/path length Mean           995
evaluation/path length Std              0
evaluation/path length Max            995
evaluation/path length Min            995
evaluation/Rewards Mean                 1.63374
evaluation/Rewards Std                  0.71281
evaluation/Rewards Max                  4.23149
evaluation/Rewards Min                  0.23197
evaluation/Returns Mean              1625.58
evaluation/Returns Std                  0
evaluation/Returns Max               1625.58
evaluation/Returns Min               1625.58
evaluation/Actions Mean                 0.113541
evaluation/Actions Std                  0.792614
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    1
evaluation/Average Returns           1625.58
time/data storing (s)                   0.00338565
time/evaluation sampling (s)            0.256294
time/exploration sampling (s)           0.254527
time/logging (s)                        0.00624203
time/saving (s)                         0.00136309
time/training (s)                       6.42347
time/epoch (s)                          6.94528
time/total (s)                        284.087
Epoch                                  39
---------------------------------  ---------------
2021-07-02 23:43:06.529450 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 40 finished
---------------------------------  ---------------
replay_buffer/size                  51000
trainer/QF Loss                      4516.61
trainer/Policy Loss                 -1938.55
trainer/Raw Policy Loss             -1938.55
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           1885.5
trainer/Q Predictions Std             804.642
trainer/Q Predictions Max            2510.28
trainer/Q Predictions Min              -1.61918
trainer/Q Targets Mean               1881.4
trainer/Q Targets Std                 813.315
trainer/Q Targets Max                2569.96
trainer/Q Targets Min                  -0.403632
trainer/Bellman Errors Mean          4516.61
trainer/Bellman Errors Std          12648.3
trainer/Bellman Errors Max         100613
trainer/Bellman Errors Min              0.00789738
trainer/Policy Action Mean             -0.110732
trainer/Policy Action Std               0.904821
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         51000
exploration/num paths total           966
exploration/path length Mean           83.3333
exploration/path length Std            54.4355
exploration/path length Max           186
exploration/path length Min             9
exploration/Rewards Mean                1.26169
exploration/Rewards Std                 0.54975
exploration/Rewards Max                 3.00098
exploration/Rewards Min                 0.0975939
exploration/Returns Mean              105.141
exploration/Returns Std                81.071
exploration/Returns Max               285.692
exploration/Returns Min                 7.75353
exploration/Actions Mean                0.168645
exploration/Actions Std                 0.711581
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  12
exploration/Average Returns           105.141
evaluation/num steps total          36909
evaluation/num paths total            309
evaluation/path length Mean           281
evaluation/path length Std              0
evaluation/path length Max            281
evaluation/path length Min            281
evaluation/Rewards Mean                 1.75482
evaluation/Rewards Std                  0.636998
evaluation/Rewards Max                  3.34103
evaluation/Rewards Min                  0.462915
evaluation/Returns Mean               493.105
evaluation/Returns Std                  0
evaluation/Returns Max                493.105
evaluation/Returns Min                493.105
evaluation/Actions Mean                 0.166105
evaluation/Actions Std                  0.840157
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    1
evaluation/Average Returns            493.105
time/data storing (s)                   0.00338926
time/evaluation sampling (s)            0.278007
time/exploration sampling (s)           0.250961
time/logging (s)                        0.00299236
time/saving (s)                         0.001771
time/training (s)                       6.51748
time/epoch (s)                          7.0546
time/total (s)                        291.144
Epoch                                  40
---------------------------------  ---------------
2021-07-02 23:43:13.521056 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 41 finished
---------------------------------  ---------------
replay_buffer/size                  52000
trainer/QF Loss                      4393
trainer/Policy Loss                 -1972.35
trainer/Raw Policy Loss             -1972.35
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           1922.01
trainer/Q Predictions Std             829.737
trainer/Q Predictions Max            2714.84
trainer/Q Predictions Min              22.3411
trainer/Q Targets Mean               1908.68
trainer/Q Targets Std                 828.358
trainer/Q Targets Max                2758.78
trainer/Q Targets Min                 -37.3283
trainer/Bellman Errors Mean          4393
trainer/Bellman Errors Std          13298.5
trainer/Bellman Errors Max         112123
trainer/Bellman Errors Min              0.0278049
trainer/Policy Action Mean             -0.0828685
trainer/Policy Action Std               0.908163
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         52000
exploration/num paths total           978
exploration/path length Mean           83.3333
exploration/path length Std            84.0727
exploration/path length Max           310
exploration/path length Min             8
exploration/Rewards Mean                1.85732
exploration/Rewards Std                 0.682873
exploration/Rewards Max                 3.95495
exploration/Rewards Min                 0.392476
exploration/Returns Mean              154.776
exploration/Returns Std               183.406
exploration/Returns Max               676.79
exploration/Returns Min                 5.88588
exploration/Actions Mean                0.140079
exploration/Actions Std                 0.733327
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  12
exploration/Average Returns           154.776
evaluation/num steps total          37692
evaluation/num paths total            312
evaluation/path length Mean           261
evaluation/path length Std            166.987
evaluation/path length Max            492
evaluation/path length Min            103
evaluation/Rewards Mean                 1.97335
evaluation/Rewards Std                  0.737315
evaluation/Rewards Max                  4.51718
evaluation/Rewards Min                  0.510008
evaluation/Returns Mean               515.044
evaluation/Returns Std                344.103
evaluation/Returns Max                984.006
evaluation/Returns Min                168.017
evaluation/Actions Mean                 0.216949
evaluation/Actions Std                  0.831489
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    3
evaluation/Average Returns            515.044
time/data storing (s)                   0.00339046
time/evaluation sampling (s)            0.252917
time/exploration sampling (s)           0.261346
time/logging (s)                        0.00369386
time/saving (s)                         0.00167346
time/training (s)                       6.4673
time/epoch (s)                          6.99032
time/total (s)                        298.136
Epoch                                  41
---------------------------------  ---------------
2021-07-02 23:43:20.565336 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 42 finished
---------------------------------  ---------------
replay_buffer/size                  53000
trainer/QF Loss                      5477.01
trainer/Policy Loss                 -1845.72
trainer/Raw Policy Loss             -1845.72
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           1785.79
trainer/Q Predictions Std             887.162
trainer/Q Predictions Max            2639.26
trainer/Q Predictions Min            -323.173
trainer/Q Targets Mean               1803.41
trainer/Q Targets Std                 887.056
trainer/Q Targets Max                2665.34
trainer/Q Targets Min                -127.719
trainer/Bellman Errors Mean          5477.01
trainer/Bellman Errors Std          18316.2
trainer/Bellman Errors Max         152215
trainer/Bellman Errors Min              1.23975
trainer/Policy Action Mean             -0.0818556
trainer/Policy Action Std               0.944697
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         53000
exploration/num paths total           986
exploration/path length Mean          125
exploration/path length Std            84.1056
exploration/path length Max           250
exploration/path length Min            24
exploration/Rewards Mean                1.67102
exploration/Rewards Std                 0.776161
exploration/Rewards Max                 4.62024
exploration/Rewards Min                 0.174399
exploration/Returns Mean              208.878
exploration/Returns Std               174.411
exploration/Returns Max               510.833
exploration/Returns Min                35.1283
exploration/Actions Mean                0.0925761
exploration/Actions Std                 0.710548
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                   8
exploration/Average Returns           208.878
evaluation/num steps total          38634
evaluation/num paths total            316
evaluation/path length Mean           235.5
evaluation/path length Std             95.5628
evaluation/path length Max            401
evaluation/path length Min            178
evaluation/Rewards Mean                 1.97151
evaluation/Rewards Std                  0.803928
evaluation/Rewards Max                  4.5659
evaluation/Rewards Min                  0.45771
evaluation/Returns Mean               464.29
evaluation/Returns Std                204.467
evaluation/Returns Max                815.793
evaluation/Returns Min                318.641
evaluation/Actions Mean                 0.107866
evaluation/Actions Std                  0.868663
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    4
evaluation/Average Returns            464.29
time/data storing (s)                   0.00353728
time/evaluation sampling (s)            0.240263
time/exploration sampling (s)           0.249837
time/logging (s)                        0.00378391
time/saving (s)                         0.00130742
time/training (s)                       6.54345
time/epoch (s)                          7.04218
time/total (s)                        305.179
Epoch                                  42
---------------------------------  ---------------
2021-07-02 23:43:27.596393 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 43 finished
---------------------------------  ---------------
replay_buffer/size                 54000
trainer/QF Loss                     3664.36
trainer/Policy Loss                -1776.9
trainer/Raw Policy Loss            -1776.9
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean          1736.8
trainer/Q Predictions Std            946.569
trainer/Q Predictions Max           2678.18
trainer/Q Predictions Min            -20.9653
trainer/Q Targets Mean              1726.75
trainer/Q Targets Std                947.392
trainer/Q Targets Max               2654.19
trainer/Q Targets Min               -107.339
trainer/Bellman Errors Mean         3664.35
trainer/Bellman Errors Std         10074.2
trainer/Bellman Errors Max         64499.8
trainer/Bellman Errors Min             7.19763e-05
trainer/Policy Action Mean            -0.0991666
trainer/Policy Action Std              0.939537
trainer/Policy Action Max              1
trainer/Policy Action Min             -1
exploration/num steps total        54000
exploration/num paths total          998
exploration/path length Mean          83.3333
exploration/path length Std           57.5852
exploration/path length Max          172
exploration/path length Min           11
exploration/Rewards Mean               1.82396
exploration/Rewards Std                0.693278
exploration/Rewards Max                3.78282
exploration/Rewards Min                0.400356
exploration/Returns Mean             151.997
exploration/Returns Std              125.297
exploration/Returns Max              338.791
exploration/Returns Min               12.1877
exploration/Actions Mean               0.0940968
exploration/Actions Std                0.717638
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                 12
exploration/Average Returns          151.997
evaluation/num steps total         39622
evaluation/num paths total           323
evaluation/path length Mean          141.143
evaluation/path length Std            11.3569
evaluation/path length Max           162
evaluation/path length Min           130
evaluation/Rewards Mean                1.92279
evaluation/Rewards Std                 0.687545
evaluation/Rewards Max                 5.45088
evaluation/Rewards Min                 0.440189
evaluation/Returns Mean              271.388
evaluation/Returns Std                49.0516
evaluation/Returns Max               373.779
evaluation/Returns Min               230.156
evaluation/Actions Mean                0.0629341
evaluation/Actions Std                 0.851953
evaluation/Actions Max                 1
evaluation/Actions Min                -1
evaluation/Num Paths                   7
evaluation/Average Returns           271.388
time/data storing (s)                  0.00348414
time/evaluation sampling (s)           0.2555
time/exploration sampling (s)          0.250382
time/logging (s)                       0.00389492
time/saving (s)                        0.00128946
time/training (s)                      6.51465
time/epoch (s)                         7.0292
time/total (s)                       312.21
Epoch                                 43
---------------------------------  ---------------
2021-07-02 23:43:34.668630 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 44 finished
---------------------------------  ---------------
replay_buffer/size                 55000
trainer/QF Loss                     4788.19
trainer/Policy Loss                -2094.46
trainer/Raw Policy Loss            -2094.46
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean          2016.37
trainer/Q Predictions Std            767.736
trainer/Q Predictions Max           2814.24
trainer/Q Predictions Min            -52.7799
trainer/Q Targets Mean              1995.4
trainer/Q Targets Std                782.83
trainer/Q Targets Max               2827.31
trainer/Q Targets Min                 -0.2593
trainer/Bellman Errors Mean         4788.19
trainer/Bellman Errors Std         13242.7
trainer/Bellman Errors Max         89211.7
trainer/Bellman Errors Min             0.000841404
trainer/Policy Action Mean             0.0386046
trainer/Policy Action Std              0.912435
trainer/Policy Action Max              1
trainer/Policy Action Min             -1
exploration/num steps total        55000
exploration/num paths total         1006
exploration/path length Mean         125
exploration/path length Std           92.9126
exploration/path length Max          312
exploration/path length Min            8
exploration/Rewards Mean               1.72041
exploration/Rewards Std                0.64829
exploration/Rewards Max                3.67618
exploration/Rewards Min                0.0654405
exploration/Returns Mean             215.051
exploration/Returns Std              159.868
exploration/Returns Max              458.797
exploration/Returns Min                7.26923
exploration/Actions Mean               0.105157
exploration/Actions Std                0.711618
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                  8
exploration/Average Returns          215.051
evaluation/num steps total         40605
evaluation/num paths total           325
evaluation/path length Mean          491.5
evaluation/path length Std           230.5
evaluation/path length Max           722
evaluation/path length Min           261
evaluation/Rewards Mean                1.69785
evaluation/Rewards Std                 0.673967
evaluation/Rewards Max                 3.7684
evaluation/Rewards Min                 0.43279
evaluation/Returns Mean              834.494
evaluation/Returns Std               295.151
evaluation/Returns Max              1129.64
evaluation/Returns Min               539.343
evaluation/Actions Mean                0.160712
evaluation/Actions Std                 0.827319
evaluation/Actions Max                 1
evaluation/Actions Min                -1
evaluation/Num Paths                   2
evaluation/Average Returns           834.494
time/data storing (s)                  0.00344911
time/evaluation sampling (s)           0.250583
time/exploration sampling (s)          0.262327
time/logging (s)                       0.0046062
time/saving (s)                        0.00134837
time/training (s)                      6.54846
time/epoch (s)                         7.07077
time/total (s)                       319.283
Epoch                                 44
---------------------------------  ---------------
2021-07-02 23:43:41.474414 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 45 finished
---------------------------------  ----------------
replay_buffer/size                  56000
trainer/QF Loss                     32694.2
trainer/Policy Loss                 -2048.9
trainer/Raw Policy Loss             -2048.9
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           1969.63
trainer/Q Predictions Std             795.386
trainer/Q Predictions Max            2879.24
trainer/Q Predictions Min               1.04785
trainer/Q Targets Mean               1952.62
trainer/Q Targets Std                 827.105
trainer/Q Targets Max                2749.81
trainer/Q Targets Min                 -42.7451
trainer/Bellman Errors Mean         32694.2
trainer/Bellman Errors Std         205530
trainer/Bellman Errors Max              2.05771e+06
trainer/Bellman Errors Min              0.186314
trainer/Policy Action Mean              0.00356387
trainer/Policy Action Std               0.925801
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         56000
exploration/num paths total          1014
exploration/path length Mean          125
exploration/path length Std            97.3884
exploration/path length Max           299
exploration/path length Min            15
exploration/Rewards Mean                1.53465
exploration/Rewards Std                 0.570887
exploration/Rewards Max                 3.06404
exploration/Rewards Min                -0.0905235
exploration/Returns Mean              191.831
exploration/Returns Std               162.005
exploration/Returns Max               468.281
exploration/Returns Min                13.3051
exploration/Actions Mean                0.188781
exploration/Actions Std                 0.716634
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                   8
exploration/Average Returns           191.831
evaluation/num steps total          41033
evaluation/num paths total            326
evaluation/path length Mean           428
evaluation/path length Std              0
evaluation/path length Max            428
evaluation/path length Min            428
evaluation/Rewards Mean                 1.76428
evaluation/Rewards Std                  0.746323
evaluation/Rewards Max                  4.75576
evaluation/Rewards Min                  0.5113
evaluation/Returns Mean               755.11
evaluation/Returns Std                  0
evaluation/Returns Max                755.11
evaluation/Returns Min                755.11
evaluation/Actions Mean                 0.19279
evaluation/Actions Std                  0.796
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    1
evaluation/Average Returns            755.11
time/data storing (s)                   0.00340615
time/evaluation sampling (s)            0.270024
time/exploration sampling (s)           0.254597
time/logging (s)                        0.00324669
time/saving (s)                         0.00146734
time/training (s)                       6.26933
time/epoch (s)                          6.80207
time/total (s)                        326.087
Epoch                                  45
---------------------------------  ----------------
2021-07-02 23:43:48.377130 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 46 finished
---------------------------------  --------------
replay_buffer/size                 57000
trainer/QF Loss                     2442.4
trainer/Policy Loss                -1958.68
trainer/Raw Policy Loss            -1958.68
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean          1914.4
trainer/Q Predictions Std            922.526
trainer/Q Predictions Max           2858.69
trainer/Q Predictions Min            -82.383
trainer/Q Targets Mean              1918.49
trainer/Q Targets Std                918.441
trainer/Q Targets Max               2794.73
trainer/Q Targets Min                  0.610656
trainer/Bellman Errors Mean         2442.4
trainer/Bellman Errors Std          7454.1
trainer/Bellman Errors Max         74282.5
trainer/Bellman Errors Min             0.407463
trainer/Policy Action Mean             0.130411
trainer/Policy Action Std              0.909412
trainer/Policy Action Max              1
trainer/Policy Action Min             -1
exploration/num steps total        57000
exploration/num paths total         1026
exploration/path length Mean          83.3333
exploration/path length Std           50.0372
exploration/path length Max          181
exploration/path length Min           12
exploration/Rewards Mean               1.49802
exploration/Rewards Std                0.550775
exploration/Rewards Max                3.11294
exploration/Rewards Min                0.477127
exploration/Returns Mean             124.835
exploration/Returns Std               77.6045
exploration/Returns Max              278.44
exploration/Returns Min                8.79594
exploration/Actions Mean               0.17185
exploration/Actions Std                0.675744
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                 12
exploration/Average Returns          124.835
evaluation/num steps total         41935
evaluation/num paths total           329
evaluation/path length Mean          300.667
evaluation/path length Std            24.4994
evaluation/path length Max           327
evaluation/path length Min           268
evaluation/Rewards Mean                1.85757
evaluation/Rewards Std                 0.679868
evaluation/Rewards Max                 3.63896
evaluation/Rewards Min                 0.562392
evaluation/Returns Mean              558.51
evaluation/Returns Std                38.4197
evaluation/Returns Max               589.459
evaluation/Returns Min               504.361
evaluation/Actions Mean                0.0591001
evaluation/Actions Std                 0.800375
evaluation/Actions Max                 1
evaluation/Actions Min                -1
evaluation/Num Paths                   3
evaluation/Average Returns           558.51
time/data storing (s)                  0.00345937
time/evaluation sampling (s)           0.268619
time/exploration sampling (s)          0.287798
time/logging (s)                       0.00393708
time/saving (s)                        0.00193858
time/training (s)                      6.33554
time/epoch (s)                         6.90129
time/total (s)                       332.989
Epoch                                 46
---------------------------------  --------------
2021-07-02 23:43:55.492765 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 47 finished
---------------------------------  ---------------
replay_buffer/size                  58000
trainer/QF Loss                      5249.57
trainer/Policy Loss                 -2114.5
trainer/Raw Policy Loss             -2114.5
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2054.75
trainer/Q Predictions Std             779.874
trainer/Q Predictions Max            2815.59
trainer/Q Predictions Min              17.5457
trainer/Q Targets Mean               2084.22
trainer/Q Targets Std                 794.521
trainer/Q Targets Max                2837.3
trainer/Q Targets Min                 -29.6648
trainer/Bellman Errors Mean          5249.57
trainer/Bellman Errors Std          13933.3
trainer/Bellman Errors Max         114566
trainer/Bellman Errors Min              0.094929
trainer/Policy Action Mean              0.287157
trainer/Policy Action Std               0.866873
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         58000
exploration/num paths total          1031
exploration/path length Mean          200
exploration/path length Std           158.279
exploration/path length Max           420
exploration/path length Min            15
exploration/Rewards Mean                1.50549
exploration/Rewards Std                 0.636376
exploration/Rewards Max                 4.28185
exploration/Rewards Min                -0.196124
exploration/Returns Mean              301.098
exploration/Returns Std               271.811
exploration/Returns Max               763.371
exploration/Returns Min                14.2733
exploration/Actions Mean                0.278379
exploration/Actions Std                 0.682024
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                   5
exploration/Average Returns           301.098
evaluation/num steps total          42898
evaluation/num paths total            333
evaluation/path length Mean           240.75
evaluation/path length Std             31.6573
evaluation/path length Max            275
evaluation/path length Min            189
evaluation/Rewards Mean                 2.15614
evaluation/Rewards Std                  0.907133
evaluation/Rewards Max                  6.11971
evaluation/Rewards Min                  0.507453
evaluation/Returns Mean               519.09
evaluation/Returns Std                 56.2726
evaluation/Returns Max                566.573
evaluation/Returns Min                423.871
evaluation/Actions Mean                 0.102912
evaluation/Actions Std                  0.876896
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    4
evaluation/Average Returns            519.09
time/data storing (s)                   0.0033792
time/evaluation sampling (s)            0.235116
time/exploration sampling (s)           0.254142
time/logging (s)                        0.00389384
time/saving (s)                         0.0012927
time/training (s)                       6.6155
time/epoch (s)                          7.11333
time/total (s)                        340.104
Epoch                                  47
---------------------------------  ---------------
2021-07-02 23:44:02.598292 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 48 finished
---------------------------------  ---------------
replay_buffer/size                  59000
trainer/QF Loss                      6346.63
trainer/Policy Loss                 -2126.61
trainer/Raw Policy Loss             -2126.61
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2074.57
trainer/Q Predictions Std             861.66
trainer/Q Predictions Max            2962.07
trainer/Q Predictions Min              17.474
trainer/Q Targets Mean               2050.86
trainer/Q Targets Std                 840.675
trainer/Q Targets Max                2888.25
trainer/Q Targets Min                  -0.100979
trainer/Bellman Errors Mean          6346.63
trainer/Bellman Errors Std          17378.1
trainer/Bellman Errors Max         126734
trainer/Bellman Errors Min              0.198086
trainer/Policy Action Mean             -0.123799
trainer/Policy Action Std               0.885692
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         59000
exploration/num paths total          1039
exploration/path length Mean          125
exploration/path length Std            57.0614
exploration/path length Max           231
exploration/path length Min            20
exploration/Rewards Mean                1.2377
exploration/Rewards Std                 0.706465
exploration/Rewards Max                 5.34996
exploration/Rewards Min                -0.166609
exploration/Returns Mean              154.712
exploration/Returns Std                83.9974
exploration/Returns Max               326.023
exploration/Returns Min                19.2626
exploration/Actions Mean                0.126841
exploration/Actions Std                 0.686456
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                   8
exploration/Average Returns           154.712
evaluation/num steps total          43526
evaluation/num paths total            334
evaluation/path length Mean           628
evaluation/path length Std              0
evaluation/path length Max            628
evaluation/path length Min            628
evaluation/Rewards Mean                 1.89691
evaluation/Rewards Std                  0.761276
evaluation/Rewards Max                  4.25457
evaluation/Rewards Min                  0.405484
evaluation/Returns Mean              1191.26
evaluation/Returns Std                  0
evaluation/Returns Max               1191.26
evaluation/Returns Min               1191.26
evaluation/Actions Mean                 0.0631314
evaluation/Actions Std                  0.762896
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    1
evaluation/Average Returns           1191.26
time/data storing (s)                   0.00341063
time/evaluation sampling (s)            0.29666
time/exploration sampling (s)           0.261919
time/logging (s)                        0.0051371
time/saving (s)                         0.00205737
time/training (s)                       6.53539
time/epoch (s)                          7.10458
time/total (s)                        347.211
Epoch                                  48
---------------------------------  ---------------
2021-07-02 23:44:09.491973 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 49 finished
---------------------------------  --------------
replay_buffer/size                 60000
trainer/QF Loss                     3731.34
trainer/Policy Loss                -2137.15
trainer/Raw Policy Loss            -2137.15
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean          2097.97
trainer/Q Predictions Std            819.418
trainer/Q Predictions Max           3117.58
trainer/Q Predictions Min             15.5067
trainer/Q Targets Mean              2094.64
trainer/Q Targets Std                820.594
trainer/Q Targets Max               3147.61
trainer/Q Targets Min                 -0.790951
trainer/Bellman Errors Mean         3731.34
trainer/Bellman Errors Std          8035.25
trainer/Bellman Errors Max         51587.2
trainer/Bellman Errors Min             0.0574782
trainer/Policy Action Mean             0.0223227
trainer/Policy Action Std              0.906065
trainer/Policy Action Max              1
trainer/Policy Action Min             -1
exploration/num steps total        60000
exploration/num paths total         1042
exploration/path length Mean         333.333
exploration/path length Std           30.4448
exploration/path length Max          376
exploration/path length Min          307
exploration/Rewards Mean               1.68488
exploration/Rewards Std                0.777651
exploration/Rewards Max                3.94912
exploration/Rewards Min                0.235127
exploration/Returns Mean             561.628
exploration/Returns Std              154.636
exploration/Returns Max              745.331
exploration/Returns Min              367.023
exploration/Actions Mean               0.22495
exploration/Actions Std                0.688276
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                  3
exploration/Average Returns          561.628
evaluation/num steps total         44428
evaluation/num paths total           336
evaluation/path length Mean          451
evaluation/path length Std            57
evaluation/path length Max           508
evaluation/path length Min           394
evaluation/Rewards Mean                2.54603
evaluation/Rewards Std                 1.14608
evaluation/Rewards Max                 5.47148
evaluation/Rewards Min                 0.437637
evaluation/Returns Mean             1148.26
evaluation/Returns Std               203.759
evaluation/Returns Max              1352.02
evaluation/Returns Min               944.499
evaluation/Actions Mean                0.275763
evaluation/Actions Std                 0.811234
evaluation/Actions Max                 1
evaluation/Actions Min                -1
evaluation/Num Paths                   2
evaluation/Average Returns          1148.26
time/data storing (s)                  0.00355722
time/evaluation sampling (s)           0.294971
time/exploration sampling (s)          0.267327
time/logging (s)                       0.00502291
time/saving (s)                        0.00160432
time/training (s)                      6.31802
time/epoch (s)                         6.8905
time/total (s)                       354.104
Epoch                                 49
---------------------------------  --------------
2021-07-02 23:44:16.623427 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 50 finished
---------------------------------  --------------
replay_buffer/size                 61000
trainer/QF Loss                     4878.08
trainer/Policy Loss                -2113.35
trainer/Raw Policy Loss            -2113.35
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean          2048.19
trainer/Q Predictions Std            872.331
trainer/Q Predictions Max           2890.9
trainer/Q Predictions Min             -3.95027
trainer/Q Targets Mean              2051.18
trainer/Q Targets Std                872.97
trainer/Q Targets Max               2905.02
trainer/Q Targets Min                -72.9471
trainer/Bellman Errors Mean         4878.08
trainer/Bellman Errors Std         11618.7
trainer/Bellman Errors Max         62840.5
trainer/Bellman Errors Min             0.156716
trainer/Policy Action Mean            -0.0386562
trainer/Policy Action Std              0.900683
trainer/Policy Action Max              1
trainer/Policy Action Min             -1
exploration/num steps total        61000
exploration/num paths total         1047
exploration/path length Mean         200
exploration/path length Std          177.268
exploration/path length Max          411
exploration/path length Min           13
exploration/Rewards Mean               1.52627
exploration/Rewards Std                0.541277
exploration/Rewards Max                3.31081
exploration/Rewards Min                0.231818
exploration/Returns Mean             305.255
exploration/Returns Std              286.407
exploration/Returns Max              730.886
exploration/Returns Min               12.7777
exploration/Actions Mean               0.097966
exploration/Actions Std                0.673788
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                  5
exploration/Average Returns          305.255
evaluation/num steps total         45389
evaluation/num paths total           337
evaluation/path length Mean          961
evaluation/path length Std             0
evaluation/path length Max           961
evaluation/path length Min           961
evaluation/Rewards Mean                1.65662
evaluation/Rewards Std                 0.641052
evaluation/Rewards Max                 3.98628
evaluation/Rewards Min                 0.34331
evaluation/Returns Mean             1592.01
evaluation/Returns Std                 0
evaluation/Returns Max              1592.01
evaluation/Returns Min              1592.01
evaluation/Actions Mean                0.0092109
evaluation/Actions Std                 0.783539
evaluation/Actions Max                 1
evaluation/Actions Min                -1
evaluation/Num Paths                   1
evaluation/Average Returns          1592.01
time/data storing (s)                  0.00339212
time/evaluation sampling (s)           0.270088
time/exploration sampling (s)          0.25039
time/logging (s)                       0.0037855
time/saving (s)                        0.00134589
time/training (s)                      6.59862
time/epoch (s)                         7.12762
time/total (s)                       361.233
Epoch                                 50
---------------------------------  --------------
2021-07-02 23:44:23.366939 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 51 finished
---------------------------------  ---------------
replay_buffer/size                  62000
trainer/QF Loss                      5539.51
trainer/Policy Loss                 -2140.27
trainer/Raw Policy Loss             -2140.27
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2080.4
trainer/Q Predictions Std             882.876
trainer/Q Predictions Max            2836.48
trainer/Q Predictions Min             -21.6659
trainer/Q Targets Mean               2062.66
trainer/Q Targets Std                 877.628
trainer/Q Targets Max                2799.17
trainer/Q Targets Min                 -17.457
trainer/Bellman Errors Mean          5539.51
trainer/Bellman Errors Std          13754.3
trainer/Bellman Errors Max         107309
trainer/Bellman Errors Min              0.636178
trainer/Policy Action Mean             -0.0600083
trainer/Policy Action Std               0.906452
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         62000
exploration/num paths total          1053
exploration/path length Mean          166.667
exploration/path length Std           119.973
exploration/path length Max           324
exploration/path length Min             9
exploration/Rewards Mean                1.77533
exploration/Rewards Std                 0.689775
exploration/Rewards Max                 5.12876
exploration/Rewards Min                 0.534748
exploration/Returns Mean              295.889
exploration/Returns Std               209.845
exploration/Returns Max               537.064
exploration/Returns Min                 6.51283
exploration/Actions Mean                0.132054
exploration/Actions Std                 0.717046
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                   6
exploration/Average Returns           295.889
evaluation/num steps total          46389
evaluation/num paths total            338
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 1.79731
evaluation/Rewards Std                  0.506997
evaluation/Rewards Max                  3.61432
evaluation/Rewards Min                  0.557556
evaluation/Returns Mean              1797.31
evaluation/Returns Std                  0
evaluation/Returns Max               1797.31
evaluation/Returns Min               1797.31
evaluation/Actions Mean                 0.227972
evaluation/Actions Std                  0.792353
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    1
evaluation/Average Returns           1797.31
time/data storing (s)                   0.00333837
time/evaluation sampling (s)            0.242012
time/exploration sampling (s)           0.267667
time/logging (s)                        0.00447062
time/saving (s)                         0.001357
time/training (s)                       6.22326
time/epoch (s)                          6.74211
time/total (s)                        367.977
Epoch                                  51
---------------------------------  ---------------
2021-07-02 23:44:30.216780 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 52 finished
---------------------------------  --------------
replay_buffer/size                 63000
trainer/QF Loss                     3381.01
trainer/Policy Loss                -2150.1
trainer/Raw Policy Loss            -2150.1
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean          2092.33
trainer/Q Predictions Std            875.029
trainer/Q Predictions Max           2874.98
trainer/Q Predictions Min             13.6534
trainer/Q Targets Mean              2100.26
trainer/Q Targets Std                885.518
trainer/Q Targets Max               2850.22
trainer/Q Targets Min                  0.363033
trainer/Bellman Errors Mean         3381.01
trainer/Bellman Errors Std          9442.51
trainer/Bellman Errors Max         91733.1
trainer/Bellman Errors Min             0.351959
trainer/Policy Action Mean            -0.223874
trainer/Policy Action Std              0.870083
trainer/Policy Action Max              1
trainer/Policy Action Min             -1
exploration/num steps total        63000
exploration/num paths total         1060
exploration/path length Mean         142.857
exploration/path length Std          147.215
exploration/path length Max          461
exploration/path length Min           15
exploration/Rewards Mean               1.27182
exploration/Rewards Std                0.550498
exploration/Rewards Max                2.95542
exploration/Rewards Min               -0.206498
exploration/Returns Mean             181.689
exploration/Returns Std              200.84
exploration/Returns Max              641.724
exploration/Returns Min               15.2387
exploration/Actions Mean               0.179226
exploration/Actions Std                0.690628
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                  7
exploration/Average Returns          181.689
evaluation/num steps total         47389
evaluation/num paths total           339
evaluation/path length Mean         1000
evaluation/path length Std             0
evaluation/path length Max          1000
evaluation/path length Min          1000
evaluation/Rewards Mean                1.34402
evaluation/Rewards Std                 0.538384
evaluation/Rewards Max                 2.88095
evaluation/Rewards Min                 0.332104
evaluation/Returns Mean             1344.02
evaluation/Returns Std                 0
evaluation/Returns Max              1344.02
evaluation/Returns Min              1344.02
evaluation/Actions Mean                0.2312
evaluation/Actions Std                 0.76046
evaluation/Actions Max                 1
evaluation/Actions Min                -1
evaluation/Num Paths                   1
evaluation/Average Returns          1344.02
time/data storing (s)                  0.00331713
time/evaluation sampling (s)           0.283696
time/exploration sampling (s)          0.252422
time/logging (s)                       0.00626861
time/saving (s)                        0.00132823
time/training (s)                      6.30244
time/epoch (s)                         6.84947
time/total (s)                       374.828
Epoch                                 52
---------------------------------  --------------
2021-07-02 23:44:37.041246 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 53 finished
---------------------------------  --------------
replay_buffer/size                 64000
trainer/QF Loss                     3053.92
trainer/Policy Loss                -2238.11
trainer/Raw Policy Loss            -2238.11
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean          2204.65
trainer/Q Predictions Std            841.572
trainer/Q Predictions Max           3055.04
trainer/Q Predictions Min             -6.76015
trainer/Q Targets Mean              2200.29
trainer/Q Targets Std                837.605
trainer/Q Targets Max               3082.46
trainer/Q Targets Min                  0.195131
trainer/Bellman Errors Mean         3053.92
trainer/Bellman Errors Std          5923.23
trainer/Bellman Errors Max         37634.2
trainer/Bellman Errors Min             0.0150206
trainer/Policy Action Mean             0.0141744
trainer/Policy Action Std              0.886209
trainer/Policy Action Max              1
trainer/Policy Action Min             -1
exploration/num steps total        64000
exploration/num paths total         1065
exploration/path length Mean         200
exploration/path length Std          133.51
exploration/path length Max          353
exploration/path length Min           11
exploration/Rewards Mean               1.39108
exploration/Rewards Std                0.591301
exploration/Rewards Max                5.69659
exploration/Rewards Min                0.131467
exploration/Returns Mean             278.217
exploration/Returns Std              166.504
exploration/Returns Max              485.598
exploration/Returns Min                9.235
exploration/Actions Mean               0.167735
exploration/Actions Std                0.680965
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                  5
exploration/Average Returns          278.217
evaluation/num steps total         48389
evaluation/num paths total           340
evaluation/path length Mean         1000
evaluation/path length Std             0
evaluation/path length Max          1000
evaluation/path length Min          1000
evaluation/Rewards Mean                1.28799
evaluation/Rewards Std                 0.407543
evaluation/Rewards Max                 2.73257
evaluation/Rewards Min                 0.41011
evaluation/Returns Mean             1287.99
evaluation/Returns Std                 0
evaluation/Returns Max              1287.99
evaluation/Returns Min              1287.99
evaluation/Actions Mean                0.193099
evaluation/Actions Std                 0.746069
evaluation/Actions Max                 1
evaluation/Actions Min                -1
evaluation/Num Paths                   1
evaluation/Average Returns          1287.99
time/data storing (s)                  0.00338548
time/evaluation sampling (s)           0.313399
time/exploration sampling (s)          0.251718
time/logging (s)                       0.00521778
time/saving (s)                        0.00159068
time/training (s)                      6.24498
time/epoch (s)                         6.82029
time/total (s)                       381.651
Epoch                                 53
---------------------------------  --------------
2021-07-02 23:44:43.844205 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 54 finished
---------------------------------  ----------------
replay_buffer/size                  65000
trainer/QF Loss                     17380.4
trainer/Policy Loss                 -2289.88
trainer/Raw Policy Loss             -2289.88
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2230.99
trainer/Q Predictions Std             855.394
trainer/Q Predictions Max            3184.29
trainer/Q Predictions Min              39.3394
trainer/Q Targets Mean               2219.74
trainer/Q Targets Std                 861.233
trainer/Q Targets Max                3327.02
trainer/Q Targets Min                   1.39571
trainer/Bellman Errors Mean         17380.4
trainer/Bellman Errors Std         104925
trainer/Bellman Errors Max              1.13051e+06
trainer/Bellman Errors Min              8.58307e-06
trainer/Policy Action Mean              0.00173246
trainer/Policy Action Std               0.885625
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         65000
exploration/num paths total          1071
exploration/path length Mean          166.667
exploration/path length Std            86.4324
exploration/path length Max           311
exploration/path length Min            38
exploration/Rewards Mean                1.98489
exploration/Rewards Std                 0.941989
exploration/Rewards Max                 5.43917
exploration/Rewards Min                 0.167235
exploration/Returns Mean              330.815
exploration/Returns Std               195.757
exploration/Returns Max               607.474
exploration/Returns Min                22.4714
exploration/Actions Mean                0.107724
exploration/Actions Std                 0.70936
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                   6
exploration/Average Returns           330.815
evaluation/num steps total          49389
evaluation/num paths total            341
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 1.09618
evaluation/Rewards Std                  0.511065
evaluation/Rewards Max                  2.73013
evaluation/Rewards Min                  0.0809037
evaluation/Returns Mean              1096.18
evaluation/Returns Std                  0
evaluation/Returns Max               1096.18
evaluation/Returns Min               1096.18
evaluation/Actions Mean                 0.0418065
evaluation/Actions Std                  0.714204
evaluation/Actions Max                  1
evaluation/Actions Min                 -0.999999
evaluation/Num Paths                    1
evaluation/Average Returns           1096.18
time/data storing (s)                   0.0044015
time/evaluation sampling (s)            0.256177
time/exploration sampling (s)           0.250454
time/logging (s)                        0.00401106
time/saving (s)                         0.0016528
time/training (s)                       6.28243
time/epoch (s)                          6.79913
time/total (s)                        388.452
Epoch                                  54
---------------------------------  ----------------
2021-07-02 23:44:50.716360 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 55 finished
---------------------------------  --------------
replay_buffer/size                 66000
trainer/QF Loss                     4266.65
trainer/Policy Loss                -2242.27
trainer/Raw Policy Loss            -2242.27
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean          2185.14
trainer/Q Predictions Std            871.517
trainer/Q Predictions Max           3046.13
trainer/Q Predictions Min            -90.1126
trainer/Q Targets Mean              2195.74
trainer/Q Targets Std                872.234
trainer/Q Targets Max               2981.09
trainer/Q Targets Min                 -0.384073
trainer/Bellman Errors Mean         4266.65
trainer/Bellman Errors Std          9259.96
trainer/Bellman Errors Max         83062.6
trainer/Bellman Errors Min             0.0317089
trainer/Policy Action Mean            -0.172396
trainer/Policy Action Std              0.866561
trainer/Policy Action Max              1
trainer/Policy Action Min             -1
exploration/num steps total        66000
exploration/num paths total         1086
exploration/path length Mean          66.6667
exploration/path length Std           38.7103
exploration/path length Max          159
exploration/path length Min           14
exploration/Rewards Mean               0.733442
exploration/Rewards Std                0.276594
exploration/Rewards Max                1.53069
exploration/Rewards Min               -0.198764
exploration/Returns Mean              48.8961
exploration/Returns Std               36.3878
exploration/Returns Max              127.661
exploration/Returns Min               10.8724
exploration/Actions Mean               0.124817
exploration/Actions Std                0.539063
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                 15
exploration/Average Returns           48.8961
evaluation/num steps total         49897
evaluation/num paths total           342
evaluation/path length Mean          508
evaluation/path length Std             0
evaluation/path length Max           508
evaluation/path length Min           508
evaluation/Rewards Mean                0.938972
evaluation/Rewards Std                 0.122074
evaluation/Rewards Max                 1.17981
evaluation/Rewards Min                 0.285568
evaluation/Returns Mean              476.998
evaluation/Returns Std                 0
evaluation/Returns Max               476.998
evaluation/Returns Min               476.998
evaluation/Actions Mean                0.504781
evaluation/Actions Std                 0.50027
evaluation/Actions Max                 0.999673
evaluation/Actions Min                -0.99825
evaluation/Num Paths                   1
evaluation/Average Returns           476.998
time/data storing (s)                  0.00333209
time/evaluation sampling (s)           0.254005
time/exploration sampling (s)          0.266208
time/logging (s)                       0.00337859
time/saving (s)                        0.0019141
time/training (s)                      6.34042
time/epoch (s)                         6.86926
time/total (s)                       395.323
Epoch                                 55
---------------------------------  --------------
2021-07-02 23:44:58.020383 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 56 finished
---------------------------------  ---------------
replay_buffer/size                  67000
trainer/QF Loss                      8192.64
trainer/Policy Loss                 -2265.55
trainer/Raw Policy Loss             -2265.55
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2190.71
trainer/Q Predictions Std             917.403
trainer/Q Predictions Max            3026.44
trainer/Q Predictions Min             -85.9373
trainer/Q Targets Mean               2202.31
trainer/Q Targets Std                 919.058
trainer/Q Targets Max                3020.47
trainer/Q Targets Min                 -56.2863
trainer/Bellman Errors Mean          8192.64
trainer/Bellman Errors Std          49834
trainer/Bellman Errors Max         549937
trainer/Bellman Errors Min              0.016933
trainer/Policy Action Mean              0.0517799
trainer/Policy Action Std               0.90811
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         67000
exploration/num paths total          1095
exploration/path length Mean          111.111
exploration/path length Std            93.4505
exploration/path length Max           333
exploration/path length Min            27
exploration/Rewards Mean                1.04706
exploration/Rewards Std                 0.471259
exploration/Rewards Max                 3.4792
exploration/Rewards Min                -0.280183
exploration/Returns Mean              116.34
exploration/Returns Std                93.5643
exploration/Returns Max               335.122
exploration/Returns Min                29.4853
exploration/Actions Mean                0.151133
exploration/Actions Std                 0.631679
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                   9
exploration/Average Returns           116.34
evaluation/num steps total          50897
evaluation/num paths total            343
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 0.990165
evaluation/Rewards Std                  0.199358
evaluation/Rewards Max                  1.81363
evaluation/Rewards Min                  0.484011
evaluation/Returns Mean               990.165
evaluation/Returns Std                  0
evaluation/Returns Max                990.165
evaluation/Returns Min                990.165
evaluation/Actions Mean                 0.0716545
evaluation/Actions Std                  0.619941
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    1
evaluation/Average Returns            990.165
time/data storing (s)                   0.0033366
time/evaluation sampling (s)            0.260816
time/exploration sampling (s)           0.282602
time/logging (s)                        0.0057302
time/saving (s)                         0.00206926
time/training (s)                       6.74972
time/epoch (s)                          7.30428
time/total (s)                        402.629
Epoch                                  56
---------------------------------  ---------------
2021-07-02 23:45:05.099001 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 57 finished
---------------------------------  ---------------
replay_buffer/size                  68000
trainer/QF Loss                      7291.86
trainer/Policy Loss                 -2344.31
trainer/Raw Policy Loss             -2344.31
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2277.24
trainer/Q Predictions Std             845.598
trainer/Q Predictions Max            3323.9
trainer/Q Predictions Min             -90.5399
trainer/Q Targets Mean               2264.7
trainer/Q Targets Std                 846.564
trainer/Q Targets Max                3387.78
trainer/Q Targets Min                 -84.4895
trainer/Bellman Errors Mean          7291.86
trainer/Bellman Errors Std          26155.1
trainer/Bellman Errors Max         243048
trainer/Bellman Errors Min              1.77496
trainer/Policy Action Mean              0.043747
trainer/Policy Action Std               0.895927
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         68000
exploration/num paths total          1101
exploration/path length Mean          166.667
exploration/path length Std            80.307
exploration/path length Max           274
exploration/path length Min            42
exploration/Rewards Mean                1.57577
exploration/Rewards Std                 0.572346
exploration/Rewards Max                 3.27728
exploration/Rewards Min                 0.352955
exploration/Returns Mean              262.628
exploration/Returns Std               111.181
exploration/Returns Max               396.156
exploration/Returns Min                55.8129
exploration/Actions Mean                0.137705
exploration/Actions Std                 0.683841
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                   6
exploration/Average Returns           262.628
evaluation/num steps total          51630
evaluation/num paths total            346
evaluation/path length Mean           244.333
evaluation/path length Std             15.1511
evaluation/path length Max            262
evaluation/path length Min            225
evaluation/Rewards Mean                 2.09908
evaluation/Rewards Std                  0.70999
evaluation/Rewards Max                  4.0118
evaluation/Rewards Min                  0.65522
evaluation/Returns Mean               512.875
evaluation/Returns Std                 35.7599
evaluation/Returns Max                561.601
evaluation/Returns Min                476.788
evaluation/Actions Mean                 0.142158
evaluation/Actions Std                  0.78915
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    3
evaluation/Average Returns            512.875
time/data storing (s)                   0.0033492
time/evaluation sampling (s)            0.27789
time/exploration sampling (s)           0.252923
time/logging (s)                        0.00369249
time/saving (s)                         0.00170784
time/training (s)                       6.53389
time/epoch (s)                          7.07346
time/total (s)                        409.704
Epoch                                  57
---------------------------------  ---------------
2021-07-02 23:45:11.924547 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 58 finished
---------------------------------  --------------
replay_buffer/size                 69000
trainer/QF Loss                     4380.15
trainer/Policy Loss                -2326.33
trainer/Raw Policy Loss            -2326.33
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean          2260.63
trainer/Q Predictions Std            870.071
trainer/Q Predictions Max           3235
trainer/Q Predictions Min           -169.14
trainer/Q Targets Mean              2273.17
trainer/Q Targets Std                863.195
trainer/Q Targets Max               3202.31
trainer/Q Targets Min                 -1.21045
trainer/Bellman Errors Mean         4380.16
trainer/Bellman Errors Std          8603.27
trainer/Bellman Errors Max         67570.4
trainer/Bellman Errors Min             0.285609
trainer/Policy Action Mean             0.0188528
trainer/Policy Action Std              0.883953
trainer/Policy Action Max              1
trainer/Policy Action Min             -1
exploration/num steps total        69000
exploration/num paths total         1111
exploration/path length Mean         100
exploration/path length Std           75.0453
exploration/path length Max          232
exploration/path length Min            8
exploration/Rewards Mean               1.90277
exploration/Rewards Std                0.835574
exploration/Rewards Max                4.74011
exploration/Rewards Min                0.489274
exploration/Returns Mean             190.277
exploration/Returns Std              163.569
exploration/Returns Max              544.704
exploration/Returns Min                6.73551
exploration/Actions Mean               0.204936
exploration/Actions Std                0.68272
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                 10
exploration/Average Returns          190.277
evaluation/num steps total         52496
evaluation/num paths total           348
evaluation/path length Mean          433
evaluation/path length Std           201
evaluation/path length Max           634
evaluation/path length Min           232
evaluation/Rewards Mean                2.57098
evaluation/Rewards Std                 1.0427
evaluation/Rewards Max                 4.91481
evaluation/Rewards Min                 0.553212
evaluation/Returns Mean             1113.24
evaluation/Returns Std               391.444
evaluation/Returns Max              1504.68
evaluation/Returns Min               721.792
evaluation/Actions Mean                0.326129
evaluation/Actions Std                 0.773553
evaluation/Actions Max                 1
evaluation/Actions Min                -1
evaluation/Num Paths                   2
evaluation/Average Returns          1113.24
time/data storing (s)                  0.00339162
time/evaluation sampling (s)           0.287946
time/exploration sampling (s)          0.275695
time/logging (s)                       0.00385667
time/saving (s)                        0.00124713
time/training (s)                      6.25133
time/epoch (s)                         6.82346
time/total (s)                       416.53
Epoch                                 58
---------------------------------  --------------
2021-07-02 23:45:18.947308 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 59 finished
---------------------------------  ----------------
replay_buffer/size                  70000
trainer/QF Loss                     19733
trainer/Policy Loss                 -2427.37
trainer/Raw Policy Loss             -2427.37
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2351.84
trainer/Q Predictions Std             706.686
trainer/Q Predictions Max            3038.56
trainer/Q Predictions Min              75.2537
trainer/Q Targets Mean               2338.53
trainer/Q Targets Std                 726.56
trainer/Q Targets Max                3000.39
trainer/Q Targets Min                   1.58446
trainer/Bellman Errors Mean         19732.9
trainer/Bellman Errors Std         138517
trainer/Bellman Errors Max              1.36244e+06
trainer/Bellman Errors Min              0.0265175
trainer/Policy Action Mean             -0.234447
trainer/Policy Action Std               0.866922
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         70000
exploration/num paths total          1130
exploration/path length Mean           52.6316
exploration/path length Std            43.4586
exploration/path length Max           170
exploration/path length Min             7
exploration/Rewards Mean                1.19262
exploration/Rewards Std                 0.626109
exploration/Rewards Max                 3.26765
exploration/Rewards Min                 0.0226535
exploration/Returns Mean               62.7696
exploration/Returns Std                67.1735
exploration/Returns Max               299.065
exploration/Returns Min                 4.2956
exploration/Actions Mean                0.102223
exploration/Actions Std                 0.664012
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  19
exploration/Average Returns            62.7696
evaluation/num steps total          53496
evaluation/num paths total            349
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 0.989077
evaluation/Rewards Std                  0.233473
evaluation/Rewards Max                  1.60773
evaluation/Rewards Min                  0.461816
evaluation/Returns Mean               989.077
evaluation/Returns Std                  0
evaluation/Returns Max                989.077
evaluation/Returns Min                989.077
evaluation/Actions Mean                 0.0183995
evaluation/Actions Std                  0.668896
evaluation/Actions Max                  0.999923
evaluation/Actions Min                 -0.999899
evaluation/Num Paths                    1
evaluation/Average Returns            989.077
time/data storing (s)                   0.00333834
time/evaluation sampling (s)            0.246422
time/exploration sampling (s)           0.259371
time/logging (s)                        0.00393023
time/saving (s)                         0.00125256
time/training (s)                       6.50641
time/epoch (s)                          7.02073
time/total (s)                        423.552
Epoch                                  59
---------------------------------  ----------------
2021-07-02 23:45:25.824009 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 60 finished
---------------------------------  ---------------
replay_buffer/size                  71000
trainer/QF Loss                      4954.66
trainer/Policy Loss                 -2418.61
trainer/Raw Policy Loss             -2418.61
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2350.01
trainer/Q Predictions Std             763.889
trainer/Q Predictions Max            2992.75
trainer/Q Predictions Min            -146.958
trainer/Q Targets Mean               2372.39
trainer/Q Targets Std                 770.692
trainer/Q Targets Max                3022.78
trainer/Q Targets Min                -144.041
trainer/Bellman Errors Mean          4954.66
trainer/Bellman Errors Std          13772.7
trainer/Bellman Errors Max         123669
trainer/Bellman Errors Min              0.00637347
trainer/Policy Action Mean             -0.115584
trainer/Policy Action Std               0.913436
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         71000
exploration/num paths total          1145
exploration/path length Mean           66.6667
exploration/path length Std            87.1127
exploration/path length Max           382
exploration/path length Min            10
exploration/Rewards Mean                1.04277
exploration/Rewards Std                 0.574449
exploration/Rewards Max                 2.93581
exploration/Rewards Min                -0.259574
exploration/Returns Mean               69.5181
exploration/Returns Std               100.817
exploration/Returns Max               420.718
exploration/Returns Min                 7.91993
exploration/Actions Mean                0.148821
exploration/Actions Std                 0.666077
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  15
exploration/Average Returns            69.5181
evaluation/num steps total          54484
evaluation/num paths total            350
evaluation/path length Mean           988
evaluation/path length Std              0
evaluation/path length Max            988
evaluation/path length Min            988
evaluation/Rewards Mean                 1.5879
evaluation/Rewards Std                  0.85726
evaluation/Rewards Max                  4.97025
evaluation/Rewards Min                  0.288985
evaluation/Returns Mean              1568.84
evaluation/Returns Std                  0
evaluation/Returns Max               1568.84
evaluation/Returns Min               1568.84
evaluation/Actions Mean                 0.166168
evaluation/Actions Std                  0.740996
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    1
evaluation/Average Returns           1568.84
time/data storing (s)                   0.00337545
time/evaluation sampling (s)            0.240798
time/exploration sampling (s)           0.259321
time/logging (s)                        0.00382026
time/saving (s)                         0.0012478
time/training (s)                       6.3658
time/epoch (s)                          6.87436
time/total (s)                        430.428
Epoch                                  60
---------------------------------  ---------------
2021-07-02 23:45:32.600576 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 61 finished
---------------------------------  ---------------
replay_buffer/size                  72000
trainer/QF Loss                      4079.23
trainer/Policy Loss                 -2386.49
trainer/Raw Policy Loss             -2386.49
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2348.91
trainer/Q Predictions Std             794.406
trainer/Q Predictions Max            3056.68
trainer/Q Predictions Min             -49.5312
trainer/Q Targets Mean               2353.68
trainer/Q Targets Std                 807.291
trainer/Q Targets Max                3035.72
trainer/Q Targets Min                  -0.261829
trainer/Bellman Errors Mean          4079.23
trainer/Bellman Errors Std          13976.5
trainer/Bellman Errors Max         124259
trainer/Bellman Errors Min              0.205322
trainer/Policy Action Mean              0.0346318
trainer/Policy Action Std               0.888307
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         72000
exploration/num paths total          1151
exploration/path length Mean          166.667
exploration/path length Std           157.057
exploration/path length Max           475
exploration/path length Min            17
exploration/Rewards Mean                1.4553
exploration/Rewards Std                 0.63414
exploration/Rewards Max                 3.38782
exploration/Rewards Min                 0.305424
exploration/Returns Mean              242.55
exploration/Returns Std               261.012
exploration/Returns Max               770.416
exploration/Returns Min                14.177
exploration/Actions Mean                0.141445
exploration/Actions Std                 0.694185
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                   6
exploration/Average Returns           242.55
evaluation/num steps total          55484
evaluation/num paths total            351
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 0.976784
evaluation/Rewards Std                  0.277285
evaluation/Rewards Max                  1.72176
evaluation/Rewards Min                  0.546106
evaluation/Returns Mean               976.784
evaluation/Returns Std                  0
evaluation/Returns Max                976.784
evaluation/Returns Min                976.784
evaluation/Actions Mean                 0.128312
evaluation/Actions Std                  0.717989
evaluation/Actions Max                  1
evaluation/Actions Min                 -0.999954
evaluation/Num Paths                    1
evaluation/Average Returns            976.784
time/data storing (s)                   0.00338476
time/evaluation sampling (s)            0.26219
time/exploration sampling (s)           0.261977
time/logging (s)                        0.00384131
time/saving (s)                         0.00126138
time/training (s)                       6.24181
time/epoch (s)                          6.77447
time/total (s)                        437.204
Epoch                                  61
---------------------------------  ---------------
2021-07-02 23:45:39.758604 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 62 finished
---------------------------------  ----------------
replay_buffer/size                  73000
trainer/QF Loss                     13335.2
trainer/Policy Loss                 -2430.6
trainer/Raw Policy Loss             -2430.6
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2384.03
trainer/Q Predictions Std             727.306
trainer/Q Predictions Max            2967.43
trainer/Q Predictions Min             -34.9092
trainer/Q Targets Mean               2392.25
trainer/Q Targets Std                 718.244
trainer/Q Targets Max                2984.68
trainer/Q Targets Min                   0.395061
trainer/Bellman Errors Mean         13335.2
trainer/Bellman Errors Std         105226
trainer/Bellman Errors Max              1.18966e+06
trainer/Bellman Errors Min              0.053454
trainer/Policy Action Mean             -0.218239
trainer/Policy Action Std               0.896972
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         73000
exploration/num paths total          1164
exploration/path length Mean           76.9231
exploration/path length Std            58.6692
exploration/path length Max           209
exploration/path length Min            17
exploration/Rewards Mean                1.01066
exploration/Rewards Std                 0.451549
exploration/Rewards Max                 2.66869
exploration/Rewards Min                 0.0949549
exploration/Returns Mean               77.743
exploration/Returns Std                71.6953
exploration/Returns Max               274.846
exploration/Returns Min                12.0404
exploration/Actions Mean                0.156463
exploration/Actions Std                 0.635309
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  13
exploration/Average Returns            77.743
evaluation/num steps total          56484
evaluation/num paths total            352
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 0.984075
evaluation/Rewards Std                  0.217785
evaluation/Rewards Max                  1.53159
evaluation/Rewards Min                  0.461838
evaluation/Returns Mean               984.075
evaluation/Returns Std                  0
evaluation/Returns Max                984.075
evaluation/Returns Min                984.075
evaluation/Actions Mean                 0.157379
evaluation/Actions Std                  0.624755
evaluation/Actions Max                  1
evaluation/Actions Min                 -0.999856
evaluation/Num Paths                    1
evaluation/Average Returns            984.075
time/data storing (s)                   0.00330702
time/evaluation sampling (s)            0.26051
time/exploration sampling (s)           0.264367
time/logging (s)                        0.00395271
time/saving (s)                         0.00130098
time/training (s)                       6.62246
time/epoch (s)                          7.15589
time/total (s)                        444.362
Epoch                                  62
---------------------------------  ----------------
2021-07-02 23:45:47.014537 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 63 finished
---------------------------------  --------------
replay_buffer/size                 74000
trainer/QF Loss                     2679.52
trainer/Policy Loss                -2285
trainer/Raw Policy Loss            -2285
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean          2254.56
trainer/Q Predictions Std            807.835
trainer/Q Predictions Max           3024.31
trainer/Q Predictions Min             -1.72922
trainer/Q Targets Mean              2273.39
trainer/Q Targets Std                811.293
trainer/Q Targets Max               3128.67
trainer/Q Targets Min                  1.23432
trainer/Bellman Errors Mean         2679.52
trainer/Bellman Errors Std          6242.71
trainer/Bellman Errors Max         53930.5
trainer/Bellman Errors Min             0.018692
trainer/Policy Action Mean            -0.36486
trainer/Policy Action Std              0.823086
trainer/Policy Action Max              1
trainer/Policy Action Min             -1
exploration/num steps total        74000
exploration/num paths total         1183
exploration/path length Mean          52.6316
exploration/path length Std           53.7971
exploration/path length Max          222
exploration/path length Min            8
exploration/Rewards Mean               0.816169
exploration/Rewards Std                0.376968
exploration/Rewards Max                2.2159
exploration/Rewards Min                0.193075
exploration/Returns Mean              42.9563
exploration/Returns Std               56.6433
exploration/Returns Max              221.643
exploration/Returns Min                5.53211
exploration/Actions Mean               0.0639873
exploration/Actions Std                0.573419
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                 19
exploration/Average Returns           42.9563
evaluation/num steps total         57484
evaluation/num paths total           353
evaluation/path length Mean         1000
evaluation/path length Std             0
evaluation/path length Max          1000
evaluation/path length Min          1000
evaluation/Rewards Mean                0.976631
evaluation/Rewards Std                 0.206104
evaluation/Rewards Max                 1.47343
evaluation/Rewards Min                 0.440502
evaluation/Returns Mean              976.631
evaluation/Returns Std                 0
evaluation/Returns Max               976.631
evaluation/Returns Min               976.631
evaluation/Actions Mean                0.124177
evaluation/Actions Std                 0.50602
evaluation/Actions Max                 0.999925
evaluation/Actions Min                -0.999853
evaluation/Num Paths                   1
evaluation/Average Returns           976.631
time/data storing (s)                  0.0033967
time/evaluation sampling (s)           0.258434
time/exploration sampling (s)          0.261617
time/logging (s)                       0.00425096
time/saving (s)                        0.00155453
time/training (s)                      6.72467
time/epoch (s)                         7.25392
time/total (s)                       451.617
Epoch                                 63
---------------------------------  --------------
2021-07-02 23:45:54.070770 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 64 finished
---------------------------------  ---------------
replay_buffer/size                  75000
trainer/QF Loss                      5272.58
trainer/Policy Loss                 -2274.45
trainer/Raw Policy Loss             -2274.45
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2224.92
trainer/Q Predictions Std             799.128
trainer/Q Predictions Max            3033.08
trainer/Q Predictions Min            -298.287
trainer/Q Targets Mean               2215.19
trainer/Q Targets Std                 791.913
trainer/Q Targets Max                2970.57
trainer/Q Targets Min                   1.00639
trainer/Bellman Errors Mean          5272.58
trainer/Bellman Errors Std          14785.6
trainer/Bellman Errors Max         107659
trainer/Bellman Errors Min              0.139346
trainer/Policy Action Mean             -0.0199506
trainer/Policy Action Std               0.885895
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         75000
exploration/num paths total          1194
exploration/path length Mean           90.9091
exploration/path length Std            51.9413
exploration/path length Max           185
exploration/path length Min            12
exploration/Rewards Mean                0.8836
exploration/Rewards Std                 0.425697
exploration/Rewards Max                 2.5453
exploration/Rewards Min                -0.337084
exploration/Returns Mean               80.3273
exploration/Returns Std                55.8301
exploration/Returns Max               187.07
exploration/Returns Min                 8.97575
exploration/Actions Mean                0.12508
exploration/Actions Std                 0.586104
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  11
exploration/Average Returns            80.3273
evaluation/num steps total          58484
evaluation/num paths total            354
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 0.992161
evaluation/Rewards Std                  0.0848166
evaluation/Rewards Max                  1.63344
evaluation/Rewards Min                  0.398267
evaluation/Returns Mean               992.161
evaluation/Returns Std                  0
evaluation/Returns Max                992.161
evaluation/Returns Min                992.161
evaluation/Actions Mean                 0.0792109
evaluation/Actions Std                  0.185296
evaluation/Actions Max                  0.999997
evaluation/Actions Min                 -0.999899
evaluation/Num Paths                    1
evaluation/Average Returns            992.161
time/data storing (s)                   0.0033202
time/evaluation sampling (s)            0.287789
time/exploration sampling (s)           0.260249
time/logging (s)                        0.00403494
time/saving (s)                         0.00127544
time/training (s)                       6.49709
time/epoch (s)                          7.05376
time/total (s)                        458.673
Epoch                                  64
---------------------------------  ---------------
2021-07-02 23:46:00.902577 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 65 finished
---------------------------------  ----------------
replay_buffer/size                  76000
trainer/QF Loss                     30641.7
trainer/Policy Loss                 -2292.92
trainer/Raw Policy Loss             -2292.92
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2199.24
trainer/Q Predictions Std             788.282
trainer/Q Predictions Max            2934.42
trainer/Q Predictions Min              -8.93038
trainer/Q Targets Mean               2205.53
trainer/Q Targets Std                 809.13
trainer/Q Targets Max                2989.4
trainer/Q Targets Min                 -47.1431
trainer/Bellman Errors Mean         30641.7
trainer/Bellman Errors Std         160919
trainer/Bellman Errors Max              1.25468e+06
trainer/Bellman Errors Min              0.337057
trainer/Policy Action Mean              0.0716064
trainer/Policy Action Std               0.901881
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         76000
exploration/num paths total          1207
exploration/path length Mean           76.9231
exploration/path length Std            93.1586
exploration/path length Max           376
exploration/path length Min            16
exploration/Rewards Mean                0.817301
exploration/Rewards Std                 0.364768
exploration/Rewards Max                 2.21008
exploration/Rewards Min                 0.084063
exploration/Returns Mean               62.8693
exploration/Returns Std                84.8133
exploration/Returns Max               330.56
exploration/Returns Min                10.9204
exploration/Actions Mean                0.133705
exploration/Actions Std                 0.591957
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  13
exploration/Average Returns            62.8693
evaluation/num steps total          59484
evaluation/num paths total            355
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 0.97468
evaluation/Rewards Std                  0.344099
evaluation/Rewards Max                  1.80897
evaluation/Rewards Min                  0.31953
evaluation/Returns Mean               974.68
evaluation/Returns Std                  0
evaluation/Returns Max                974.68
evaluation/Returns Min                974.68
evaluation/Actions Mean                 0.158392
evaluation/Actions Std                  0.53486
evaluation/Actions Max                  0.999989
evaluation/Actions Min                 -0.999554
evaluation/Num Paths                    1
evaluation/Average Returns            974.68
time/data storing (s)                   0.00336471
time/evaluation sampling (s)            0.243697
time/exploration sampling (s)           0.25867
time/logging (s)                        0.00533282
time/saving (s)                         0.00161566
time/training (s)                       6.31825
time/epoch (s)                          6.83093
time/total (s)                        465.505
Epoch                                  65
---------------------------------  ----------------
2021-07-02 23:46:08.464699 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 66 finished
---------------------------------  ---------------
replay_buffer/size                  77000
trainer/QF Loss                      5077.79
trainer/Policy Loss                 -2461.26
trainer/Raw Policy Loss             -2461.26
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2388.84
trainer/Q Predictions Std             663.612
trainer/Q Predictions Max            3084.03
trainer/Q Predictions Min              18.0205
trainer/Q Targets Mean               2400.47
trainer/Q Targets Std                 666.099
trainer/Q Targets Max                2992.88
trainer/Q Targets Min                   3.44381
trainer/Bellman Errors Mean          5077.79
trainer/Bellman Errors Std          20936.1
trainer/Bellman Errors Max         221218
trainer/Bellman Errors Min              0.288224
trainer/Policy Action Mean              0.0624984
trainer/Policy Action Std               0.903767
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         77000
exploration/num paths total          1218
exploration/path length Mean           90.9091
exploration/path length Std            85.9222
exploration/path length Max           288
exploration/path length Min            11
exploration/Rewards Mean                0.920757
exploration/Rewards Std                 0.353215
exploration/Rewards Max                 1.9014
exploration/Rewards Min                 0.189309
exploration/Returns Mean               83.7052
exploration/Returns Std                81.2762
exploration/Returns Max               266.239
exploration/Returns Min                 7.94171
exploration/Actions Mean                0.0849134
exploration/Actions Std                 0.556882
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  11
exploration/Average Returns            83.7052
evaluation/num steps total          60484
evaluation/num paths total            356
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 0.982795
evaluation/Rewards Std                  0.327473
evaluation/Rewards Max                  1.75031
evaluation/Rewards Min                  0.436378
evaluation/Returns Mean               982.795
evaluation/Returns Std                  0
evaluation/Returns Max                982.795
evaluation/Returns Min                982.795
evaluation/Actions Mean                 0.0792405
evaluation/Actions Std                  0.594355
evaluation/Actions Max                  0.999972
evaluation/Actions Min                 -0.999689
evaluation/Num Paths                    1
evaluation/Average Returns            982.795
time/data storing (s)                   0.00335675
time/evaluation sampling (s)            0.286189
time/exploration sampling (s)           0.260479
time/logging (s)                        0.00529501
time/saving (s)                         0.00162833
time/training (s)                       7.00234
time/epoch (s)                          7.55929
time/total (s)                        473.067
Epoch                                  66
---------------------------------  ---------------
2021-07-02 23:46:15.650699 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 67 finished
---------------------------------  ---------------
replay_buffer/size                  78000
trainer/QF Loss                     13632.2
trainer/Policy Loss                 -2251.61
trainer/Raw Policy Loss             -2251.61
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2180.42
trainer/Q Predictions Std             903.458
trainer/Q Predictions Max            3006.23
trainer/Q Predictions Min            -329.509
trainer/Q Targets Mean               2191.91
trainer/Q Targets Std                 902.205
trainer/Q Targets Max                2933.13
trainer/Q Targets Min                   0.0996485
trainer/Bellman Errors Mean         13632.2
trainer/Bellman Errors Std          71448.4
trainer/Bellman Errors Max         631294
trainer/Bellman Errors Min              0.210723
trainer/Policy Action Mean              0.103066
trainer/Policy Action Std               0.897754
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         78000
exploration/num paths total          1233
exploration/path length Mean           66.6667
exploration/path length Std            72.1246
exploration/path length Max           294
exploration/path length Min             7
exploration/Rewards Mean                1.19261
exploration/Rewards Std                 0.551816
exploration/Rewards Max                 2.92797
exploration/Rewards Min                 0.208343
exploration/Returns Mean               79.5074
exploration/Returns Std                90.0462
exploration/Returns Max               321.532
exploration/Returns Min                 5.22066
exploration/Actions Mean                0.127611
exploration/Actions Std                 0.617721
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  15
exploration/Average Returns            79.5074
evaluation/num steps total          61484
evaluation/num paths total            357
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 0.990045
evaluation/Rewards Std                  0.176308
evaluation/Rewards Max                  1.69144
evaluation/Rewards Min                  0.536805
evaluation/Returns Mean               990.045
evaluation/Returns Std                  0
evaluation/Returns Max                990.045
evaluation/Returns Min                990.045
evaluation/Actions Mean                 0.0865628
evaluation/Actions Std                  0.379109
evaluation/Actions Max                  0.999974
evaluation/Actions Min                 -0.99995
evaluation/Num Paths                    1
evaluation/Average Returns            990.045
time/data storing (s)                   0.00338137
time/evaluation sampling (s)            0.290564
time/exploration sampling (s)           0.258937
time/logging (s)                        0.00414622
time/saving (s)                         0.00128487
time/training (s)                       6.62377
time/epoch (s)                          7.18208
time/total (s)                        480.251
Epoch                                  67
---------------------------------  ---------------
2021-07-02 23:46:23.246678 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 68 finished
---------------------------------  ----------------
replay_buffer/size                  79000
trainer/QF Loss                     15964.2
trainer/Policy Loss                 -2267.65
trainer/Raw Policy Loss             -2267.65
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2198.4
trainer/Q Predictions Std             808.61
trainer/Q Predictions Max            2962.33
trainer/Q Predictions Min            -124.618
trainer/Q Targets Mean               2186.93
trainer/Q Targets Std                 829.565
trainer/Q Targets Max                3037.01
trainer/Q Targets Min                 -41.0018
trainer/Bellman Errors Mean         15964.2
trainer/Bellman Errors Std         100563
trainer/Bellman Errors Max              1.02133e+06
trainer/Bellman Errors Min              0.0189243
trainer/Policy Action Mean              0.115116
trainer/Policy Action Std               0.893323
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         79000
exploration/num paths total          1247
exploration/path length Mean           71.4286
exploration/path length Std            71.377
exploration/path length Max           201
exploration/path length Min             8
exploration/Rewards Mean                1.48709
exploration/Rewards Std                 0.924701
exploration/Rewards Max                 4.97467
exploration/Rewards Min                 0.0324633
exploration/Returns Mean              106.221
exploration/Returns Std               124.102
exploration/Returns Max               348.877
exploration/Returns Min                 6.32258
exploration/Actions Mean                0.125331
exploration/Actions Std                 0.70083
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  14
exploration/Average Returns           106.221
evaluation/num steps total          61963
evaluation/num paths total            361
evaluation/path length Mean           119.75
evaluation/path length Std              7.59523
evaluation/path length Max            127
evaluation/path length Min            107
evaluation/Rewards Mean                 2.33022
evaluation/Rewards Std                  0.885952
evaluation/Rewards Max                  4.65639
evaluation/Rewards Min                  0.590717
evaluation/Returns Mean               279.044
evaluation/Returns Std                 31.979
evaluation/Returns Max                304.788
evaluation/Returns Min                224.882
evaluation/Actions Mean                 0.116348
evaluation/Actions Std                  0.881658
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    4
evaluation/Average Returns            279.044
time/data storing (s)                   0.00336945
time/evaluation sampling (s)            0.244758
time/exploration sampling (s)           0.256359
time/logging (s)                        0.00375642
time/saving (s)                         0.00232854
time/training (s)                       7.0828
time/epoch (s)                          7.59337
time/total (s)                        487.846
Epoch                                  68
---------------------------------  ----------------
2021-07-02 23:46:30.608949 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 69 finished
---------------------------------  --------------
replay_buffer/size                 80000
trainer/QF Loss                     2155.67
trainer/Policy Loss                -2354.31
trainer/Raw Policy Loss            -2354.31
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean          2308.41
trainer/Q Predictions Std            740.3
trainer/Q Predictions Max           2994.71
trainer/Q Predictions Min            -44.3459
trainer/Q Targets Mean              2310.46
trainer/Q Targets Std                745.304
trainer/Q Targets Max               3065.87
trainer/Q Targets Min                -43.901
trainer/Bellman Errors Mean         2155.67
trainer/Bellman Errors Std          5131.6
trainer/Bellman Errors Max         37275.9
trainer/Bellman Errors Min             0.00354862
trainer/Policy Action Mean             0.158098
trainer/Policy Action Std              0.893393
trainer/Policy Action Max              1
trainer/Policy Action Min             -1
exploration/num steps total        80000
exploration/num paths total         1257
exploration/path length Mean         100
exploration/path length Std           88.5393
exploration/path length Max          345
exploration/path length Min           16
exploration/Rewards Mean               1.47807
exploration/Rewards Std                0.598892
exploration/Rewards Max                2.94498
exploration/Rewards Min               -0.129357
exploration/Returns Mean             147.807
exploration/Returns Std              129.307
exploration/Returns Max              496.091
exploration/Returns Min               11.8246
exploration/Actions Mean               0.109725
exploration/Actions Std                0.72547
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                 10
exploration/Average Returns          147.807
evaluation/num steps total         62250
evaluation/num paths total           363
evaluation/path length Mean          143.5
evaluation/path length Std             2.5
evaluation/path length Max           146
evaluation/path length Min           141
evaluation/Rewards Mean                1.45972
evaluation/Rewards Std                 0.310797
evaluation/Rewards Max                 2.37067
evaluation/Rewards Min                 0.622744
evaluation/Returns Mean              209.47
evaluation/Returns Std                 3.11024
evaluation/Returns Max               212.58
evaluation/Returns Min               206.36
evaluation/Actions Mean                0.0883261
evaluation/Actions Std                 0.762112
evaluation/Actions Max                 1
evaluation/Actions Min                -1
evaluation/Num Paths                   2
evaluation/Average Returns           209.47
time/data storing (s)                  0.00337131
time/evaluation sampling (s)           0.297486
time/exploration sampling (s)          0.259216
time/logging (s)                       0.00279844
time/saving (s)                        0.00127676
time/training (s)                      6.79374
time/epoch (s)                         7.35789
time/total (s)                       495.206
Epoch                                 69
---------------------------------  --------------
2021-07-02 23:46:37.858223 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 70 finished
---------------------------------  ----------------
replay_buffer/size                  81000
trainer/QF Loss                      4006.23
trainer/Policy Loss                 -2191.66
trainer/Raw Policy Loss             -2191.66
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2128.86
trainer/Q Predictions Std             781.17
trainer/Q Predictions Max            2906.09
trainer/Q Predictions Min              31.6936
trainer/Q Targets Mean               2138.98
trainer/Q Targets Std                 788.887
trainer/Q Targets Max                2878.37
trainer/Q Targets Min                   0.0336782
trainer/Bellman Errors Mean          4006.23
trainer/Bellman Errors Std          13990.9
trainer/Bellman Errors Max         105465
trainer/Bellman Errors Min              0.000572443
trainer/Policy Action Mean              0.0885669
trainer/Policy Action Std               0.897488
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         81000
exploration/num paths total          1266
exploration/path length Mean          111.111
exploration/path length Std           161.888
exploration/path length Max           505
exploration/path length Min             9
exploration/Rewards Mean                1.55931
exploration/Rewards Std                 0.66469
exploration/Rewards Max                 3.66925
exploration/Rewards Min                 0.311172
exploration/Returns Mean              173.257
exploration/Returns Std               273.025
exploration/Returns Max               857.776
exploration/Returns Min                 7.20936
exploration/Actions Mean                0.0628056
exploration/Actions Std                 0.656489
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                   9
exploration/Average Returns           173.257
evaluation/num steps total          63250
evaluation/num paths total            364
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 0.987499
evaluation/Rewards Std                  0.0985952
evaluation/Rewards Max                  1.66751
evaluation/Rewards Min                  0.60276
evaluation/Returns Mean               987.499
evaluation/Returns Std                  0
evaluation/Returns Max                987.499
evaluation/Returns Min                987.499
evaluation/Actions Mean                 0.0126892
evaluation/Actions Std                  0.418716
evaluation/Actions Max                  0.997415
evaluation/Actions Min                 -0.999888
evaluation/Num Paths                    1
evaluation/Average Returns            987.499
time/data storing (s)                   0.00341576
time/evaluation sampling (s)            0.296376
time/exploration sampling (s)           0.25915
time/logging (s)                        0.00537785
time/saving (s)                         0.00169774
time/training (s)                       6.68397
time/epoch (s)                          7.24999
time/total (s)                        502.457
Epoch                                  70
---------------------------------  ----------------
2021-07-02 23:46:45.170927 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 71 finished
---------------------------------  --------------
replay_buffer/size                 82000
trainer/QF Loss                     2444.44
trainer/Policy Loss                -2153.76
trainer/Raw Policy Loss            -2153.76
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean          2097.66
trainer/Q Predictions Std            845.623
trainer/Q Predictions Max           2873.49
trainer/Q Predictions Min            -68.3738
trainer/Q Targets Mean              2105.17
trainer/Q Targets Std                846.746
trainer/Q Targets Max               2869.95
trainer/Q Targets Min                 -0.821148
trainer/Bellman Errors Mean         2444.44
trainer/Bellman Errors Std          6417.53
trainer/Bellman Errors Max         50978.7
trainer/Bellman Errors Min             0.00399834
trainer/Policy Action Mean            -0.229137
trainer/Policy Action Std              0.881093
trainer/Policy Action Max              1
trainer/Policy Action Min             -1
exploration/num steps total        82000
exploration/num paths total         1279
exploration/path length Mean          76.9231
exploration/path length Std           75.0471
exploration/path length Max          254
exploration/path length Min            7
exploration/Rewards Mean               0.800745
exploration/Rewards Std                0.324766
exploration/Rewards Max                1.79526
exploration/Rewards Min               -0.246594
exploration/Returns Mean              61.5958
exploration/Returns Std               62.3688
exploration/Returns Max              204.386
exploration/Returns Min                4.43279
exploration/Actions Mean               0.0772554
exploration/Actions Std                0.632555
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                 13
exploration/Average Returns           61.5958
evaluation/num steps total         64250
evaluation/num paths total           365
evaluation/path length Mean         1000
evaluation/path length Std             0
evaluation/path length Max          1000
evaluation/path length Min          1000
evaluation/Rewards Mean                0.976372
evaluation/Rewards Std                 0.173326
evaluation/Rewards Max                 1.39293
evaluation/Rewards Min                 0.369809
evaluation/Returns Mean              976.372
evaluation/Returns Std                 0
evaluation/Returns Max               976.372
evaluation/Returns Min               976.372
evaluation/Actions Mean                0.0228061
evaluation/Actions Std                 0.507115
evaluation/Actions Max                 0.9989
evaluation/Actions Min                -0.999861
evaluation/Num Paths                   1
evaluation/Average Returns           976.372
time/data storing (s)                  0.00338883
time/evaluation sampling (s)           0.292247
time/exploration sampling (s)          0.259807
time/logging (s)                       0.0053321
time/saving (s)                        0.00159411
time/training (s)                      6.74729
time/epoch (s)                         7.30966
time/total (s)                       509.769
Epoch                                 71
---------------------------------  --------------
2021-07-02 23:46:52.309460 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 72 finished
---------------------------------  ---------------
replay_buffer/size                  83000
trainer/QF Loss                      7175.89
trainer/Policy Loss                 -2229.06
trainer/Raw Policy Loss             -2229.06
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2201.2
trainer/Q Predictions Std             678.119
trainer/Q Predictions Max            2749.42
trainer/Q Predictions Min              25.6206
trainer/Q Targets Mean               2205.07
trainer/Q Targets Std                 674.687
trainer/Q Targets Max                2679.34
trainer/Q Targets Min                  27.2135
trainer/Bellman Errors Mean          7175.89
trainer/Bellman Errors Std          33971.9
trainer/Bellman Errors Max         265514
trainer/Bellman Errors Min              0.157975
trainer/Policy Action Mean             -0.264902
trainer/Policy Action Std               0.853566
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         83000
exploration/num paths total          1299
exploration/path length Mean           50
exploration/path length Std            53.2879
exploration/path length Max           241
exploration/path length Min             7
exploration/Rewards Mean                0.665582
exploration/Rewards Std                 0.34622
exploration/Rewards Max                 1.73533
exploration/Rewards Min                -0.285384
exploration/Returns Mean               33.2791
exploration/Returns Std                44.7628
exploration/Returns Max               197.785
exploration/Returns Min                 3.85998
exploration/Actions Mean                0.065584
exploration/Actions Std                 0.610527
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  20
exploration/Average Returns            33.2791
evaluation/num steps total          65250
evaluation/num paths total            366
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 0.970435
evaluation/Rewards Std                  0.169249
evaluation/Rewards Max                  1.42734
evaluation/Rewards Min                  0.293464
evaluation/Returns Mean               970.435
evaluation/Returns Std                  0
evaluation/Returns Max                970.435
evaluation/Returns Min                970.435
evaluation/Actions Mean                 0.0490748
evaluation/Actions Std                  0.440923
evaluation/Actions Max                  0.962169
evaluation/Actions Min                 -0.999908
evaluation/Num Paths                    1
evaluation/Average Returns            970.435
time/data storing (s)                   0.00337643
time/evaluation sampling (s)            0.289466
time/exploration sampling (s)           0.255252
time/logging (s)                        0.0053852
time/saving (s)                         0.0016186
time/training (s)                       6.58072
time/epoch (s)                          7.13582
time/total (s)                        516.907
Epoch                                  72
---------------------------------  ---------------
2021-07-02 23:46:59.622609 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 73 finished
---------------------------------  --------------
replay_buffer/size                 84000
trainer/QF Loss                     3818.76
trainer/Policy Loss                -2293.7
trainer/Raw Policy Loss            -2293.7
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean          2250.6
trainer/Q Predictions Std            595.997
trainer/Q Predictions Max           2819.11
trainer/Q Predictions Min             33.8209
trainer/Q Targets Mean              2237.11
trainer/Q Targets Std                592.691
trainer/Q Targets Max               2894.87
trainer/Q Targets Min                  0.66423
trainer/Bellman Errors Mean         3818.76
trainer/Bellman Errors Std          7585.06
trainer/Bellman Errors Max         47721.2
trainer/Bellman Errors Min             0.144126
trainer/Policy Action Mean            -0.0493052
trainer/Policy Action Std              0.895388
trainer/Policy Action Max              1
trainer/Policy Action Min             -1
exploration/num steps total        84000
exploration/num paths total         1314
exploration/path length Mean          66.6667
exploration/path length Std           59.4818
exploration/path length Max          238
exploration/path length Min            8
exploration/Rewards Mean               0.757416
exploration/Rewards Std                0.338699
exploration/Rewards Max                1.7502
exploration/Rewards Min               -0.623018
exploration/Returns Mean              50.4944
exploration/Returns Std               48.0815
exploration/Returns Max              171.646
exploration/Returns Min                5.51513
exploration/Actions Mean               0.0649331
exploration/Actions Std                0.590159
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                 15
exploration/Average Returns           50.4944
evaluation/num steps total         66250
evaluation/num paths total           367
evaluation/path length Mean         1000
evaluation/path length Std             0
evaluation/path length Max          1000
evaluation/path length Min          1000
evaluation/Rewards Mean                0.981797
evaluation/Rewards Std                 0.214921
evaluation/Rewards Max                 1.60533
evaluation/Rewards Min                 0.487618
evaluation/Returns Mean              981.797
evaluation/Returns Std                 0
evaluation/Returns Max               981.797
evaluation/Returns Min               981.797
evaluation/Actions Mean                0.0319652
evaluation/Actions Std                 0.579495
evaluation/Actions Max                 0.998878
evaluation/Actions Min                -0.999721
evaluation/Num Paths                   1
evaluation/Average Returns           981.797
time/data storing (s)                  0.00353477
time/evaluation sampling (s)           0.288548
time/exploration sampling (s)          0.257888
time/logging (s)                       0.00395318
time/saving (s)                        0.00130948
time/training (s)                      6.75369
time/epoch (s)                         7.30892
time/total (s)                       524.218
Epoch                                 73
---------------------------------  --------------
2021-07-02 23:47:06.960932 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 74 finished
---------------------------------  ---------------
replay_buffer/size                  85000
trainer/QF Loss                      8368.17
trainer/Policy Loss                 -2220.66
trainer/Raw Policy Loss             -2220.66
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2169.05
trainer/Q Predictions Std             686.528
trainer/Q Predictions Max            2697.16
trainer/Q Predictions Min              39.8025
trainer/Q Targets Mean               2171.78
trainer/Q Targets Std                 691.801
trainer/Q Targets Max                2670.48
trainer/Q Targets Min                   2.75561
trainer/Bellman Errors Mean          8368.17
trainer/Bellman Errors Std          44001.1
trainer/Bellman Errors Max         416309
trainer/Bellman Errors Min              0.00141358
trainer/Policy Action Mean             -0.0405842
trainer/Policy Action Std               0.89315
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         85000
exploration/num paths total          1324
exploration/path length Mean          100
exploration/path length Std           109.84
exploration/path length Max           406
exploration/path length Min             7
exploration/Rewards Mean                1.40212
exploration/Rewards Std                 0.957022
exploration/Rewards Max                 5.37464
exploration/Rewards Min                 0.395535
exploration/Returns Mean              140.212
exploration/Returns Std               250.311
exploration/Returns Max               881.015
exploration/Returns Min                 4.65388
exploration/Actions Mean                0.0620534
exploration/Actions Std                 0.590885
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  10
exploration/Average Returns           140.212
evaluation/num steps total          67250
evaluation/num paths total            368
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 0.982991
evaluation/Rewards Std                  0.070533
evaluation/Rewards Max                  1.00159
evaluation/Rewards Min                  0.476227
evaluation/Returns Mean               982.991
evaluation/Returns Std                  0
evaluation/Returns Max                982.991
evaluation/Returns Min                982.991
evaluation/Actions Mean                 0.0192694
evaluation/Actions Std                  0.394804
evaluation/Actions Max                  0.954215
evaluation/Actions Min                 -0.999854
evaluation/Num Paths                    1
evaluation/Average Returns            982.991
time/data storing (s)                   0.00345431
time/evaluation sampling (s)            0.279682
time/exploration sampling (s)           0.260019
time/logging (s)                        0.00533135
time/saving (s)                         0.00162757
time/training (s)                       6.78736
time/epoch (s)                          7.33747
time/total (s)                        531.557
Epoch                                  74
---------------------------------  ---------------
2021-07-02 23:47:14.354022 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 75 finished
---------------------------------  --------------
replay_buffer/size                 86000
trainer/QF Loss                     1707.7
trainer/Policy Loss                -2234.14
trainer/Raw Policy Loss            -2234.14
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean          2198.02
trainer/Q Predictions Std            624.782
trainer/Q Predictions Max           2850.11
trainer/Q Predictions Min             65.3502
trainer/Q Targets Mean              2194.04
trainer/Q Targets Std                627.005
trainer/Q Targets Max               2808.84
trainer/Q Targets Min                -24.3909
trainer/Bellman Errors Mean         1707.7
trainer/Bellman Errors Std          3636.53
trainer/Bellman Errors Max         26865
trainer/Bellman Errors Min             0.319159
trainer/Policy Action Mean            -0.102291
trainer/Policy Action Std              0.874785
trainer/Policy Action Max              1
trainer/Policy Action Min             -1
exploration/num steps total        86000
exploration/num paths total         1330
exploration/path length Mean         166.667
exploration/path length Std           58.3486
exploration/path length Max          238
exploration/path length Min           81
exploration/Rewards Mean               0.956302
exploration/Rewards Std                0.404618
exploration/Rewards Max                2.05856
exploration/Rewards Min               -0.0160235
exploration/Returns Mean             159.384
exploration/Returns Std               54.9929
exploration/Returns Max              213.903
exploration/Returns Min               67.8244
exploration/Actions Mean               0.0774562
exploration/Actions Std                0.582395
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                  6
exploration/Average Returns          159.384
evaluation/num steps total         68250
evaluation/num paths total           369
evaluation/path length Mean         1000
evaluation/path length Std             0
evaluation/path length Max          1000
evaluation/path length Min          1000
evaluation/Rewards Mean                0.98549
evaluation/Rewards Std                 0.229283
evaluation/Rewards Max                 1.66222
evaluation/Rewards Min                 0.472866
evaluation/Returns Mean              985.49
evaluation/Returns Std                 0
evaluation/Returns Max               985.49
evaluation/Returns Min               985.49
evaluation/Actions Mean                0.016657
evaluation/Actions Std                 0.442164
evaluation/Actions Max                 0.993236
evaluation/Actions Min                -0.999886
evaluation/Num Paths                   1
evaluation/Average Returns           985.49
time/data storing (s)                  0.00346823
time/evaluation sampling (s)           0.289432
time/exploration sampling (s)          0.255206
time/logging (s)                       0.00659502
time/saving (s)                        0.00189168
time/training (s)                      6.83494
time/epoch (s)                         7.39153
time/total (s)                       538.951
Epoch                                 75
---------------------------------  --------------
2021-07-02 23:47:21.615576 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 76 finished
---------------------------------  ---------------
replay_buffer/size                  87000
trainer/QF Loss                      4486.55
trainer/Policy Loss                 -2118.68
trainer/Raw Policy Loss             -2118.68
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2072.19
trainer/Q Predictions Std             754.399
trainer/Q Predictions Max            2791.83
trainer/Q Predictions Min              19.1483
trainer/Q Targets Mean               2076.59
trainer/Q Targets Std                 756.053
trainer/Q Targets Max                2771.95
trainer/Q Targets Min                   2.93465
trainer/Bellman Errors Mean          4486.55
trainer/Bellman Errors Std          19551.3
trainer/Bellman Errors Max         168830
trainer/Bellman Errors Min              0.0920921
trainer/Policy Action Mean             -0.0722677
trainer/Policy Action Std               0.896354
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         87000
exploration/num paths total          1336
exploration/path length Mean          166.667
exploration/path length Std           103.148
exploration/path length Max           325
exploration/path length Min            25
exploration/Rewards Mean                1.09825
exploration/Rewards Std                 0.696196
exploration/Rewards Max                 3.40273
exploration/Rewards Min                -0.176867
exploration/Returns Mean              183.042
exploration/Returns Std               147.757
exploration/Returns Max               469.603
exploration/Returns Min                25.9536
exploration/Actions Mean                0.0716986
exploration/Actions Std                 0.629501
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                   6
exploration/Average Returns           183.042
evaluation/num steps total          69250
evaluation/num paths total            370
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 1.00377
evaluation/Rewards Std                  0.157051
evaluation/Rewards Max                  1.81744
evaluation/Rewards Min                  0.424499
evaluation/Returns Mean              1003.77
evaluation/Returns Std                  0
evaluation/Returns Max               1003.77
evaluation/Returns Min               1003.77
evaluation/Actions Mean                 0.0472987
evaluation/Actions Std                  0.447409
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    1
evaluation/Average Returns           1003.77
time/data storing (s)                   0.00341196
time/evaluation sampling (s)            0.260674
time/exploration sampling (s)           0.259677
time/logging (s)                        0.00614839
time/saving (s)                         0.00195134
time/training (s)                       6.72604
time/epoch (s)                          7.2579
time/total (s)                        546.21
Epoch                                  76
---------------------------------  ---------------
2021-07-02 23:47:28.858126 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 77 finished
---------------------------------  ---------------
replay_buffer/size                  88000
trainer/QF Loss                      3858.16
trainer/Policy Loss                 -2156.39
trainer/Raw Policy Loss             -2156.39
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2099.55
trainer/Q Predictions Std             767.393
trainer/Q Predictions Max            3019.34
trainer/Q Predictions Min             -27.4871
trainer/Q Targets Mean               2096.05
trainer/Q Targets Std                 755.441
trainer/Q Targets Max                2765.27
trainer/Q Targets Min                 -10.8846
trainer/Bellman Errors Mean          3858.16
trainer/Bellman Errors Std          19039.9
trainer/Bellman Errors Max         155777
trainer/Bellman Errors Min              0.154309
trainer/Policy Action Mean             -0.169086
trainer/Policy Action Std               0.889385
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         88000
exploration/num paths total          1345
exploration/path length Mean          111.111
exploration/path length Std           143.07
exploration/path length Max           440
exploration/path length Min             8
exploration/Rewards Mean                1.23547
exploration/Rewards Std                 0.825827
exploration/Rewards Max                 3.66321
exploration/Rewards Min                -0.312504
exploration/Returns Mean              137.275
exploration/Returns Std               247.018
exploration/Returns Max               808.794
exploration/Returns Min                 5.13626
exploration/Actions Mean                0.0995172
exploration/Actions Std                 0.702796
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                   9
exploration/Average Returns           137.275
evaluation/num steps total          70250
evaluation/num paths total            371
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 0.947264
evaluation/Rewards Std                  0.20228
evaluation/Rewards Max                  1.55785
evaluation/Rewards Min                 -0.317541
evaluation/Returns Mean               947.264
evaluation/Returns Std                  0
evaluation/Returns Max                947.264
evaluation/Returns Min                947.264
evaluation/Actions Mean                 0.0348438
evaluation/Actions Std                  0.362711
evaluation/Actions Max                  1
evaluation/Actions Min                 -0.999999
evaluation/Num Paths                    1
evaluation/Average Returns            947.264
time/data storing (s)                   0.00335645
time/evaluation sampling (s)            0.249288
time/exploration sampling (s)           0.256445
time/logging (s)                        0.00456154
time/saving (s)                         0.00166332
time/training (s)                       6.72312
time/epoch (s)                          7.23843
time/total (s)                        553.451
Epoch                                  77
---------------------------------  ---------------
2021-07-02 23:47:36.082101 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 78 finished
---------------------------------  ---------------
replay_buffer/size                  89000
trainer/QF Loss                      9563.13
trainer/Policy Loss                 -2283.23
trainer/Raw Policy Loss             -2283.23
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2225.33
trainer/Q Predictions Std             561.601
trainer/Q Predictions Max            2802.21
trainer/Q Predictions Min              42.6676
trainer/Q Targets Mean               2219.68
trainer/Q Targets Std                 569.933
trainer/Q Targets Max                2808.33
trainer/Q Targets Min                   0.347492
trainer/Bellman Errors Mean          9563.13
trainer/Bellman Errors Std          36443.2
trainer/Bellman Errors Max         307702
trainer/Bellman Errors Min              0.316956
trainer/Policy Action Mean              0.039917
trainer/Policy Action Std               0.889773
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         89000
exploration/num paths total          1370
exploration/path length Mean           40
exploration/path length Std            28.1468
exploration/path length Max            94
exploration/path length Min             7
exploration/Rewards Mean                0.705326
exploration/Rewards Std                 0.235515
exploration/Rewards Max                 1.58901
exploration/Rewards Min                 0.166585
exploration/Returns Mean               28.213
exploration/Returns Std                22.3557
exploration/Returns Max                78.686
exploration/Returns Min                 4.1487
exploration/Actions Mean                0.0595544
exploration/Actions Std                 0.558133
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  25
exploration/Average Returns            28.213
evaluation/num steps total          71250
evaluation/num paths total            372
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 0.977959
evaluation/Rewards Std                  0.0948411
evaluation/Rewards Max                  1.0156
evaluation/Rewards Min                  0.368911
evaluation/Returns Mean               977.959
evaluation/Returns Std                  0
evaluation/Returns Max                977.959
evaluation/Returns Min                977.959
evaluation/Actions Mean                 0.0999561
evaluation/Actions Std                  0.129464
evaluation/Actions Max                  0.988271
evaluation/Actions Min                 -0.999963
evaluation/Num Paths                    1
evaluation/Average Returns            977.959
time/data storing (s)                   0.00344997
time/evaluation sampling (s)            0.267268
time/exploration sampling (s)           0.263824
time/logging (s)                        0.00419358
time/saving (s)                         0.00131953
time/training (s)                       6.68106
time/epoch (s)                          7.22111
time/total (s)                        560.674
Epoch                                  78
---------------------------------  ---------------
2021-07-02 23:47:43.230876 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 79 finished
---------------------------------  --------------
replay_buffer/size                 90000
trainer/QF Loss                     2907.65
trainer/Policy Loss                -2131.05
trainer/Raw Policy Loss            -2131.05
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean          2094.06
trainer/Q Predictions Std            679.323
trainer/Q Predictions Max           2678.52
trainer/Q Predictions Min           -117.542
trainer/Q Targets Mean              2091.75
trainer/Q Targets Std                672.966
trainer/Q Targets Max               2690.56
trainer/Q Targets Min                  0.945553
trainer/Bellman Errors Mean         2907.65
trainer/Bellman Errors Std          8519.31
trainer/Bellman Errors Max         75869.8
trainer/Bellman Errors Min             0.204894
trainer/Policy Action Mean             0.0738286
trainer/Policy Action Std              0.885789
trainer/Policy Action Max              1
trainer/Policy Action Min             -1
exploration/num steps total        90000
exploration/num paths total         1380
exploration/path length Mean         100
exploration/path length Std           91.5183
exploration/path length Max          256
exploration/path length Min            9
exploration/Rewards Mean               1.39783
exploration/Rewards Std                0.706491
exploration/Rewards Max                3.06958
exploration/Rewards Min                0.0453945
exploration/Returns Mean             139.783
exploration/Returns Std              159.453
exploration/Returns Max              445.28
exploration/Returns Min                5.02669
exploration/Actions Mean               0.135613
exploration/Actions Std                0.664053
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                 10
exploration/Average Returns          139.783
evaluation/num steps total         72250
evaluation/num paths total           373
evaluation/path length Mean         1000
evaluation/path length Std             0
evaluation/path length Max          1000
evaluation/path length Min          1000
evaluation/Rewards Mean                1.01098
evaluation/Rewards Std                 0.24788
evaluation/Rewards Max                 1.74635
evaluation/Rewards Min                 0.341854
evaluation/Returns Mean             1010.98
evaluation/Returns Std                 0
evaluation/Returns Max              1010.98
evaluation/Returns Min              1010.98
evaluation/Actions Mean                0.0332198
evaluation/Actions Std                 0.463924
evaluation/Actions Max                 1
evaluation/Actions Min                -0.999988
evaluation/Num Paths                   1
evaluation/Average Returns          1010.98
time/data storing (s)                  0.00331973
time/evaluation sampling (s)           0.271427
time/exploration sampling (s)          0.258864
time/logging (s)                       0.00446872
time/saving (s)                        0.00132287
time/training (s)                      6.60735
time/epoch (s)                         7.14675
time/total (s)                       567.822
Epoch                                 79
---------------------------------  --------------
2021-07-02 23:47:50.382471 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 80 finished
---------------------------------  ----------------
replay_buffer/size                  91000
trainer/QF Loss                      3869.08
trainer/Policy Loss                 -2128.63
trainer/Raw Policy Loss             -2128.63
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2060.51
trainer/Q Predictions Std             721.416
trainer/Q Predictions Max            2843.68
trainer/Q Predictions Min              12.8059
trainer/Q Targets Mean               2074.04
trainer/Q Targets Std                 721.431
trainer/Q Targets Max                2749.12
trainer/Q Targets Min                   0.678951
trainer/Bellman Errors Mean          3869.08
trainer/Bellman Errors Std          17476.6
trainer/Bellman Errors Max         192814
trainer/Bellman Errors Min              0.000501756
trainer/Policy Action Mean              0.212527
trainer/Policy Action Std               0.920874
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         91000
exploration/num paths total          1395
exploration/path length Mean           66.6667
exploration/path length Std            41.6264
exploration/path length Max           168
exploration/path length Min            19
exploration/Rewards Mean                2.04113
exploration/Rewards Std                 0.862845
exploration/Rewards Max                 5.26808
exploration/Rewards Min                 0.483917
exploration/Returns Mean              136.075
exploration/Returns Std               105.718
exploration/Returns Max               360.752
exploration/Returns Min                21.5566
exploration/Actions Mean                0.0888004
exploration/Actions Std                 0.778169
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  15
exploration/Average Returns           136.075
evaluation/num steps total          73239
evaluation/num paths total            380
evaluation/path length Mean           141.286
evaluation/path length Std              3.36852
evaluation/path length Max            145
evaluation/path length Min            135
evaluation/Rewards Mean                 2.34866
evaluation/Rewards Std                  0.960463
evaluation/Rewards Max                  5.71742
evaluation/Rewards Min                  0.616045
evaluation/Returns Mean               331.833
evaluation/Returns Std                 60.1232
evaluation/Returns Max                404.896
evaluation/Returns Min                257.76
evaluation/Actions Mean                -0.0231482
evaluation/Actions Std                  0.923598
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    7
evaluation/Average Returns            331.833
time/data storing (s)                   0.00333086
time/evaluation sampling (s)            0.245878
time/exploration sampling (s)           0.255426
time/logging (s)                        0.0040882
time/saving (s)                         0.00136964
time/training (s)                       6.63861
time/epoch (s)                          7.1487
time/total (s)                        574.973
Epoch                                  80
---------------------------------  ----------------
2021-07-02 23:47:57.738481 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 81 finished
---------------------------------  ---------------
replay_buffer/size                  92000
trainer/QF Loss                     12145.6
trainer/Policy Loss                 -2273.82
trainer/Raw Policy Loss             -2273.82
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2192.4
trainer/Q Predictions Std             766.901
trainer/Q Predictions Max            2926.74
trainer/Q Predictions Min            -533.671
trainer/Q Targets Mean               2190.05
trainer/Q Targets Std                 755.08
trainer/Q Targets Max                2821.41
trainer/Q Targets Min                -539.422
trainer/Bellman Errors Mean         12145.6
trainer/Bellman Errors Std          78412.8
trainer/Bellman Errors Max         852944
trainer/Bellman Errors Min              0.173882
trainer/Policy Action Mean              0.00336521
trainer/Policy Action Std               0.942865
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         92000
exploration/num paths total          1404
exploration/path length Mean          111.111
exploration/path length Std            82.3292
exploration/path length Max           267
exploration/path length Min             8
exploration/Rewards Mean                2.14835
exploration/Rewards Std                 0.829161
exploration/Rewards Max                 4.71843
exploration/Rewards Min                 0.483382
exploration/Returns Mean              238.706
exploration/Returns Std               202.194
exploration/Returns Max               641.991
exploration/Returns Min                 6.33501
exploration/Actions Mean                0.102918
exploration/Actions Std                 0.734174
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                   9
exploration/Average Returns           238.706
evaluation/num steps total          74109
evaluation/num paths total            385
evaluation/path length Mean           174
evaluation/path length Std             17.5157
evaluation/path length Max            195
evaluation/path length Min            154
evaluation/Rewards Mean                 2.23626
evaluation/Rewards Std                  0.689057
evaluation/Rewards Max                  4.18296
evaluation/Rewards Min                  0.588388
evaluation/Returns Mean               389.109
evaluation/Returns Std                 27.4073
evaluation/Returns Max                427.779
evaluation/Returns Min                365.451
evaluation/Actions Mean                 0.115569
evaluation/Actions Std                  0.881824
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    5
evaluation/Average Returns            389.109
time/data storing (s)                   0.00349916
time/evaluation sampling (s)            0.247071
time/exploration sampling (s)           0.28764
time/logging (s)                        0.004172
time/saving (s)                         0.00146194
time/training (s)                       6.80996
time/epoch (s)                          7.3538
time/total (s)                        582.328
Epoch                                  81
---------------------------------  ---------------
2021-07-02 23:48:05.016583 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 82 finished
---------------------------------  ----------------
replay_buffer/size                  93000
trainer/QF Loss                     17860.1
trainer/Policy Loss                 -2282.56
trainer/Raw Policy Loss             -2282.56
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2255.35
trainer/Q Predictions Std             647.968
trainer/Q Predictions Max            2728.76
trainer/Q Predictions Min             -53.2057
trainer/Q Targets Mean               2262.84
trainer/Q Targets Std                 675.478
trainer/Q Targets Max                2765.95
trainer/Q Targets Min                   1.36364
trainer/Bellman Errors Mean         17860.1
trainer/Bellman Errors Std         153739
trainer/Bellman Errors Max              1.74795e+06
trainer/Bellman Errors Min              0.026438
trainer/Policy Action Mean              0.0371147
trainer/Policy Action Std               0.904022
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         93000
exploration/num paths total          1418
exploration/path length Mean           71.4286
exploration/path length Std            37.4218
exploration/path length Max           125
exploration/path length Min             8
exploration/Rewards Mean                1.34136
exploration/Rewards Std                 0.537608
exploration/Rewards Max                 3.10022
exploration/Rewards Min                 0.0383611
exploration/Returns Mean               95.8111
exploration/Returns Std                54.9467
exploration/Returns Max               177.871
exploration/Returns Min                 5.76277
exploration/Actions Mean                0.115979
exploration/Actions Std                 0.683245
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  14
exploration/Average Returns            95.8111
evaluation/num steps total          74379
evaluation/num paths total            387
evaluation/path length Mean           135
evaluation/path length Std             33
evaluation/path length Max            168
evaluation/path length Min            102
evaluation/Rewards Mean                 1.40413
evaluation/Rewards Std                  0.512995
evaluation/Rewards Max                  2.4875
evaluation/Rewards Min                  0.558435
evaluation/Returns Mean               189.557
evaluation/Returns Std                 30.7302
evaluation/Returns Max                220.287
evaluation/Returns Min                158.827
evaluation/Actions Mean                 0.0148437
evaluation/Actions Std                  0.657741
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    2
evaluation/Average Returns            189.557
time/data storing (s)                   0.00414616
time/evaluation sampling (s)            0.292833
time/exploration sampling (s)           0.315112
time/logging (s)                        0.00285394
time/saving (s)                         0.00133217
time/training (s)                       6.65781
time/epoch (s)                          7.27409
time/total (s)                        589.604
Epoch                                  82
---------------------------------  ----------------
2021-07-02 23:48:12.479383 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 83 finished
---------------------------------  ---------------
replay_buffer/size                  94000
trainer/QF Loss                      6173.37
trainer/Policy Loss                 -2169.48
trainer/Raw Policy Loss             -2169.48
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2117.25
trainer/Q Predictions Std             775.803
trainer/Q Predictions Max            2741.38
trainer/Q Predictions Min            -111.321
trainer/Q Targets Mean               2105.85
trainer/Q Targets Std                 781.354
trainer/Q Targets Max                2778.79
trainer/Q Targets Min                -247.587
trainer/Bellman Errors Mean          6173.37
trainer/Bellman Errors Std          23969.9
trainer/Bellman Errors Max         241871
trainer/Bellman Errors Min              0.0253382
trainer/Policy Action Mean             -0.209335
trainer/Policy Action Std               0.886467
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         94000
exploration/num paths total          1434
exploration/path length Mean           62.5
exploration/path length Std            65.6173
exploration/path length Max           258
exploration/path length Min             7
exploration/Rewards Mean                0.970816
exploration/Rewards Std                 0.378824
exploration/Rewards Max                 2.88815
exploration/Rewards Min                 0.219514
exploration/Returns Mean               60.676
exploration/Returns Std                69.66
exploration/Returns Max               250.792
exploration/Returns Min                 3.66707
exploration/Actions Mean                0.0648276
exploration/Actions Std                 0.54286
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  16
exploration/Average Returns            60.676
evaluation/num steps total          75379
evaluation/num paths total            388
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 0.998517
evaluation/Rewards Std                  0.14588
evaluation/Rewards Max                  1.74923
evaluation/Rewards Min                  0.424864
evaluation/Returns Mean               998.517
evaluation/Returns Std                  0
evaluation/Returns Max                998.517
evaluation/Returns Min                998.517
evaluation/Actions Mean                 0.0211548
evaluation/Actions Std                  0.547217
evaluation/Actions Max                  0.997296
evaluation/Actions Min                 -0.999982
evaluation/Num Paths                    1
evaluation/Average Returns            998.517
time/data storing (s)                   0.00341475
time/evaluation sampling (s)            0.250727
time/exploration sampling (s)           0.259241
time/logging (s)                        0.00649929
time/saving (s)                         0.00182902
time/training (s)                       6.94277
time/epoch (s)                          7.46448
time/total (s)                        597.07
Epoch                                  83
---------------------------------  ---------------
2021-07-02 23:48:19.766524 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 84 finished
---------------------------------  ---------------
replay_buffer/size                  95000
trainer/QF Loss                      3248.8
trainer/Policy Loss                 -2267.77
trainer/Raw Policy Loss             -2267.77
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2211.77
trainer/Q Predictions Std             732.501
trainer/Q Predictions Max            3049.91
trainer/Q Predictions Min             -27.7625
trainer/Q Targets Mean               2200.52
trainer/Q Targets Std                 740.343
trainer/Q Targets Max                3055
trainer/Q Targets Min                 -64.565
trainer/Bellman Errors Mean          3248.8
trainer/Bellman Errors Std          11689
trainer/Bellman Errors Max         128437
trainer/Bellman Errors Min              1.24456
trainer/Policy Action Mean             -0.28275
trainer/Policy Action Std               0.868577
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         95000
exploration/num paths total          1447
exploration/path length Mean           76.9231
exploration/path length Std            78.9318
exploration/path length Max           286
exploration/path length Min             8
exploration/Rewards Mean                0.964955
exploration/Rewards Std                 0.36763
exploration/Rewards Max                 1.98793
exploration/Rewards Min                 0.132366
exploration/Returns Mean               74.2273
exploration/Returns Std                87.436
exploration/Returns Max               326.325
exploration/Returns Min                 5.33302
exploration/Actions Mean                0.0384639
exploration/Actions Std                 0.573641
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  13
exploration/Average Returns            74.2273
evaluation/num steps total          76379
evaluation/num paths total            389
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 0.985286
evaluation/Rewards Std                  0.0623599
evaluation/Rewards Max                  1.02153
evaluation/Rewards Min                  0.47079
evaluation/Returns Mean               985.286
evaluation/Returns Std                  0
evaluation/Returns Max                985.286
evaluation/Returns Min                985.286
evaluation/Actions Mean                 0.0159945
evaluation/Actions Std                  0.596076
evaluation/Actions Max                  0.954411
evaluation/Actions Min                 -0.999978
evaluation/Num Paths                    1
evaluation/Average Returns            985.286
time/data storing (s)                   0.0034407
time/evaluation sampling (s)            0.251762
time/exploration sampling (s)           0.265874
time/logging (s)                        0.00399717
time/saving (s)                         0.00210177
time/training (s)                       6.75423
time/epoch (s)                          7.28141
time/total (s)                        604.353
Epoch                                  84
---------------------------------  ---------------
2021-07-02 23:48:26.793366 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 85 finished
---------------------------------  ---------------
replay_buffer/size                  96000
trainer/QF Loss                      3793.29
trainer/Policy Loss                 -2341.64
trainer/Raw Policy Loss             -2341.64
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2305.35
trainer/Q Predictions Std             724.474
trainer/Q Predictions Max            3057.84
trainer/Q Predictions Min            -321.262
trainer/Q Targets Mean               2296.27
trainer/Q Targets Std                 729.599
trainer/Q Targets Max                3030.02
trainer/Q Targets Min                -640.95
trainer/Bellman Errors Mean          3793.29
trainer/Bellman Errors Std          11182
trainer/Bellman Errors Max         102201
trainer/Bellman Errors Min              0.0998042
trainer/Policy Action Mean             -0.226613
trainer/Policy Action Std               0.864838
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         96000
exploration/num paths total          1459
exploration/path length Mean           83.3333
exploration/path length Std            58.1196
exploration/path length Max           244
exploration/path length Min            19
exploration/Rewards Mean                0.879552
exploration/Rewards Std                 0.188406
exploration/Rewards Max                 1.39288
exploration/Rewards Min                 0.357448
exploration/Returns Mean               73.296
exploration/Returns Std                56.6945
exploration/Returns Max               230.329
exploration/Returns Min                12.0856
exploration/Actions Mean                0.0192128
exploration/Actions Std                 0.507497
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  12
exploration/Average Returns            73.296
evaluation/num steps total          77379
evaluation/num paths total            390
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 0.983721
evaluation/Rewards Std                  0.0497819
evaluation/Rewards Max                  1.01236
evaluation/Rewards Min                  0.491947
evaluation/Returns Mean               983.721
evaluation/Returns Std                  0
evaluation/Returns Max                983.721
evaluation/Returns Min                983.721
evaluation/Actions Mean                 0.0144314
evaluation/Actions Std                  0.64774
evaluation/Actions Max                  0.924417
evaluation/Actions Min                 -0.999952
evaluation/Num Paths                    1
evaluation/Average Returns            983.721
time/data storing (s)                   0.0033671
time/evaluation sampling (s)            0.24862
time/exploration sampling (s)           0.279897
time/logging (s)                        0.00389363
time/saving (s)                         0.00126366
time/training (s)                       6.48728
time/epoch (s)                          7.02432
time/total (s)                        611.379
Epoch                                  85
---------------------------------  ---------------
2021-07-02 23:48:33.971050 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 86 finished
---------------------------------  --------------
replay_buffer/size                 97000
trainer/QF Loss                     2124.43
trainer/Policy Loss                -2371.07
trainer/Raw Policy Loss            -2371.07
trainer/Preactivation Policy Loss      0
trainer/Q Predictions Mean          2337.83
trainer/Q Predictions Std            703.225
trainer/Q Predictions Max           3168.4
trainer/Q Predictions Min           -101.167
trainer/Q Targets Mean              2347.32
trainer/Q Targets Std                708.605
trainer/Q Targets Max               3184.23
trainer/Q Targets Min                  0.145194
trainer/Bellman Errors Mean         2124.43
trainer/Bellman Errors Std          3982.88
trainer/Bellman Errors Max         28147
trainer/Bellman Errors Min             0.00172257
trainer/Policy Action Mean            -0.193399
trainer/Policy Action Std              0.883226
trainer/Policy Action Max              1
trainer/Policy Action Min             -1
exploration/num steps total        97000
exploration/num paths total         1474
exploration/path length Mean          66.6667
exploration/path length Std          118.756
exploration/path length Max          498
exploration/path length Min            8
exploration/Rewards Mean               0.884339
exploration/Rewards Std                0.239645
exploration/Rewards Max                1.59527
exploration/Rewards Min                0.244982
exploration/Returns Mean              58.9559
exploration/Returns Std              114.993
exploration/Returns Max              477.22
exploration/Returns Min                5.4505
exploration/Actions Mean               0.0761724
exploration/Actions Std                0.514253
exploration/Actions Max                1
exploration/Actions Min               -1
exploration/Num Paths                 15
exploration/Average Returns           58.9559
evaluation/num steps total         78379
evaluation/num paths total           391
evaluation/path length Mean         1000
evaluation/path length Std             0
evaluation/path length Max          1000
evaluation/path length Min          1000
evaluation/Rewards Mean                0.992749
evaluation/Rewards Std                 0.0466749
evaluation/Rewards Max                 1.00489
evaluation/Rewards Min                 0.514316
evaluation/Returns Mean              992.749
evaluation/Returns Std                 0
evaluation/Returns Max               992.749
evaluation/Returns Min               992.749
evaluation/Actions Mean                0.0124763
evaluation/Actions Std                 0.528791
evaluation/Actions Max                 0.92255
evaluation/Actions Min                -0.999934
evaluation/Num Paths                   1
evaluation/Average Returns           992.749
time/data storing (s)                  0.00337596
time/evaluation sampling (s)           0.273126
time/exploration sampling (s)          0.312373
time/logging (s)                       0.00525295
time/saving (s)                        0.00171504
time/training (s)                      6.581
time/epoch (s)                         7.17684
time/total (s)                       618.558
Epoch                                 86
---------------------------------  --------------
2021-07-02 23:48:41.331891 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 87 finished
---------------------------------  ----------------
replay_buffer/size                  98000
trainer/QF Loss                     54353.7
trainer/Policy Loss                 -2453.87
trainer/Raw Policy Loss             -2453.87
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2402.42
trainer/Q Predictions Std             734.156
trainer/Q Predictions Max            3237.96
trainer/Q Predictions Min            -167.321
trainer/Q Targets Mean               2379.43
trainer/Q Targets Std                 772.403
trainer/Q Targets Max                3197.8
trainer/Q Targets Min                -173.516
trainer/Bellman Errors Mean         54353.7
trainer/Bellman Errors Std         371413
trainer/Bellman Errors Max              3.21903e+06
trainer/Bellman Errors Min              3.54039
trainer/Policy Action Mean             -0.0213018
trainer/Policy Action Std               0.913791
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         98000
exploration/num paths total          1485
exploration/path length Mean           90.9091
exploration/path length Std           109.611
exploration/path length Max           409
exploration/path length Min             9
exploration/Rewards Mean                0.963622
exploration/Rewards Std                 0.27883
exploration/Rewards Max                 2.1067
exploration/Rewards Min                 0.33318
exploration/Returns Mean               87.602
exploration/Returns Std               108.102
exploration/Returns Max               397.011
exploration/Returns Min                 6.42882
exploration/Actions Mean                0.0481035
exploration/Actions Std                 0.536815
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  11
exploration/Average Returns            87.602
evaluation/num steps total          79379
evaluation/num paths total            392
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 0.991629
evaluation/Rewards Std                  0.0436089
evaluation/Rewards Max                  1.00892
evaluation/Rewards Min                  0.500327
evaluation/Returns Mean               991.629
evaluation/Returns Std                  0
evaluation/Returns Max                991.629
evaluation/Returns Min                991.629
evaluation/Actions Mean                 0.0110376
evaluation/Actions Std                  0.573432
evaluation/Actions Max                  0.881538
evaluation/Actions Min                 -0.999933
evaluation/Num Paths                    1
evaluation/Average Returns            991.629
time/data storing (s)                   0.00341744
time/evaluation sampling (s)            0.264157
time/exploration sampling (s)           0.262906
time/logging (s)                        0.00535178
time/saving (s)                         0.00161545
time/training (s)                       6.82044
time/epoch (s)                          7.35789
time/total (s)                        625.918
Epoch                                  87
---------------------------------  ----------------
2021-07-02 23:48:48.351187 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 88 finished
---------------------------------  ---------------
replay_buffer/size                  99000
trainer/QF Loss                     10660.3
trainer/Policy Loss                 -2324.19
trainer/Raw Policy Loss             -2324.19
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2262.86
trainer/Q Predictions Std             887.689
trainer/Q Predictions Max            3169.41
trainer/Q Predictions Min            -832.12
trainer/Q Targets Mean               2290.86
trainer/Q Targets Std                 877.783
trainer/Q Targets Max                3252.27
trainer/Q Targets Min                   0.120463
trainer/Bellman Errors Mean         10660.3
trainer/Bellman Errors Std          63425.9
trainer/Bellman Errors Max         694565
trainer/Bellman Errors Min              1.29415
trainer/Policy Action Mean              0.0276212
trainer/Policy Action Std               0.910862
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total         99000
exploration/num paths total          1504
exploration/path length Mean           52.6316
exploration/path length Std            45.4674
exploration/path length Max           153
exploration/path length Min             7
exploration/Rewards Mean                1.03864
exploration/Rewards Std                 0.411302
exploration/Rewards Max                 2.31109
exploration/Rewards Min                 0.239888
exploration/Returns Mean               54.6652
exploration/Returns Std                52.6371
exploration/Returns Max               181.376
exploration/Returns Min                 4.62464
exploration/Actions Mean                0.0837422
exploration/Actions Std                 0.608367
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  19
exploration/Average Returns            54.6652
evaluation/num steps total          80379
evaluation/num paths total            393
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 0.992785
evaluation/Rewards Std                  0.0376354
evaluation/Rewards Max                  1.10679
evaluation/Rewards Min                  0.466947
evaluation/Returns Mean               992.785
evaluation/Returns Std                  0
evaluation/Returns Max                992.785
evaluation/Returns Min                992.785
evaluation/Actions Mean                 0.0605216
evaluation/Actions Std                  0.134486
evaluation/Actions Max                  0.984027
evaluation/Actions Min                 -0.999974
evaluation/Num Paths                    1
evaluation/Average Returns            992.785
time/data storing (s)                   0.00342829
time/evaluation sampling (s)            0.27388
time/exploration sampling (s)           0.264379
time/logging (s)                        0.0053798
time/saving (s)                         0.00161506
time/training (s)                       6.4677
time/epoch (s)                          7.01638
time/total (s)                        632.936
Epoch                                  88
---------------------------------  ---------------
2021-07-02 23:48:55.416471 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 89 finished
---------------------------------  ---------------
replay_buffer/size                 100000
trainer/QF Loss                      7110.11
trainer/Policy Loss                 -2360.76
trainer/Raw Policy Loss             -2360.76
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2304.19
trainer/Q Predictions Std             872.264
trainer/Q Predictions Max            3242.58
trainer/Q Predictions Min            -277.181
trainer/Q Targets Mean               2312.83
trainer/Q Targets Std                 876.312
trainer/Q Targets Max                3281.39
trainer/Q Targets Min                  -0.593662
trainer/Bellman Errors Mean          7110.11
trainer/Bellman Errors Std          23251.3
trainer/Bellman Errors Max         178830
trainer/Bellman Errors Min              0.136987
trainer/Policy Action Mean             -0.0431874
trainer/Policy Action Std               0.927795
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total        100000
exploration/num paths total          1526
exploration/path length Mean           45.4545
exploration/path length Std            56.0785
exploration/path length Max           234
exploration/path length Min             7
exploration/Rewards Mean                1.56736
exploration/Rewards Std                 0.599441
exploration/Rewards Max                 2.77681
exploration/Rewards Min                 0.39216
exploration/Returns Mean               71.2438
exploration/Returns Std               108.354
exploration/Returns Max               457.657
exploration/Returns Min                 4.6461
exploration/Actions Mean                0.131662
exploration/Actions Std                 0.73167
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  22
exploration/Average Returns            71.2438
evaluation/num steps total          81094
evaluation/num paths total            395
evaluation/path length Mean           357.5
evaluation/path length Std            109.5
evaluation/path length Max            467
evaluation/path length Min            248
evaluation/Rewards Mean                 2.20547
evaluation/Rewards Std                  1.02528
evaluation/Rewards Max                  5.69358
evaluation/Rewards Min                  0.518664
evaluation/Returns Mean               788.454
evaluation/Returns Std                186.863
evaluation/Returns Max                975.317
evaluation/Returns Min                601.591
evaluation/Actions Mean                 0.107808
evaluation/Actions Std                  0.839943
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    2
evaluation/Average Returns            788.454
time/data storing (s)                   0.00338046
time/evaluation sampling (s)            0.271914
time/exploration sampling (s)           0.255989
time/logging (s)                        0.00363487
time/saving (s)                         0.00126114
time/training (s)                       6.52447
time/epoch (s)                          7.06065
time/total (s)                        639.999
Epoch                                  89
---------------------------------  ---------------
2021-07-02 23:49:02.616622 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 90 finished
---------------------------------  ---------------
replay_buffer/size                 101000
trainer/QF Loss                      7462.43
trainer/Policy Loss                 -2525.52
trainer/Raw Policy Loss             -2525.52
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2478.14
trainer/Q Predictions Std             820.947
trainer/Q Predictions Max            3180.08
trainer/Q Predictions Min             -70.5882
trainer/Q Targets Mean               2464.18
trainer/Q Targets Std                 826.432
trainer/Q Targets Max                3164.92
trainer/Q Targets Min                -125.297
trainer/Bellman Errors Mean          7462.43
trainer/Bellman Errors Std          26053.2
trainer/Bellman Errors Max         224484
trainer/Bellman Errors Min              0.0593665
trainer/Policy Action Mean             -0.00369455
trainer/Policy Action Std               0.908074
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total        101000
exploration/num paths total          1545
exploration/path length Mean           52.6316
exploration/path length Std            59.1734
exploration/path length Max           216
exploration/path length Min             7
exploration/Rewards Mean                1.09835
exploration/Rewards Std                 0.334692
exploration/Rewards Max                 2.76747
exploration/Rewards Min                 0.368587
exploration/Returns Mean               57.8079
exploration/Returns Std                66.4877
exploration/Returns Max               246.035
exploration/Returns Min                 4.81447
exploration/Actions Mean                0.132444
exploration/Actions Std                 0.650043
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  19
exploration/Average Returns            57.8079
evaluation/num steps total          82094
evaluation/num paths total            396
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 1.00852
evaluation/Rewards Std                  0.210657
evaluation/Rewards Max                  1.71776
evaluation/Rewards Min                  0.441072
evaluation/Returns Mean              1008.52
evaluation/Returns Std                  0
evaluation/Returns Max               1008.52
evaluation/Returns Min               1008.52
evaluation/Actions Mean                 0.0271489
evaluation/Actions Std                  0.680217
evaluation/Actions Max                  0.999954
evaluation/Actions Min                 -0.999996
evaluation/Num Paths                    1
evaluation/Average Returns           1008.52
time/data storing (s)                   0.0034417
time/evaluation sampling (s)            0.338715
time/exploration sampling (s)           0.259944
time/logging (s)                        0.00518737
time/saving (s)                         0.00191859
time/training (s)                       6.59017
time/epoch (s)                          7.19938
time/total (s)                        647.2
Epoch                                  90
---------------------------------  ---------------
2021-07-02 23:49:09.846912 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 91 finished
---------------------------------  ---------------
replay_buffer/size                 102000
trainer/QF Loss                      3091.59
trainer/Policy Loss                 -2534.99
trainer/Raw Policy Loss             -2534.99
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2491.56
trainer/Q Predictions Std             894.186
trainer/Q Predictions Max            3347.29
trainer/Q Predictions Min            -316.834
trainer/Q Targets Mean               2506.59
trainer/Q Targets Std                 889.54
trainer/Q Targets Max                3291.5
trainer/Q Targets Min                   0.0502099
trainer/Bellman Errors Mean          3091.59
trainer/Bellman Errors Std          10521.4
trainer/Bellman Errors Max         101451
trainer/Bellman Errors Min              0.344468
trainer/Policy Action Mean             -0.131202
trainer/Policy Action Std               0.889653
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total        102000
exploration/num paths total          1564
exploration/path length Mean           52.6316
exploration/path length Std            50.1537
exploration/path length Max           187
exploration/path length Min             8
exploration/Rewards Mean                0.979146
exploration/Rewards Std                 0.301078
exploration/Rewards Max                 2.31133
exploration/Rewards Min                 0.284605
exploration/Returns Mean               51.534
exploration/Returns Std                52.4029
exploration/Returns Max               185.802
exploration/Returns Min                 5.79166
exploration/Actions Mean                0.0698867
exploration/Actions Std                 0.631869
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  19
exploration/Average Returns            51.534
evaluation/num steps total          83094
evaluation/num paths total            397
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 0.997765
evaluation/Rewards Std                  0.0522015
evaluation/Rewards Max                  1.4493
evaluation/Rewards Min                  0.605314
evaluation/Returns Mean               997.765
evaluation/Returns Std                  0
evaluation/Returns Max                997.765
evaluation/Returns Min                997.765
evaluation/Actions Mean                 0.00957569
evaluation/Actions Std                  0.689911
evaluation/Actions Max                  0.999828
evaluation/Actions Min                 -0.999979
evaluation/Num Paths                    1
evaluation/Average Returns            997.765
time/data storing (s)                   0.00340094
time/evaluation sampling (s)            0.259778
time/exploration sampling (s)           0.270087
time/logging (s)                        0.00394138
time/saving (s)                         0.00126246
time/training (s)                       6.68772
time/epoch (s)                          7.22619
time/total (s)                        654.428
Epoch                                  91
---------------------------------  ---------------
2021-07-02 23:49:17.087857 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 92 finished
---------------------------------  ----------------
replay_buffer/size                 103000
trainer/QF Loss                     10739
trainer/Policy Loss                 -2574.91
trainer/Raw Policy Loss             -2574.91
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2538.85
trainer/Q Predictions Std             912.558
trainer/Q Predictions Max            3416.39
trainer/Q Predictions Min              30.8735
trainer/Q Targets Mean               2521.1
trainer/Q Targets Std                 921.521
trainer/Q Targets Max                3368.9
trainer/Q Targets Min                   1.23156
trainer/Bellman Errors Mean         10739
trainer/Bellman Errors Std          96766.9
trainer/Bellman Errors Max              1.09983e+06
trainer/Bellman Errors Min              0.000143111
trainer/Policy Action Mean              0.0725648
trainer/Policy Action Std               0.901894
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total        103000
exploration/num paths total          1576
exploration/path length Mean           83.3333
exploration/path length Std            80.1918
exploration/path length Max           303
exploration/path length Min            17
exploration/Rewards Mean                1.19831
exploration/Rewards Std                 0.397164
exploration/Rewards Max                 2.39333
exploration/Rewards Min                 0.432393
exploration/Returns Mean               99.8594
exploration/Returns Std                95.7951
exploration/Returns Max               364.225
exploration/Returns Min                16.3762
exploration/Actions Mean                0.128382
exploration/Actions Std                 0.615952
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  12
exploration/Average Returns            99.8594
evaluation/num steps total          84094
evaluation/num paths total            398
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 1.00727
evaluation/Rewards Std                  0.0872505
evaluation/Rewards Max                  1.85373
evaluation/Rewards Min                  0.595382
evaluation/Returns Mean              1007.27
evaluation/Returns Std                  0
evaluation/Returns Max               1007.27
evaluation/Returns Min               1007.27
evaluation/Actions Mean                 0.198776
evaluation/Actions Std                  0.642649
evaluation/Actions Max                  0.999561
evaluation/Actions Min                 -0.999734
evaluation/Num Paths                    1
evaluation/Average Returns           1007.27
time/data storing (s)                   0.00337679
time/evaluation sampling (s)            0.256953
time/exploration sampling (s)           0.263903
time/logging (s)                        0.00527268
time/saving (s)                         0.00158462
time/training (s)                       6.70901
time/epoch (s)                          7.2401
time/total (s)                        661.67
Epoch                                  92
---------------------------------  ----------------
2021-07-02 23:49:24.387106 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 93 finished
---------------------------------  ---------------
replay_buffer/size                 104000
trainer/QF Loss                      5147.59
trainer/Policy Loss                 -2537.13
trainer/Raw Policy Loss             -2537.13
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2468.25
trainer/Q Predictions Std             949.378
trainer/Q Predictions Max            3286.93
trainer/Q Predictions Min              -6.86441
trainer/Q Targets Mean               2442.26
trainer/Q Targets Std                 951.624
trainer/Q Targets Max                3279.98
trainer/Q Targets Min                   0.517533
trainer/Bellman Errors Mean          5147.59
trainer/Bellman Errors Std          13775.9
trainer/Bellman Errors Max         111482
trainer/Bellman Errors Min              0.0721913
trainer/Policy Action Mean              0.0282641
trainer/Policy Action Std               0.906765
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total        104000
exploration/num paths total          1586
exploration/path length Mean          100
exploration/path length Std           135.144
exploration/path length Max           374
exploration/path length Min             7
exploration/Rewards Mean                1.8362
exploration/Rewards Std                 0.93386
exploration/Rewards Max                 4.98934
exploration/Rewards Min                 0.453369
exploration/Returns Mean              183.62
exploration/Returns Std               296.602
exploration/Returns Max               874.119
exploration/Returns Min                 4.47936
exploration/Actions Mean                0.0936929
exploration/Actions Std                 0.670065
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  10
exploration/Average Returns           183.62
evaluation/num steps total          85094
evaluation/num paths total            399
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 1.0047
evaluation/Rewards Std                  0.104974
evaluation/Rewards Max                  1.90832
evaluation/Rewards Min                  0.570191
evaluation/Returns Mean              1004.7
evaluation/Returns Std                  0
evaluation/Returns Max               1004.7
evaluation/Returns Min               1004.7
evaluation/Actions Mean                 0.2369
evaluation/Actions Std                  0.586867
evaluation/Actions Max                  0.999992
evaluation/Actions Min                 -0.999905
evaluation/Num Paths                    1
evaluation/Average Returns           1004.7
time/data storing (s)                   0.00341474
time/evaluation sampling (s)            0.279425
time/exploration sampling (s)           0.256106
time/logging (s)                        0.00388879
time/saving (s)                         0.00125554
time/training (s)                       6.75086
time/epoch (s)                          7.29495
time/total (s)                        668.967
Epoch                                  93
---------------------------------  ---------------
2021-07-02 23:49:31.710589 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 94 finished
---------------------------------  ---------------
replay_buffer/size                 105000
trainer/QF Loss                      6966.8
trainer/Policy Loss                 -2532.53
trainer/Raw Policy Loss             -2532.53
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2494.43
trainer/Q Predictions Std             829.007
trainer/Q Predictions Max            3096.45
trainer/Q Predictions Min            -150.356
trainer/Q Targets Mean               2518.76
trainer/Q Targets Std                 819.002
trainer/Q Targets Max                3141.5
trainer/Q Targets Min                -232.905
trainer/Bellman Errors Mean          6966.8
trainer/Bellman Errors Std          28315.2
trainer/Bellman Errors Max         236401
trainer/Bellman Errors Min              0.0178537
trainer/Policy Action Mean             -0.0127996
trainer/Policy Action Std               0.892706
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total        105000
exploration/num paths total          1604
exploration/path length Mean           55.5556
exploration/path length Std            57.0655
exploration/path length Max           197
exploration/path length Min             7
exploration/Rewards Mean                1.28692
exploration/Rewards Std                 0.59276
exploration/Rewards Max                 3.5637
exploration/Rewards Min                 0.33953
exploration/Returns Mean               71.4956
exploration/Returns Std                87.5397
exploration/Returns Max               273.103
exploration/Returns Min                 4.5744
exploration/Actions Mean                0.107361
exploration/Actions Std                 0.623816
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  18
exploration/Average Returns            71.4956
evaluation/num steps total          86094
evaluation/num paths total            400
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 0.988593
evaluation/Rewards Std                  0.0511127
evaluation/Rewards Max                  1.47774
evaluation/Rewards Min                  0.530426
evaluation/Returns Mean               988.593
evaluation/Returns Std                  0
evaluation/Returns Max                988.593
evaluation/Returns Min                988.593
evaluation/Actions Mean                 0.00483126
evaluation/Actions Std                  0.711812
evaluation/Actions Max                  0.999747
evaluation/Actions Min                 -0.999582
evaluation/Num Paths                    1
evaluation/Average Returns            988.593
time/data storing (s)                   0.00335295
time/evaluation sampling (s)            0.300386
time/exploration sampling (s)           0.264719
time/logging (s)                        0.00499884
time/saving (s)                         0.00148721
time/training (s)                       6.74745
time/epoch (s)                          7.32239
time/total (s)                        676.29
Epoch                                  94
---------------------------------  ---------------
2021-07-02 23:49:39.200397 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 95 finished
---------------------------------  ----------------
replay_buffer/size                 106000
trainer/QF Loss                      5166.95
trainer/Policy Loss                 -2376.51
trainer/Raw Policy Loss             -2376.51
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2338.21
trainer/Q Predictions Std             953.183
trainer/Q Predictions Max            3187.39
trainer/Q Predictions Min             -91.0889
trainer/Q Targets Mean               2337.56
trainer/Q Targets Std                 951.06
trainer/Q Targets Max                3106.79
trainer/Q Targets Min                   0.951078
trainer/Bellman Errors Mean          5166.95
trainer/Bellman Errors Std          30188.6
trainer/Bellman Errors Max         339696
trainer/Bellman Errors Min              0.042032
trainer/Policy Action Mean              0.0243534
trainer/Policy Action Std               0.905521
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total        106000
exploration/num paths total          1624
exploration/path length Mean           50
exploration/path length Std            54.3673
exploration/path length Max           196
exploration/path length Min             7
exploration/Rewards Mean                1.07652
exploration/Rewards Std                 0.274656
exploration/Rewards Max                 1.86631
exploration/Rewards Min                 0.329447
exploration/Returns Mean               53.8261
exploration/Returns Std                61.0746
exploration/Returns Max               212.595
exploration/Returns Min                 5.04611
exploration/Actions Mean                0.0685667
exploration/Actions Std                 0.621328
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  20
exploration/Average Returns            53.8261
evaluation/num steps total          87094
evaluation/num paths total            401
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 0.998187
evaluation/Rewards Std                  0.0578278
evaluation/Rewards Max                  1.65861
evaluation/Rewards Min                  0.66082
evaluation/Returns Mean               998.187
evaluation/Returns Std                  0
evaluation/Returns Max                998.187
evaluation/Returns Min                998.187
evaluation/Actions Mean                -0.000786056
evaluation/Actions Std                  0.768681
evaluation/Actions Max                  0.999997
evaluation/Actions Min                 -0.999893
evaluation/Num Paths                    1
evaluation/Average Returns            998.187
time/data storing (s)                   0.00333437
time/evaluation sampling (s)            0.339758
time/exploration sampling (s)           0.262033
time/logging (s)                        0.00420836
time/saving (s)                         0.0016817
time/training (s)                       6.8753
time/epoch (s)                          7.48631
time/total (s)                        683.779
Epoch                                  95
---------------------------------  ----------------
2021-07-02 23:49:46.534217 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 96 finished
---------------------------------  ---------------
replay_buffer/size                 107000
trainer/QF Loss                      4288.11
trainer/Policy Loss                 -2611.94
trainer/Raw Policy Loss             -2611.94
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2565.67
trainer/Q Predictions Std             672.646
trainer/Q Predictions Max            3172.93
trainer/Q Predictions Min              69.8025
trainer/Q Targets Mean               2586.57
trainer/Q Targets Std                 675.622
trainer/Q Targets Max                3231.5
trainer/Q Targets Min                  60.0337
trainer/Bellman Errors Mean          4288.11
trainer/Bellman Errors Std          13743.4
trainer/Bellman Errors Max         107137
trainer/Bellman Errors Min              0.0140784
trainer/Policy Action Mean             -0.185425
trainer/Policy Action Std               0.861803
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total        107000
exploration/num paths total          1642
exploration/path length Mean           55.5556
exploration/path length Std            57.9216
exploration/path length Max           200
exploration/path length Min             7
exploration/Rewards Mean                0.978156
exploration/Rewards Std                 0.251353
exploration/Rewards Max                 1.86541
exploration/Rewards Min                 0.416025
exploration/Returns Mean               54.342
exploration/Returns Std                58.795
exploration/Returns Max               199.059
exploration/Returns Min                 4.30939
exploration/Actions Mean                0.0306884
exploration/Actions Std                 0.546378
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  18
exploration/Average Returns            54.342
evaluation/num steps total          88094
evaluation/num paths total            402
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 0.988133
evaluation/Rewards Std                  0.0270116
evaluation/Rewards Max                  1.11637
evaluation/Rewards Min                  0.541285
evaluation/Returns Mean               988.133
evaluation/Returns Std                  0
evaluation/Returns Max                988.133
evaluation/Returns Min                988.133
evaluation/Actions Mean                 0.00361172
evaluation/Actions Std                  0.619113
evaluation/Actions Max                  0.909823
evaluation/Actions Min                 -0.999929
evaluation/Num Paths                    1
evaluation/Average Returns            988.133
time/data storing (s)                   0.00338551
time/evaluation sampling (s)            0.276981
time/exploration sampling (s)           0.262732
time/logging (s)                        0.00387078
time/saving (s)                         0.00127549
time/training (s)                       6.78273
time/epoch (s)                          7.33098
time/total (s)                        691.111
Epoch                                  96
---------------------------------  ---------------
2021-07-02 23:49:53.506648 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 97 finished
---------------------------------  ---------------
replay_buffer/size                 108000
trainer/QF Loss                      9736.03
trainer/Policy Loss                 -2559.03
trainer/Raw Policy Loss             -2559.03
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2482.26
trainer/Q Predictions Std             691.635
trainer/Q Predictions Max            3059.35
trainer/Q Predictions Min              30.0798
trainer/Q Targets Mean               2516.09
trainer/Q Targets Std                 682.531
trainer/Q Targets Max                3084.39
trainer/Q Targets Min                  55.3759
trainer/Bellman Errors Mean          9736.03
trainer/Bellman Errors Std          36196.2
trainer/Bellman Errors Max         251462
trainer/Bellman Errors Min              0.064097
trainer/Policy Action Mean             -0.164392
trainer/Policy Action Std               0.879268
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total        108000
exploration/num paths total          1661
exploration/path length Mean           52.6316
exploration/path length Std            51.2632
exploration/path length Max           226
exploration/path length Min             7
exploration/Rewards Mean                0.961008
exploration/Rewards Std                 0.315219
exploration/Rewards Max                 2.42363
exploration/Rewards Min                 0.171531
exploration/Returns Mean               50.5794
exploration/Returns Std                52.1207
exploration/Returns Max               219.102
exploration/Returns Min                 4.23116
exploration/Actions Mean                0.0653887
exploration/Actions Std                 0.588879
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  19
exploration/Average Returns            50.5794
evaluation/num steps total          89094
evaluation/num paths total            403
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 0.990165
evaluation/Rewards Std                  0.037619
evaluation/Rewards Max                  1.10096
evaluation/Rewards Min                  0.596959
evaluation/Returns Mean               990.165
evaluation/Returns Std                  0
evaluation/Returns Max                990.165
evaluation/Returns Min                990.165
evaluation/Actions Mean                 0.00991677
evaluation/Actions Std                  0.483152
evaluation/Actions Max                  0.996869
evaluation/Actions Min                 -0.999733
evaluation/Num Paths                    1
evaluation/Average Returns            990.165
time/data storing (s)                   0.00333846
time/evaluation sampling (s)            0.263869
time/exploration sampling (s)           0.269371
time/logging (s)                        0.00402392
time/saving (s)                         0.00141029
time/training (s)                       6.42837
time/epoch (s)                          6.97038
time/total (s)                        698.083
Epoch                                  97
---------------------------------  ---------------
2021-07-02 23:49:59.289564 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 98 finished
---------------------------------  ---------------
replay_buffer/size                 109000
trainer/QF Loss                     10295.9
trainer/Policy Loss                 -2449.48
trainer/Raw Policy Loss             -2449.48
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2383.31
trainer/Q Predictions Std             770.375
trainer/Q Predictions Max            3094.49
trainer/Q Predictions Min            -358.587
trainer/Q Targets Mean               2396.29
trainer/Q Targets Std                 758.818
trainer/Q Targets Max                3080.96
trainer/Q Targets Min                  -0.357382
trainer/Bellman Errors Mean         10295.9
trainer/Bellman Errors Std          38325.4
trainer/Bellman Errors Max         341100
trainer/Bellman Errors Min              0.222021
trainer/Policy Action Mean              0.00645476
trainer/Policy Action Std               0.900927
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total        109000
exploration/num paths total          1679
exploration/path length Mean           55.5556
exploration/path length Std            68.0263
exploration/path length Max           297
exploration/path length Min             5
exploration/Rewards Mean                1.43023
exploration/Rewards Std                 0.743096
exploration/Rewards Max                 4.90869
exploration/Rewards Min                 0.427375
exploration/Returns Mean               79.4572
exploration/Returns Std               140.597
exploration/Returns Max               632.801
exploration/Returns Min                 3.067
exploration/Actions Mean                0.0705458
exploration/Actions Std                 0.629335
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  18
exploration/Average Returns            79.4572
evaluation/num steps total          90094
evaluation/num paths total            404
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 1.0076
evaluation/Rewards Std                  0.066211
evaluation/Rewards Max                  1.37476
evaluation/Rewards Min                  0.509803
evaluation/Returns Mean              1007.6
evaluation/Returns Std                  0
evaluation/Returns Max               1007.6
evaluation/Returns Min               1007.6
evaluation/Actions Mean                -0.0156535
evaluation/Actions Std                  0.696947
evaluation/Actions Max                  0.994554
evaluation/Actions Min                 -0.99991
evaluation/Num Paths                    1
evaluation/Average Returns           1007.6
time/data storing (s)                   0.00346368
time/evaluation sampling (s)            0.267367
time/exploration sampling (s)           0.263557
time/logging (s)                        0.00391072
time/saving (s)                         0.00128211
time/training (s)                       5.24083
time/epoch (s)                          5.78041
time/total (s)                        703.865
Epoch                                  98
---------------------------------  ---------------
2021-07-02 23:50:05.647171 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 99 finished
---------------------------------  ---------------
replay_buffer/size                 110000
trainer/QF Loss                      4463.59
trainer/Policy Loss                 -2570.16
trainer/Raw Policy Loss             -2570.16
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2525.09
trainer/Q Predictions Std             613.663
trainer/Q Predictions Max            2946.05
trainer/Q Predictions Min             -84.9593
trainer/Q Targets Mean               2527.05
trainer/Q Targets Std                 622.043
trainer/Q Targets Max                2968.42
trainer/Q Targets Min                 -36.2364
trainer/Bellman Errors Mean          4463.59
trainer/Bellman Errors Std          14197.6
trainer/Bellman Errors Max         123665
trainer/Bellman Errors Min              0.00440979
trainer/Policy Action Mean             -0.024789
trainer/Policy Action Std               0.892545
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total        110000
exploration/num paths total          1701
exploration/path length Mean           45.4545
exploration/path length Std            39.734
exploration/path length Max           153
exploration/path length Min             2
exploration/Rewards Mean                1.10132
exploration/Rewards Std                 0.309066
exploration/Rewards Max                 2.08199
exploration/Rewards Min                 0.460088
exploration/Returns Mean               50.0599
exploration/Returns Std                45.8671
exploration/Returns Max               171.4
exploration/Returns Min                 1.7181
exploration/Actions Mean                0.0656979
exploration/Actions Std                 0.565929
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  22
exploration/Average Returns            50.0599
evaluation/num steps total          91094
evaluation/num paths total            405
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 1.00283
evaluation/Rewards Std                  0.0374824
evaluation/Rewards Max                  1.21604
evaluation/Rewards Min                  0.549131
evaluation/Returns Mean              1002.83
evaluation/Returns Std                  0
evaluation/Returns Max               1002.83
evaluation/Returns Min               1002.83
evaluation/Actions Mean                -0.00663786
evaluation/Actions Std                  0.796419
evaluation/Actions Max                  0.935582
evaluation/Actions Min                 -0.999733
evaluation/Num Paths                    1
evaluation/Average Returns           1002.83
time/data storing (s)                   0.00337289
time/evaluation sampling (s)            0.253064
time/exploration sampling (s)           0.261431
time/logging (s)                        0.00376813
time/saving (s)                         0.00125358
time/training (s)                       5.83244
time/epoch (s)                          6.35533
time/total (s)                        710.222
Epoch                                  99
---------------------------------  ---------------
2021-07-02 23:50:12.144974 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 100 finished
---------------------------------  ----------------
replay_buffer/size                 111000
trainer/QF Loss                     19313.9
trainer/Policy Loss                 -2532.52
trainer/Raw Policy Loss             -2532.52
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2472.61
trainer/Q Predictions Std             675.031
trainer/Q Predictions Max            3043.18
trainer/Q Predictions Min              39.1404
trainer/Q Targets Mean               2473.23
trainer/Q Targets Std                 714.113
trainer/Q Targets Max                3051.34
trainer/Q Targets Min                   1.02358
trainer/Bellman Errors Mean         19313.9
trainer/Bellman Errors Std         112083
trainer/Bellman Errors Max              1.15228e+06
trainer/Bellman Errors Min              0.0482798
trainer/Policy Action Mean             -0.159455
trainer/Policy Action Std               0.880545
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total        111000
exploration/num paths total          1724
exploration/path length Mean           43.4783
exploration/path length Std            33.5337
exploration/path length Max           109
exploration/path length Min             7
exploration/Rewards Mean                1.00933
exploration/Rewards Std                 0.299927
exploration/Rewards Max                 2.08017
exploration/Rewards Min                -0.291606
exploration/Returns Mean               43.8838
exploration/Returns Std                38.1348
exploration/Returns Max               123.009
exploration/Returns Min                 4.46309
exploration/Actions Mean                0.0911125
exploration/Actions Std                 0.547386
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  23
exploration/Average Returns            43.8838
evaluation/num steps total          92094
evaluation/num paths total            406
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 1.00144
evaluation/Rewards Std                  0.0291172
evaluation/Rewards Max                  1.23518
evaluation/Rewards Min                  0.563367
evaluation/Returns Mean              1001.44
evaluation/Returns Std                  0
evaluation/Returns Max               1001.44
evaluation/Returns Min               1001.44
evaluation/Actions Mean                -0.00243131
evaluation/Actions Std                  0.605963
evaluation/Actions Max                  0.941365
evaluation/Actions Min                 -0.999865
evaluation/Num Paths                    1
evaluation/Average Returns           1001.44
time/data storing (s)                   0.00333775
time/evaluation sampling (s)            0.251686
time/exploration sampling (s)           0.262479
time/logging (s)                        0.00392968
time/saving (s)                         0.00126928
time/training (s)                       5.97309
time/epoch (s)                          6.4958
time/total (s)                        716.719
Epoch                                 100
---------------------------------  ----------------
2021-07-02 23:50:17.725041 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 101 finished
---------------------------------  ---------------
replay_buffer/size                 112000
trainer/QF Loss                      2985.18
trainer/Policy Loss                 -2274.59
trainer/Raw Policy Loss             -2274.59
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2226.48
trainer/Q Predictions Std             915.58
trainer/Q Predictions Max            3140.86
trainer/Q Predictions Min             -70.4218
trainer/Q Targets Mean               2225.42
trainer/Q Targets Std                 917.835
trainer/Q Targets Max                3093.3
trainer/Q Targets Min                -159.39
trainer/Bellman Errors Mean          2985.18
trainer/Bellman Errors Std           7523.62
trainer/Bellman Errors Max          52162.6
trainer/Bellman Errors Min              0.402308
trainer/Policy Action Mean              0.0154327
trainer/Policy Action Std               0.901528
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total        112000
exploration/num paths total          1736
exploration/path length Mean           83.3333
exploration/path length Std            96.1815
exploration/path length Max           280
exploration/path length Min             8
exploration/Rewards Mean                1.32057
exploration/Rewards Std                 0.406135
exploration/Rewards Max                 2.78161
exploration/Rewards Min                -0.00186735
exploration/Returns Mean              110.048
exploration/Returns Std               137.304
exploration/Returns Max               395.246
exploration/Returns Min                 5.76403
exploration/Actions Mean                0.0572775
exploration/Actions Std                 0.631449
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  12
exploration/Average Returns           110.048
evaluation/num steps total          93094
evaluation/num paths total            407
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 1.00626
evaluation/Rewards Std                  0.0776962
evaluation/Rewards Max                  1.73779
evaluation/Rewards Min                  0.552518
evaluation/Returns Mean              1006.26
evaluation/Returns Std                  0
evaluation/Returns Max               1006.26
evaluation/Returns Min               1006.26
evaluation/Actions Mean                -0.00716218
evaluation/Actions Std                  0.632984
evaluation/Actions Max                  0.97938
evaluation/Actions Min                 -0.999613
evaluation/Num Paths                    1
evaluation/Average Returns           1006.26
time/data storing (s)                   0.00338008
time/evaluation sampling (s)            0.254936
time/exploration sampling (s)           0.262335
time/logging (s)                        0.00396447
time/saving (s)                         0.00127039
time/training (s)                       5.05177
time/epoch (s)                          5.57766
time/total (s)                        722.299
Epoch                                 101
---------------------------------  ---------------
2021-07-02 23:50:23.097504 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 102 finished
---------------------------------  ---------------
replay_buffer/size                 113000
trainer/QF Loss                      6649.55
trainer/Policy Loss                 -2436.91
trainer/Raw Policy Loss             -2436.91
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2398.33
trainer/Q Predictions Std             648.109
trainer/Q Predictions Max            3094.17
trainer/Q Predictions Min              13.99
trainer/Q Targets Mean               2428.4
trainer/Q Targets Std                 649.73
trainer/Q Targets Max                2997.79
trainer/Q Targets Min                  24.345
trainer/Bellman Errors Mean          6649.55
trainer/Bellman Errors Std          34867.9
trainer/Bellman Errors Max         385104
trainer/Bellman Errors Min              2.23976
trainer/Policy Action Mean             -0.064647
trainer/Policy Action Std               0.898985
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total        113000
exploration/num paths total          1765
exploration/path length Mean           34.4828
exploration/path length Std            29.9731
exploration/path length Max           110
exploration/path length Min             5
exploration/Rewards Mean                1.06795
exploration/Rewards Std                 0.512973
exploration/Rewards Max                 2.65866
exploration/Rewards Min                -0.0905179
exploration/Returns Mean               36.8258
exploration/Returns Std                35.3261
exploration/Returns Max               128.748
exploration/Returns Min                 3.37613
exploration/Actions Mean                0.210449
exploration/Actions Std                 0.719805
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  29
exploration/Average Returns            36.8258
evaluation/num steps total          93873
evaluation/num paths total            410
evaluation/path length Mean           259.667
evaluation/path length Std             58.7896
evaluation/path length Max            314
evaluation/path length Min            178
evaluation/Rewards Mean                 1.1469
evaluation/Rewards Std                  0.493081
evaluation/Rewards Max                  2.56284
evaluation/Rewards Min                  0.0211962
evaluation/Returns Mean               297.811
evaluation/Returns Std                 66.8376
evaluation/Returns Max                363.659
evaluation/Returns Min                206.16
evaluation/Actions Mean                 0.286394
evaluation/Actions Std                  0.765686
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    3
evaluation/Average Returns            297.811
time/data storing (s)                   0.00336434
time/evaluation sampling (s)            0.238421
time/exploration sampling (s)           0.255513
time/logging (s)                        0.00362935
time/saving (s)                         0.00121142
time/training (s)                       4.86776
time/epoch (s)                          5.3699
time/total (s)                        727.67
Epoch                                 102
---------------------------------  ---------------
2021-07-02 23:50:29.195292 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 103 finished
---------------------------------  ---------------
replay_buffer/size                 114000
trainer/QF Loss                      2377.26
trainer/Policy Loss                 -2504.09
trainer/Raw Policy Loss             -2504.09
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2451.98
trainer/Q Predictions Std             724.225
trainer/Q Predictions Max            3015.28
trainer/Q Predictions Min            -119.281
trainer/Q Targets Mean               2433.24
trainer/Q Targets Std                 715.368
trainer/Q Targets Max                3017.78
trainer/Q Targets Min                   0.869897
trainer/Bellman Errors Mean          2377.26
trainer/Bellman Errors Std           4958.63
trainer/Bellman Errors Max          47872.7
trainer/Bellman Errors Min              0.0594102
trainer/Policy Action Mean             -0.0557391
trainer/Policy Action Std               0.893465
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total        114000
exploration/num paths total          1787
exploration/path length Mean           45.4545
exploration/path length Std            37.4223
exploration/path length Max           140
exploration/path length Min             7
exploration/Rewards Mean                1.17895
exploration/Rewards Std                 0.45366
exploration/Rewards Max                 2.68967
exploration/Rewards Min                 0.139532
exploration/Returns Mean               53.5887
exploration/Returns Std                50.0462
exploration/Returns Max               177.221
exploration/Returns Min                 4.9028
exploration/Actions Mean                0.0715518
exploration/Actions Std                 0.690403
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  22
exploration/Average Returns            53.5887
evaluation/num steps total          94873
evaluation/num paths total            411
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 1.05534
evaluation/Rewards Std                  0.371254
evaluation/Rewards Max                  2.27743
evaluation/Rewards Min                  0.179242
evaluation/Returns Mean              1055.34
evaluation/Returns Std                  0
evaluation/Returns Max               1055.34
evaluation/Returns Min               1055.34
evaluation/Actions Mean                 0.00765611
evaluation/Actions Std                  0.713437
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    1
evaluation/Average Returns           1055.34
time/data storing (s)                   0.0034075
time/evaluation sampling (s)            0.248343
time/exploration sampling (s)           0.256056
time/logging (s)                        0.00410084
time/saving (s)                         0.00148903
time/training (s)                       5.58252
time/epoch (s)                          6.09592
time/total (s)                        733.768
Epoch                                 103
---------------------------------  ---------------
2021-07-02 23:50:35.045705 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 104 finished
---------------------------------  ----------------
replay_buffer/size                 115000
trainer/QF Loss                     34753.1
trainer/Policy Loss                 -2436.41
trainer/Raw Policy Loss             -2436.41
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2391.15
trainer/Q Predictions Std             722.762
trainer/Q Predictions Max            3302.28
trainer/Q Predictions Min              15.935
trainer/Q Targets Mean               2373.29
trainer/Q Targets Std                 769.153
trainer/Q Targets Max                3337.79
trainer/Q Targets Min                 -33.2087
trainer/Bellman Errors Mean         34753.1
trainer/Bellman Errors Std         264702
trainer/Bellman Errors Max              2.77215e+06
trainer/Bellman Errors Min              0.0335276
trainer/Policy Action Mean              0.0490841
trainer/Policy Action Std               0.881905
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total        115000
exploration/num paths total          1797
exploration/path length Mean          100
exploration/path length Std            95.8687
exploration/path length Max           373
exploration/path length Min            19
exploration/Rewards Mean                1.47239
exploration/Rewards Std                 0.571407
exploration/Rewards Max                 3.0888
exploration/Rewards Min                 0.247193
exploration/Returns Mean              147.239
exploration/Returns Std               162.248
exploration/Returns Max               606.304
exploration/Returns Min                18.7199
exploration/Actions Mean                0.157476
exploration/Actions Std                 0.656055
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  10
exploration/Average Returns           147.239
evaluation/num steps total          95873
evaluation/num paths total            412
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 1.78026
evaluation/Rewards Std                  0.513527
evaluation/Rewards Max                  3.7589
evaluation/Rewards Min                  0.504231
evaluation/Returns Mean              1780.26
evaluation/Returns Std                  0
evaluation/Returns Max               1780.26
evaluation/Returns Min               1780.26
evaluation/Actions Mean                 0.0511305
evaluation/Actions Std                  0.783671
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    1
evaluation/Average Returns           1780.26
time/data storing (s)                   0.00342597
time/evaluation sampling (s)            0.248216
time/exploration sampling (s)           0.256479
time/logging (s)                        0.00390823
time/saving (s)                         0.00121861
time/training (s)                       5.33452
time/epoch (s)                          5.84777
time/total (s)                        739.617
Epoch                                 104
---------------------------------  ----------------
2021-07-02 23:50:40.304492 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 105 finished
---------------------------------  ---------------
replay_buffer/size                 116000
trainer/QF Loss                     11284.6
trainer/Policy Loss                 -2558.56
trainer/Raw Policy Loss             -2558.56
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2487.21
trainer/Q Predictions Std             733.812
trainer/Q Predictions Max            3103.56
trainer/Q Predictions Min              26.3775
trainer/Q Targets Mean               2457.9
trainer/Q Targets Std                 753.803
trainer/Q Targets Max                3073.42
trainer/Q Targets Min                   1.1445
trainer/Bellman Errors Mean         11284.6
trainer/Bellman Errors Std          62079.5
trainer/Bellman Errors Max         592687
trainer/Bellman Errors Min              0.349847
trainer/Policy Action Mean             -0.0222664
trainer/Policy Action Std               0.892753
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total        116000
exploration/num paths total          1812
exploration/path length Mean           66.6667
exploration/path length Std            88.7045
exploration/path length Max           285
exploration/path length Min             6
exploration/Rewards Mean                1.31907
exploration/Rewards Std                 0.555127
exploration/Rewards Max                 3.26044
exploration/Rewards Min                 0.0106556
exploration/Returns Mean               87.9379
exploration/Returns Std               128.859
exploration/Returns Max               406.51
exploration/Returns Min                 4.24068
exploration/Actions Mean                0.233509
exploration/Actions Std                 0.664686
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  15
exploration/Average Returns            87.9379
evaluation/num steps total          96873
evaluation/num paths total            413
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 1.30831
evaluation/Rewards Std                  0.522821
evaluation/Rewards Max                  2.89487
evaluation/Rewards Min                  0.196383
evaluation/Returns Mean              1308.31
evaluation/Returns Std                  0
evaluation/Returns Max               1308.31
evaluation/Returns Min               1308.31
evaluation/Actions Mean                 0.267882
evaluation/Actions Std                  0.708107
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    1
evaluation/Average Returns           1308.31
time/data storing (s)                   0.00334534
time/evaluation sampling (s)            0.236184
time/exploration sampling (s)           0.253941
time/logging (s)                        0.00379432
time/saving (s)                         0.00123569
time/training (s)                       4.75777
time/epoch (s)                          5.25627
time/total (s)                        744.875
Epoch                                 105
---------------------------------  ---------------
2021-07-02 23:50:45.656537 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 106 finished
---------------------------------  ---------------
replay_buffer/size                 117000
trainer/QF Loss                     11882
trainer/Policy Loss                 -2542.81
trainer/Raw Policy Loss             -2542.81
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2464.88
trainer/Q Predictions Std             835.319
trainer/Q Predictions Max            3255.72
trainer/Q Predictions Min             -90.1768
trainer/Q Targets Mean               2477.54
trainer/Q Targets Std                 822.168
trainer/Q Targets Max                3199.49
trainer/Q Targets Min                 -21.3333
trainer/Bellman Errors Mean         11882
trainer/Bellman Errors Std          63396.2
trainer/Bellman Errors Max         524370
trainer/Bellman Errors Min              0.0233576
trainer/Policy Action Mean              0.0799627
trainer/Policy Action Std               0.900036
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total        117000
exploration/num paths total          1832
exploration/path length Mean           50
exploration/path length Std            65.6544
exploration/path length Max           293
exploration/path length Min             8
exploration/Rewards Mean                1.57066
exploration/Rewards Std                 0.669423
exploration/Rewards Max                 3.76697
exploration/Rewards Min                 0.37089
exploration/Returns Mean               78.5332
exploration/Returns Std               127.219
exploration/Returns Max               563.237
exploration/Returns Min                 5.59697
exploration/Actions Mean                0.199913
exploration/Actions Std                 0.717439
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  20
exploration/Average Returns            78.5332
evaluation/num steps total          97697
evaluation/num paths total            417
evaluation/path length Mean           206
evaluation/path length Std            148.14
evaluation/path length Max            456
evaluation/path length Min             70
evaluation/Rewards Mean                 1.50006
evaluation/Rewards Std                  0.490899
evaluation/Rewards Max                  2.74319
evaluation/Rewards Min                  0.528662
evaluation/Returns Mean               309.013
evaluation/Returns Std                223.626
evaluation/Returns Max                686.376
evaluation/Returns Min                104.263
evaluation/Actions Mean                 0.251318
evaluation/Actions Std                  0.778476
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    4
evaluation/Average Returns            309.013
time/data storing (s)                   0.00338185
time/evaluation sampling (s)            0.240602
time/exploration sampling (s)           0.25588
time/logging (s)                        0.00387633
time/saving (s)                         0.00131315
time/training (s)                       4.8447
time/epoch (s)                          5.34975
time/total (s)                        750.227
Epoch                                 106
---------------------------------  ---------------
2021-07-02 23:50:51.013348 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 107 finished
---------------------------------  ---------------
replay_buffer/size                 118000
trainer/QF Loss                      4904.63
trainer/Policy Loss                 -2424.23
trainer/Raw Policy Loss             -2424.23
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2368.38
trainer/Q Predictions Std             850.005
trainer/Q Predictions Max            3152.19
trainer/Q Predictions Min            -121.431
trainer/Q Targets Mean               2399.46
trainer/Q Targets Std                 850.523
trainer/Q Targets Max                3243.34
trainer/Q Targets Min                   0.961783
trainer/Bellman Errors Mean          4904.63
trainer/Bellman Errors Std          12536.6
trainer/Bellman Errors Max          84661.5
trainer/Bellman Errors Min              0.00992203
trainer/Policy Action Mean              0.1027
trainer/Policy Action Std               0.906079
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total        118000
exploration/num paths total          1855
exploration/path length Mean           43.4783
exploration/path length Std            35.6698
exploration/path length Max           126
exploration/path length Min             6
exploration/Rewards Mean                1.24503
exploration/Rewards Std                 0.489122
exploration/Rewards Max                 2.62695
exploration/Rewards Min                 0.346439
exploration/Returns Mean               54.1316
exploration/Returns Std                46.8583
exploration/Returns Max               165.444
exploration/Returns Min                 4.19104
exploration/Actions Mean                0.179947
exploration/Actions Std                 0.692092
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  23
exploration/Average Returns            54.1316
evaluation/num steps total          97889
evaluation/num paths total            419
evaluation/path length Mean            96
evaluation/path length Std             21
evaluation/path length Max            117
evaluation/path length Min             75
evaluation/Rewards Mean                 1.28492
evaluation/Rewards Std                  0.457724
evaluation/Rewards Max                  2.16963
evaluation/Rewards Min                  0.360139
evaluation/Returns Mean               123.352
evaluation/Returns Std                 24.4542
evaluation/Returns Max                147.806
evaluation/Returns Min                 98.8978
evaluation/Actions Mean                 0.141706
evaluation/Actions Std                  0.760206
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    2
evaluation/Average Returns            123.352
time/data storing (s)                   0.00340394
time/evaluation sampling (s)            0.248032
time/exploration sampling (s)           0.257396
time/logging (s)                        0.00260763
time/saving (s)                         0.00123283
time/training (s)                       4.84067
time/epoch (s)                          5.35334
time/total (s)                        755.581
Epoch                                 107
---------------------------------  ---------------
2021-07-02 23:50:56.662560 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 108 finished
---------------------------------  ----------------
replay_buffer/size                 119000
trainer/QF Loss                     24343.9
trainer/Policy Loss                 -2636.53
trainer/Raw Policy Loss             -2636.53
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2588.6
trainer/Q Predictions Std             645.511
trainer/Q Predictions Max            3128.62
trainer/Q Predictions Min              91.0783
trainer/Q Targets Mean               2562.16
trainer/Q Targets Std                 682.617
trainer/Q Targets Max                3161.47
trainer/Q Targets Min                   0.918168
trainer/Bellman Errors Mean         24343.9
trainer/Bellman Errors Std         233427
trainer/Bellman Errors Max              2.65377e+06
trainer/Bellman Errors Min              0.315034
trainer/Policy Action Mean             -0.229873
trainer/Policy Action Std               0.872715
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total        119000
exploration/num paths total          1872
exploration/path length Mean           58.8235
exploration/path length Std            69.1131
exploration/path length Max           250
exploration/path length Min             9
exploration/Rewards Mean                1.03766
exploration/Rewards Std                 0.337673
exploration/Rewards Max                 2.49078
exploration/Rewards Min                 0.362711
exploration/Returns Mean               61.0385
exploration/Returns Std                78.8371
exploration/Returns Max               248.393
exploration/Returns Min                 5.22174
exploration/Actions Mean                0.0887954
exploration/Actions Std                 0.582609
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  17
exploration/Average Returns            61.0385
evaluation/num steps total          98889
evaluation/num paths total            420
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 1.0036
evaluation/Rewards Std                  0.0462596
evaluation/Rewards Max                  1.18671
evaluation/Rewards Min                  0.446091
evaluation/Returns Mean              1003.6
evaluation/Returns Std                  0
evaluation/Returns Max               1003.6
evaluation/Returns Min               1003.6
evaluation/Actions Mean                 0.264556
evaluation/Actions Std                  0.593168
evaluation/Actions Max                  0.988949
evaluation/Actions Min                 -0.999974
evaluation/Num Paths                    1
evaluation/Average Returns           1003.6
time/data storing (s)                   0.00343948
time/evaluation sampling (s)            0.268674
time/exploration sampling (s)           0.259277
time/logging (s)                        0.00379157
time/saving (s)                         0.00128732
time/training (s)                       5.11199
time/epoch (s)                          5.64846
time/total (s)                        761.231
Epoch                                 108
---------------------------------  ----------------
2021-07-02 23:51:01.979033 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 109 finished
---------------------------------  ----------------
replay_buffer/size                 120000
trainer/QF Loss                     22208.9
trainer/Policy Loss                 -2500.04
trainer/Raw Policy Loss             -2500.04
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2398.31
trainer/Q Predictions Std             918.531
trainer/Q Predictions Max            3332.54
trainer/Q Predictions Min            -143.149
trainer/Q Targets Mean               2378.79
trainer/Q Targets Std                 958.386
trainer/Q Targets Max                3340.62
trainer/Q Targets Min                 -61.5009
trainer/Bellman Errors Mean         22208.9
trainer/Bellman Errors Std         201704
trainer/Bellman Errors Max              2.28535e+06
trainer/Bellman Errors Min              0.115312
trainer/Policy Action Mean             -0.10809
trainer/Policy Action Std               0.874549
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total        120000
exploration/num paths total          1890
exploration/path length Mean           55.5556
exploration/path length Std            36.2755
exploration/path length Max           136
exploration/path length Min             9
exploration/Rewards Mean                0.96513
exploration/Rewards Std                 0.25441
exploration/Rewards Max                 1.626
exploration/Rewards Min                -0.136138
exploration/Returns Mean               53.6183
exploration/Returns Std                38.5032
exploration/Returns Max               142.65
exploration/Returns Min                 4.38421
exploration/Actions Mean                0.0731972
exploration/Actions Std                 0.567772
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  18
exploration/Average Returns            53.6183
evaluation/num steps total          99889
evaluation/num paths total            421
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 1.00533
evaluation/Rewards Std                  0.0459312
evaluation/Rewards Max                  1.18306
evaluation/Rewards Min                  0.448518
evaluation/Returns Mean              1005.33
evaluation/Returns Std                  0
evaluation/Returns Max               1005.33
evaluation/Returns Min               1005.33
evaluation/Actions Mean                 0.225669
evaluation/Actions Std                  0.496105
evaluation/Actions Max                  0.979535
evaluation/Actions Min                 -0.999992
evaluation/Num Paths                    1
evaluation/Average Returns           1005.33
time/data storing (s)                   0.00340345
time/evaluation sampling (s)            0.251111
time/exploration sampling (s)           0.261863
time/logging (s)                        0.00381287
time/saving (s)                         0.00119065
time/training (s)                       4.79275
time/epoch (s)                          5.31414
time/total (s)                        766.547
Epoch                                 109
---------------------------------  ----------------
2021-07-02 23:51:08.023439 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 110 finished
---------------------------------  ----------------
replay_buffer/size                 121000
trainer/QF Loss                     44423.6
trainer/Policy Loss                 -2631.01
trainer/Raw Policy Loss             -2631.01
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2561.19
trainer/Q Predictions Std             775.643
trainer/Q Predictions Max            3304.1
trainer/Q Predictions Min              25.3277
trainer/Q Targets Mean               2525.49
trainer/Q Targets Std                 815.346
trainer/Q Targets Max                3164.3
trainer/Q Targets Min                 -42.922
trainer/Bellman Errors Mean         44423.6
trainer/Bellman Errors Std         376358
trainer/Bellman Errors Max              4.24402e+06
trainer/Bellman Errors Min              0.0146637
trainer/Policy Action Mean              0.0614887
trainer/Policy Action Std               0.899727
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total        121000
exploration/num paths total          1903
exploration/path length Mean           76.9231
exploration/path length Std            84.4243
exploration/path length Max           333
exploration/path length Min             9
exploration/Rewards Mean                1.43625
exploration/Rewards Std                 0.601202
exploration/Rewards Max                 3.96788
exploration/Rewards Min                 0.329075
exploration/Returns Mean              110.481
exploration/Returns Std               149.554
exploration/Returns Max               594.999
exploration/Returns Min                 5.60617
exploration/Actions Mean                0.105698
exploration/Actions Std                 0.703464
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  13
exploration/Average Returns           110.481
evaluation/num steps total         100889
evaluation/num paths total            422
evaluation/path length Mean          1000
evaluation/path length Std              0
evaluation/path length Max           1000
evaluation/path length Min           1000
evaluation/Rewards Mean                 1.02969
evaluation/Rewards Std                  0.171471
evaluation/Rewards Max                  2.27718
evaluation/Rewards Min                  0.564792
evaluation/Returns Mean              1029.69
evaluation/Returns Std                  0
evaluation/Returns Max               1029.69
evaluation/Returns Min               1029.69
evaluation/Actions Mean                 0.179877
evaluation/Actions Std                  0.653946
evaluation/Actions Max                  1
evaluation/Actions Min                 -0.999985
evaluation/Num Paths                    1
evaluation/Average Returns           1029.69
time/data storing (s)                   0.00331279
time/evaluation sampling (s)            0.247964
time/exploration sampling (s)           0.252226
time/logging (s)                        0.0041161
time/saving (s)                         0.00171494
time/training (s)                       5.53321
time/epoch (s)                          6.04254
time/total (s)                        772.591
Epoch                                 110
---------------------------------  ----------------
2021-07-02 23:51:13.951607 CST | [ddpg-experiment-on-hopper_2021_07_02_23_38_15_0000--s-0] Epoch 111 finished
---------------------------------  ---------------
replay_buffer/size                 122000
trainer/QF Loss                      5964.21
trainer/Policy Loss                 -2593.53
trainer/Raw Policy Loss             -2593.53
trainer/Preactivation Policy Loss       0
trainer/Q Predictions Mean           2523.59
trainer/Q Predictions Std             779.67
trainer/Q Predictions Max            3317.11
trainer/Q Predictions Min              34.6383
trainer/Q Targets Mean               2528.5
trainer/Q Targets Std                 805.23
trainer/Q Targets Max                3327.13
trainer/Q Targets Min                   1.4486
trainer/Bellman Errors Mean          5964.21
trainer/Bellman Errors Std          31073.7
trainer/Bellman Errors Max         338737
trainer/Bellman Errors Min              0.119342
trainer/Policy Action Mean             -0.056895
trainer/Policy Action Std               0.899554
trainer/Policy Action Max               1
trainer/Policy Action Min              -1
exploration/num steps total        122000
exploration/num paths total          1933
exploration/path length Mean           33.3333
exploration/path length Std            49.5858
exploration/path length Max           241
exploration/path length Min             7
exploration/Rewards Mean                1.43543
exploration/Rewards Std                 0.727264
exploration/Rewards Max                 4.50364
exploration/Rewards Min                 0.162079
exploration/Returns Mean               47.8477
exploration/Returns Std                98.8281
exploration/Returns Max               522.181
exploration/Returns Min                 3.99473
exploration/Actions Mean                0.110099
exploration/Actions Std                 0.694175
exploration/Actions Max                 1
exploration/Actions Min                -1
exploration/Num Paths                  30
exploration/Average Returns            47.8477
evaluation/num steps total         101795
evaluation/num paths total            431
evaluation/path length Mean           100.667
evaluation/path length Std              2.90593
evaluation/path length Max            106
evaluation/path length Min             96
evaluation/Rewards Mean                 1.47808
evaluation/Rewards Std                  0.552101
evaluation/Rewards Max                  2.80531
evaluation/Rewards Min                  0.569131
evaluation/Returns Mean               148.794
evaluation/Returns Std                  5.59075
evaluation/Returns Max                159.278
evaluation/Returns Min                139.77
evaluation/Actions Mean                 0.0852321
evaluation/Actions Std                  0.768162
evaluation/Actions Max                  1
evaluation/Actions Min                 -1
evaluation/Num Paths                    9
evaluation/Average Returns            148.794
time/data storing (s)                   0.00348452
time/evaluation sampling (s)            0.250307
time/exploration sampling (s)           0.265143
time/logging (s)                        0.00385494
time/saving (s)                         0.0013216
time/training (s)                       5.40118
time/epoch (s)                          5.9253
time/total (s)                        778.518
Epoch                                 111
---------------------------------  ---------------
