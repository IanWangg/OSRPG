diff --git a/examples/sac.py b/examples/sac.py
index 0d86396..7b98fb0 100644
--- a/examples/sac.py
+++ b/examples/sac.py
@@ -1,4 +1,7 @@
-from gym.envs.mujoco import HalfCheetahEnv
+from gym.envs.mujoco import HopperEnv
+
+import sys
+sys.path.append('../')
 
 import rlkit.torch.pytorch_util as ptu
 from rlkit.data_management.env_replay_buffer import EnvReplayBuffer
@@ -12,8 +15,8 @@ from rlkit.torch.torch_rl_algorithm import TorchBatchRLAlgorithm
 
 
 def experiment(variant):
-    expl_env = NormalizedBoxEnv(HalfCheetahEnv())
-    eval_env = NormalizedBoxEnv(HalfCheetahEnv())
+    expl_env = NormalizedBoxEnv(HopperEnv())
+    eval_env = NormalizedBoxEnv(HopperEnv())
     obs_dim = expl_env.observation_space.low.size
     action_dim = eval_env.action_space.low.size
 
@@ -106,6 +109,6 @@ if __name__ == "__main__":
             use_automatic_entropy_tuning=True,
         ),
     )
-    setup_logger('name-of-experiment', variant=variant)
+    setup_logger('Test sac on hopper env', variant=variant)
     # ptu.set_gpu_mode(True)  # optionally set the GPU (default=False)
     experiment(variant)
diff --git a/examples/td3.py b/examples/td3.py
index b86fa50..11d3503 100644
--- a/examples/td3.py
+++ b/examples/td3.py
@@ -6,7 +6,9 @@ a bit noisy from one epoch to the next (occasionally dips dow to ~2000).
 
 Note that one epoch = 5k steps, so 200 epochs = 1 million steps.
 """
-from gym.envs.mujoco import HalfCheetahEnv
+from gym.envs.mujoco import HopperEnv
+import sys
+sys.path.append('../')
 
 import rlkit.torch.pytorch_util as ptu
 from rlkit.data_management.env_replay_buffer import EnvReplayBuffer
@@ -22,8 +24,8 @@ from rlkit.torch.torch_rl_algorithm import TorchBatchRLAlgorithm
 
 
 def experiment(variant):
-    expl_env = NormalizedBoxEnv(HalfCheetahEnv())
-    eval_env = NormalizedBoxEnv(HalfCheetahEnv())
+    expl_env = NormalizedBoxEnv(HopperEnv())
+    eval_env = NormalizedBoxEnv(HopperEnv())
     obs_dim = expl_env.observation_space.low.size
     action_dim = expl_env.action_space.low.size
     qf1 = ConcatMlp(
diff --git a/rlkit/core/batch_rl_algorithm.py b/rlkit/core/batch_rl_algorithm.py
index 848c5eb..4a5ce08 100644
--- a/rlkit/core/batch_rl_algorithm.py
+++ b/rlkit/core/batch_rl_algorithm.py
@@ -23,6 +23,7 @@ class BatchRLAlgorithm(BaseRLAlgorithm, metaclass=abc.ABCMeta):
             num_trains_per_train_loop,
             num_train_loops_per_epoch=1,
             min_num_steps_before_training=0,
+            offline=False,
     ):
         super().__init__(
             trainer,
@@ -40,9 +41,10 @@ class BatchRLAlgorithm(BaseRLAlgorithm, metaclass=abc.ABCMeta):
         self.num_train_loops_per_epoch = num_train_loops_per_epoch
         self.num_expl_steps_per_train_loop = num_expl_steps_per_train_loop
         self.min_num_steps_before_training = min_num_steps_before_training
+        self.offline = offline
 
     def _train(self):
-        if self.min_num_steps_before_training > 0:
+        if self.min_num_steps_before_training > 0 and not self.offline:
             init_expl_paths = self.expl_data_collector.collect_new_paths(
                 self.max_path_length,
                 self.min_num_steps_before_training,
@@ -63,15 +65,17 @@ class BatchRLAlgorithm(BaseRLAlgorithm, metaclass=abc.ABCMeta):
             gt.stamp('evaluation sampling')
 
             for _ in range(self.num_train_loops_per_epoch):
-                new_expl_paths = self.expl_data_collector.collect_new_paths(
-                    self.max_path_length,
-                    self.num_expl_steps_per_train_loop,
-                    discard_incomplete_paths=False,
-                )
-                gt.stamp('exploration sampling', unique=False)
+                # if under offline setting, there is no need to sample new transitions
+                if not self.offline:
+                    new_expl_paths = self.expl_data_collector.collect_new_paths(
+                        self.max_path_length,
+                        self.num_expl_steps_per_train_loop,
+                        discard_incomplete_paths=False,
+                    )
+                    gt.stamp('exploration sampling', unique=False)
 
-                self.replay_buffer.add_paths(new_expl_paths)
-                gt.stamp('data storing', unique=False)
+                    self.replay_buffer.add_paths(new_expl_paths)
+                    gt.stamp('data storing', unique=False)
 
                 self.training_mode(True)
                 for _ in range(self.num_trains_per_train_loop):
diff --git a/rlkit/core/rl_algorithm.py b/rlkit/core/rl_algorithm.py
index 284ef46..ac03e43 100644
--- a/rlkit/core/rl_algorithm.py
+++ b/rlkit/core/rl_algorithm.py
@@ -106,10 +106,11 @@ class BaseRLAlgorithm(object, metaclass=abc.ABCMeta):
                 self.expl_env.get_diagnostics(expl_paths),
                 prefix='exploration/',
             )
-        logger.record_dict(
-            eval_util.get_generic_path_information(expl_paths),
-            prefix="exploration/",
-        )
+        if not self.offline:
+            logger.record_dict(
+                eval_util.get_generic_path_information(expl_paths),
+                prefix="exploration/",
+            )
         """
         Evaluation
         """
diff --git a/rlkit/torch/networks/__init__.py b/rlkit/torch/networks/__init__.py
index 47d00e0..2e5d867 100644
--- a/rlkit/torch/networks/__init__.py
+++ b/rlkit/torch/networks/__init__.py
@@ -13,11 +13,12 @@ from rlkit.torch.networks.image_state import ImageStatePolicy, ImageStateQ
 from rlkit.torch.networks.linear_transform import LinearTransform
 from rlkit.torch.networks.normalization import LayerNorm
 from rlkit.torch.networks.mlp import (
-    Mlp, ConcatMlp, MlpPolicy, TanhMlpPolicy,
+    Mlp, ConcatMlp, MlpPolicy, TanhMlpPolicy, FlattenMlp,
     MlpQf,
     MlpQfWithObsProcessor,
     ConcatMultiHeadedMlp,
 )
+from rlkit.torch.networks.custom import *
 from rlkit.torch.networks.pretrained_cnn import PretrainedCNN
 from rlkit.torch.networks.two_headed_mlp import TwoHeadMlp
 
diff --git a/rlkit/torch/networks/custom.py b/rlkit/torch/networks/custom.py
index d43435a..0e72671 100644
--- a/rlkit/torch/networks/custom.py
+++ b/rlkit/torch/networks/custom.py
@@ -1,3 +1,14 @@
 """
 Random networks
 """
+from rlkit.torch.core import PyTorchModule
+from torch import nn
+import torch.nn.functional as F
+
+class LinearMlp(PyTorchModule):
+    def __init__(self, input_size, output_size):
+        super().__init__()
+        self.fc = nn.Linear(input_size, output_size)
+
+    def forward(self, x):
+        return self.fc(x)
diff --git a/rlkit/torch/networks/mlp.py b/rlkit/torch/networks/mlp.py
index a9ad8f1..310b6e3 100644
--- a/rlkit/torch/networks/mlp.py
+++ b/rlkit/torch/networks/mlp.py
@@ -11,6 +11,7 @@ from rlkit.torch.networks import LayerNorm
 from rlkit.torch.pytorch_util import activation_from_string
 
 
+
 class Mlp(PyTorchModule):
     def __init__(
             self,
@@ -55,6 +56,8 @@ class Mlp(PyTorchModule):
         self.last_fc = nn.Linear(in_size, output_size)
         self.last_fc.weight.data.uniform_(-init_w, init_w)
         self.last_fc.bias.data.fill_(0)
+        
+        self.to(ptu.device)
 
     def forward(self, input, return_preactivations=False):
         h = input
@@ -71,6 +74,16 @@ class Mlp(PyTorchModule):
             return output
 
 
+class FlattenMlp(Mlp):
+    """
+    Flatten inputs along dimension 1 and then pass through MLP.
+    """
+    def forward(self, *inputs, **kwargs):
+        flat_inputs = torch.cat(inputs, dim=1)
+        return super().forward(flat_inputs, **kwargs)
+
+
+
 class MultiHeadedMlp(Mlp):
     """
                    .-> linear head 0
@@ -128,7 +141,7 @@ class ConcatMultiHeadedMlp(MultiHeadedMlp):
 
 class ConcatMlp(Mlp):
     """
-    Concatenate inputs along dimension and then pass through MLP.
+    Concatenate inputs along dimension and then pass through MLP. It is just a better version of flattenMLP
     """
     def __init__(self, *args, dim=1, **kwargs):
         super().__init__(*args, **kwargs)
diff --git a/rlkit/torch/sac/sac.py b/rlkit/torch/sac/sac.py
index 825dfb9..a225f66 100644
--- a/rlkit/torch/sac/sac.py
+++ b/rlkit/torch/sac/sac.py
@@ -99,7 +99,7 @@ class SACTrainer(TorchTrainer, LossFunction):
             skip_statistics=not self._need_to_update_eval_statistics,
         )
         """
-        Update networks
+        Update networks, alpha can also be tuned
         """
         if self.use_automatic_entropy_tuning:
             self.alpha_optimizer.zero_grad()
@@ -163,6 +163,7 @@ class SACTrainer(TorchTrainer, LossFunction):
             alpha_loss = 0
             alpha = 1
 
+        # the double q is the problem
         q_new_actions = torch.min(
             self.qf1(obs, new_obs_actions),
             self.qf2(obs, new_obs_actions),
@@ -175,6 +176,7 @@ class SACTrainer(TorchTrainer, LossFunction):
         q1_pred = self.qf1(obs, actions)
         q2_pred = self.qf2(obs, actions)
         next_dist = self.policy(next_obs)
+        # the new_log_pi is the entropy of pi, not log probability
         new_next_actions, new_log_pi = next_dist.rsample_and_logprob()
         new_log_pi = new_log_pi.unsqueeze(-1)
         target_q_values = torch.min(
@@ -212,6 +214,7 @@ class SACTrainer(TorchTrainer, LossFunction):
                 'Log Pis',
                 ptu.get_numpy(log_pi),
             ))
+            
             policy_statistics = add_prefix(dist.get_diagnostics(), "policy/")
             eval_statistics.update(policy_statistics)
             if self.use_automatic_entropy_tuning:
